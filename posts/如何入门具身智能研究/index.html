<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=dark data-auto-appearance=false><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><meta name=theme-color><title>【转载】如何入门具身智能研究 &#183; MyBlog</title><meta name=title content="【转载】如何入门具身智能研究 &#183; MyBlog"><meta name=description content="My awesome website"><meta name=keywords content="embodied-ai,"><link rel=canonical href=https://blog.xiadengma.com/posts/%E5%A6%82%E4%BD%95%E5%85%A5%E9%97%A8%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6/><meta name=author content="xiadengma"><link href=https://github.com/xiadengma rel=me><link href=https://xiadengma.com/ rel=me><meta property="og:url" content="https://blog.xiadengma.com/posts/%E5%A6%82%E4%BD%95%E5%85%A5%E9%97%A8%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6/"><meta property="og:site_name" content="MyBlog"><meta property="og:title" content="【转载】如何入门具身智能研究"><meta property="og:description" content="My awesome website"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-13T13:36:15+08:00"><meta property="article:modified_time" content="2025-11-13T13:51:37+08:00"><meta property="article:tag" content="Embodied-Ai"><meta name=twitter:card content="summary"><meta name=twitter:title content="【转载】如何入门具身智能研究"><meta name=twitter:description content="My awesome website"><link type=text/css rel=stylesheet href=/css/main.bundle.min.997782727eac75b1cd00ab662cb32a13720e8c99d5f2e98eff3cd38fbb1254e63f0e0d149be28b5174c4027ea603b02ca3325d4964f41805ec58e30016cafe8a.css integrity="sha512-mXeCcn6sdbHNAKtmLLMqE3IOjJnV8umO/zzTj7sSVOY/Dg0Um+KLUXTEAn6mA7AsozJdSWT0GAXsWOMAFsr+ig=="><script type=text/javascript src=/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js integrity="sha512-b0EXSzoFtoCCD+CMrb+l+3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script><script src=/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7+kfJ6kKCJxQGC+8wm+Bz9JucDjDTGNew=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.f9fe268fcd89dff326e0b5bf1183a8dd1c1ec5059292635b1eed4e4ed7b35aa272234cecb245d91be3e1b7fd9403e6e5cfcdd7d5a4fcb2c8ba11deb4b81dadd2.js integrity="sha512-+f4mj82J3/Mm4LW/EYOo3RwexQWSkmNbHu1OTtezWqJyI0zsskXZG+Pht/2UA+blz83X1aT8ssi6Ed60uB2t0g==" data-copy=复制 data-copied=已复制></script><script src=/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><link rel=stylesheet href=/css/repo-cards.min.baaf512416b3dfd1e3a95a3f3d8d49e978f267c0efded75be3635a9fd4655746757c1277a3a67d77c541de80b862394f6aacc26b32d3c09fb016a071661253e9.css integrity="sha512-uq9RJBaz39HjqVo/PY1J6XjyZ8Dv3tdb42Nan9RlV0Z1fBJ3o6Z9d8VB3oC4YjlPaqzCazLTwJ+wFqBxZhJT6Q=="><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"文章","name":"【转载】如何入门具身智能研究","headline":"【转载】如何入门具身智能研究","inLanguage":"en","url":"https://blog.xiadengma.com/posts/%E5%A6%82%E4%BD%95%E5%85%A5%E9%97%A8%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6/","author":{"@type":"Person","name":"xiadengma"},"copyrightYear":"2025","dateCreated":"2025-11-13T13:36:15\u002b08:00","datePublished":"2025-11-13T13:36:15\u002b08:00","dateModified":"2025-11-13T13:51:37\u002b08:00","keywords":["embodied-ai"],"mainEntityOfPage":"true","wordCount":"777"}]</script><script data-id=umami-script async src=https://analytics.umami.is/script.js data-website-id=678aa4f9-0d3a-485e-88ea-9a4e51d5aa9f></script><script type=text/javascript>document.querySelector('script[data-id="umami-script"]').addEventListener("load",function(){const e=document.head.querySelector('meta[property = "og:type"]').getAttribute("content");let t=document.head.querySelector('meta[property = "og:title"]').getAttribute("content"),n=document.head.querySelector('meta[property = "og:url"]').getAttribute("content");umami.track(e+":"+t,{url:n})})</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},chtml:{linebreaks:{automatic:!0,width:"container"}},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js id=MathJax-script></script><script>(()=>{function s(){const e=document.querySelectorAll('.article-content mjx-container[display="true"]');for(const s of e){const t=s.querySelector("mjx-math");if(!t)continue;t.style.transform="",t.style.transformOrigin="",t.style.display="";const n=s.getBoundingClientRect().width;if(!n||n<=0)continue;const o=t.getBoundingClientRect().width;if(o>n+1){const e=n/o;t.style.transform=`scale(${e})`,t.style.transformOrigin="0 0",t.style.display="inline-block"}}}let e=0;function t(){e&&cancelAnimationFrame(e),e=requestAnimationFrame(()=>{e=0,s()})}function n(){window.MathJax?.startup?.promise?MathJax.startup.promise.then(()=>{t(),setTimeout(t,300)}):t(),window.addEventListener("resize",t,{passive:!0})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",n):n()})()</script><script>(()=>{const n=/[A-Za-z]{2,}[^\S\r\n]+[A-Za-z]{2,}/,s=/[A-Za-z]/g,o=/[\u4E00-\u9FFF]/g;function e(e,t){const n=t.match(e);return n?n.length:0}function t(){const t=document.querySelectorAll(".article-content p, .article-content li");for(const a of t){const i=(a.textContent||"").trim();if(!i)continue;if(!n.test(i))continue;const r=e(s,i);if(r<12)continue;const c=e(o,i);if(r<=c)continue;const l=getComputedStyle(a);if(l.textAlign!=="justify")continue;a.classList.add("bf-no-justify")}}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",t):t()})()</script></head><body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
跳过正文</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 min-h-[130px] opacity-65 bg-gradient-to-b from-neutral from-60% dark:from-neutral-800 to-transparent mix-blend-normal z-80"></div><div class="fixed inset-x-0 z-100"><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32"><div class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0"><div><a href=/ class=flex><span class=sr-only>MyBlog</span>
<img src=/img/logo.png width=90 height=90 class="logo max-h-[5rem] max-w-[5rem] object-scale-down object-left nozoom" alt></a></div><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium">MyBlog</a></nav><nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12"><a href=/posts/ class="flex items-center hover:text-primary-600 dark:hover:text-primary-400" aria-label=文章 title=文章><span class=mr-1><span class="relative block icon"><svg height="1em" viewBox="0 0 512 512"><path fill="currentColor" d="M441 58.9 453.1 71c9.4 9.4 9.4 24.6.0 33.9L424 134.1 377.9 88 407 58.9c9.4-9.4 24.6-9.4 33.9.0zM209.8 256.2 344 121.9 390.1 168 255.8 302.2c-2.9 2.9-6.5 5-10.4 6.1L186.9 325l16.7-58.5c1.1-3.9 3.2-7.5 6.1-10.4zM373.1 25 175.8 222.2c-8.7 8.7-15 19.4-18.3 31.1l-28.6 1e2c-2.4 8.4-.1 17.4 6.1 23.6s15.2 8.5 23.6 6.1l1e2-28.6c11.8-3.4 22.5-9.7 31.1-18.3L487 138.9c28.1-28.1 28.1-73.7.0-101.8L474.9 25C446.8-3.1 401.2-3.1 373.1 25zM88 64C39.4 64 0 103.4.0 152V424c0 48.6 39.4 88 88 88H360c48.6.0 88-39.4 88-88V312c0-13.3-10.7-24-24-24s-24 10.7-24 24V424c0 22.1-17.9 40-40 40H88c-22.1.0-40-17.9-40-40V152c0-22.1 17.9-40 40-40H2e2c13.3.0 24-10.7 24-24s-10.7-24-24-24H88z"/></svg></span></span><p class="text-base font-medium">文章</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title="搜索 (/)">
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button></nav><div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title="搜索 (/)">
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button></div></div><div class="-my-2 md:hidden"><div id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/posts/ class="flex items-center hover:text-primary-600 dark:hover:text-primary-400" aria-label=文章 title=文章><div class=mr-2><span class="relative block icon"><svg height="1em" viewBox="0 0 512 512"><path fill="currentColor" d="M441 58.9 453.1 71c9.4 9.4 9.4 24.6.0 33.9L424 134.1 377.9 88 407 58.9c9.4-9.4 24.6-9.4 33.9.0zM209.8 256.2 344 121.9 390.1 168 255.8 302.2c-2.9 2.9-6.5 5-10.4 6.1L186.9 325l16.7-58.5c1.1-3.9 3.2-7.5 6.1-10.4zM373.1 25 175.8 222.2c-8.7 8.7-15 19.4-18.3 31.1l-28.6 1e2c-2.4 8.4-.1 17.4 6.1 23.6s15.2 8.5 23.6 6.1l1e2-28.6c11.8-3.4 22.5-9.7 31.1-18.3L487 138.9c28.1-28.1 28.1-73.7.0-101.8L474.9 25C446.8-3.1 401.2-3.1 373.1 25zM88 64C39.4 64 0 103.4.0 152V424c0 48.6 39.4 88 88 88H360c48.6.0 88-39.4 88-88V312c0-13.3-10.7-24-24-24s-24 10.7-24 24V424c0 22.1-17.9 40-40 40H88c-22.1.0-40-17.9-40-40V152c0-22.1 17.9-40 40-40H2e2c13.3.0 24-10.7 24-24s-10.7-24-24-24H88z"/></svg></span></div><p class="text-bg font-bg">文章</p></a></li></ul></div></div></div></div></div></div><script type=text/javascript src=/js/background-blur.min.605b3b942818f0ab5a717ae446135ec46b8ee5a2ad12ae56fb90dc2a76ce30c388f9fec8bcc18db15bd47e3fa8a09d779fa12aa9c184cf614a315bc72c6c163d.js integrity="sha512-YFs7lCgY8KtacXrkRhNexGuO5aKtEq5W+5DcKnbOMMOI+f7IvMGNsVvUfj+ooJ13n6EqqcGEz2FKMVvHLGwWPQ==" data-blur-id=menu-blur></script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom"><img id=background-image src=/img/background_1.svg role=presentation loading=eager decoding=async fetchpriority=high class="absolute inset-0 w-full h-full object-cover"><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-xl bg-neutral-100/75 dark:bg-neutral-800/60"></div><script type=text/javascript src=/js/background-blur.min.605b3b942818f0ab5a717ae446135ec46b8ee5a2ad12ae56fb90dc2a76ce30c388f9fec8bcc18db15bd47e3fa8a09d779fa12aa9c184cf614a315bc72c6c163d.js integrity="sha512-YFs7lCgY8KtacXrkRhNexGuO5aKtEq5W+5DcKnbOMMOI+f7IvMGNsVvUfj+ooJ13n6EqqcGEz2FKMVvHLGwWPQ==" data-blur-id=background-blur data-image-id=background-image data-image-url=/img/background_1.svg></script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class=hidden><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/>MyBlog</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/posts/>文章</a><span class="px-1 text-primary-500">/</span></li><li class=hidden><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/posts/%E5%A6%82%E4%BD%95%E5%85%A5%E9%97%A8%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD%E7%A0%94%E7%A9%B6/>【转载】如何入门具身智能研究</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">【转载】如何入门具身智能研究</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-11-13T13:36:15+08:00>2025年11月13日</time><span class="px-2 text-primary-500">&#183;</span><span>777 字</span><span class="px-2 text-primary-500">&#183;</span><span title=预计阅读>4 分钟</span></div><div class="flex flex-row flex-wrap items-center"><a class="relative mt-[0.5rem] me-2" href=/tags/embodied-ai/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Embodied-Ai</span></span></a></div></div></header><section class="bf-single-layout bf-has-toc flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="bf-article-toc order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-10"><details open id=TOCView class="toc-right mt-0 overflow-y-auto overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg -ms-5 ps-5 pe-2 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 -ms-5 ps-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">目录</summary><div class="min-w-[220px] py-2 border-dotted border-s-1 -ms-5 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#how-to-get-started-with-embodied-ai-research>How to Get Started with Embodied AI Research</a></li><li><a href=#o前言>O、前言</a></li><li><a href=#一基础概念-basic-concepts>一、基础概念 (Basic Concepts)</a><ul><li><a href=#1-什么是具身智能-what-is-embodied-ai>1. 什么是具身智能 (What is Embodied AI)</a></li><li><a href=#2-具身智能与其他-ai-的区别-differences-from-traditional-ai>2. 具身智能与其他 AI 的区别 (Differences from Traditional AI)</a></li><li><a href=#3-研究具身智能的核心原则-core-principles>3. 研究具身智能的核心原则 (Core Principles)</a></li></ul></li><li><a href=#二前置技能>二、前置技能</a></li><li><a href=#三ai-and-robotics-basis>三、AI and Robotics Basis</a><ul><li><a href=#1-intro-to-embodied-ai>1. Intro-to-Embodied-AI</a></li><li><a href=#2-intro-to-cv>2. Intro-to-CV</a></li><li><a href=#3-optional-deep-reinforcement-learning-cs285>3. (Optional) Deep Reinforcement Learning (CS285)</a></li></ul></li><li><a href=#四研究平台与工具-research-platforms-and-tools>四、研究平台与工具 (Research Platforms and Tools)</a><ul><li><a href=#1-模拟环境-simulation-environments>1. 模拟环境 (Simulation Environments)</a></li><li><a href=#2-机器人平台-robotic-platforms>2. 机器人平台 (Robotic Platforms)</a></li><li><a href=#3-embodied-ai-daily-arxiv-advanced>3. Embodied AI Daily ArXiv (Advanced)</a></li></ul></li><li><a href=#五对机器人技能的研究-research-on-robot-skills>五、对机器人技能的研究 (Research on Robot Skills)</a><ul><li><a href=#1-grasping>1. Grasping</a></li><li><a href=#2-manipulation>2. Manipulation</a></li><li><a href=#3-navigation>3. Navigation</a></li><li><a href=#4-locomotion>4. Locomotion</a></li></ul></li><li><a href=#六基于-learning-的主要研究方向>六、基于 learning 的主要研究方向</a><ul><li><a href=#1-few-shot-imitation-learning>1. Few-shot Imitation Learning</a></li><li><a href=#2-robot-foundation-model>2. Robot Foundation Model</a></li><li><a href=#3-sim-to-real-reinforcement-learning-distillation>3. Sim-to-Real Reinforcement Learning (Distillation)</a></li><li><a href=#4-real-world-reinforcement-learning>4. Real-World Reinforcement Learning</a></li><li><a href=#5-world-models>5. World Models</a></li></ul></li><li><a href=#七相关领域>七、相关领域</a><ul><li><a href=#1-graphics>1. Graphics</a></li><li><a href=#2-hardware>2. Hardware</a></li><li><a href=#3-mainstream-models>3. Mainstream Models</a></li><li><a href=#4-foundation-models>4. Foundation Models</a></li><li><a href=#5-3d-vision>5. 3D Vision</a></li></ul></li><li><a href=#八optional-科研工作中的必备能力>八、(Optional) 科研工作中的必备能力</a></li><li><a href=#相关仓库>相关仓库</a></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg -ms-5 ps-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 -ms-5 ps-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">目录</summary><div class="py-2 border-dotted border-neutral-300 border-s-1 -ms-5 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#how-to-get-started-with-embodied-ai-research>How to Get Started with Embodied AI Research</a></li><li><a href=#o前言>O、前言</a></li><li><a href=#一基础概念-basic-concepts>一、基础概念 (Basic Concepts)</a><ul><li><a href=#1-什么是具身智能-what-is-embodied-ai>1. 什么是具身智能 (What is Embodied AI)</a></li><li><a href=#2-具身智能与其他-ai-的区别-differences-from-traditional-ai>2. 具身智能与其他 AI 的区别 (Differences from Traditional AI)</a></li><li><a href=#3-研究具身智能的核心原则-core-principles>3. 研究具身智能的核心原则 (Core Principles)</a></li></ul></li><li><a href=#二前置技能>二、前置技能</a></li><li><a href=#三ai-and-robotics-basis>三、AI and Robotics Basis</a><ul><li><a href=#1-intro-to-embodied-ai>1. Intro-to-Embodied-AI</a></li><li><a href=#2-intro-to-cv>2. Intro-to-CV</a></li><li><a href=#3-optional-deep-reinforcement-learning-cs285>3. (Optional) Deep Reinforcement Learning (CS285)</a></li></ul></li><li><a href=#四研究平台与工具-research-platforms-and-tools>四、研究平台与工具 (Research Platforms and Tools)</a><ul><li><a href=#1-模拟环境-simulation-environments>1. 模拟环境 (Simulation Environments)</a></li><li><a href=#2-机器人平台-robotic-platforms>2. 机器人平台 (Robotic Platforms)</a></li><li><a href=#3-embodied-ai-daily-arxiv-advanced>3. Embodied AI Daily ArXiv (Advanced)</a></li></ul></li><li><a href=#五对机器人技能的研究-research-on-robot-skills>五、对机器人技能的研究 (Research on Robot Skills)</a><ul><li><a href=#1-grasping>1. Grasping</a></li><li><a href=#2-manipulation>2. Manipulation</a></li><li><a href=#3-navigation>3. Navigation</a></li><li><a href=#4-locomotion>4. Locomotion</a></li></ul></li><li><a href=#六基于-learning-的主要研究方向>六、基于 learning 的主要研究方向</a><ul><li><a href=#1-few-shot-imitation-learning>1. Few-shot Imitation Learning</a></li><li><a href=#2-robot-foundation-model>2. Robot Foundation Model</a></li><li><a href=#3-sim-to-real-reinforcement-learning-distillation>3. Sim-to-Real Reinforcement Learning (Distillation)</a></li><li><a href=#4-real-world-reinforcement-learning>4. Real-World Reinforcement Learning</a></li><li><a href=#5-world-models>5. World Models</a></li></ul></li><li><a href=#七相关领域>七、相关领域</a><ul><li><a href=#1-graphics>1. Graphics</a></li><li><a href=#2-hardware>2. Hardware</a></li><li><a href=#3-mainstream-models>3. Mainstream Models</a></li><li><a href=#4-foundation-models>4. Foundation Models</a></li><li><a href=#5-3d-vision>5. 3D Vision</a></li></ul></li><li><a href=#八optional-科研工作中的必备能力>八、(Optional) 科研工作中的必备能力</a></li><li><a href=#相关仓库>相关仓库</a></li></ul></nav></div></details><script>(function(){"use strict";const s=.33,o="#TableOfContents",i=".anchor",a='a[href^="#"]',r="li ul",c="active";let t=!1;function l(e,n){const o=window.scrollY+window.innerHeight*n,i=[...document.querySelectorAll('#TableOfContents a[href^="#"]')],s=new Set(i.map(e=>e.getAttribute("href").substring(1)));if(t)for(let t=0;t<e.length;t++){const n=e[t];if(!s.has(n.id))continue;const o=n.getBoundingClientRect().top+window.scrollY;if(Math.abs(window.scrollY-o)<100)return n.id}for(let t=e.length-1;t>=0;t--){const n=e[t].getBoundingClientRect().top+window.scrollY;if(n<=o&&s.has(e[t].id))return e[t].id}return e.find(e=>s.has(e.id))?.id||""}function e({toc:e,anchors:t,links:n,scrollOffset:s,collapseInactive:o}){const i=l(t,s);if(!i)return;if(n.forEach(e=>{const t=e.getAttribute("href")===`#${i}`;if(e.classList.toggle(c,t),o){const n=e.closest("li")?.querySelector("ul");n&&(n.style.display=t?"":"none")}}),o){const n=e.querySelector(`a[href="#${CSS.escape(i)}"]`);let t=n;for(;t&&t!==e;)t.tagName==="UL"&&(t.style.display=""),t.tagName==="LI"&&t.querySelector("ul")?.style.setProperty("display",""),t=t.parentElement}}function n(){const n=document.querySelector(o);if(!n)return;const l=!0,u=[...document.querySelectorAll(i)],d=[...n.querySelectorAll(a)];l&&n.querySelectorAll(r).forEach(e=>e.style.display="none"),d.forEach(e=>{e.addEventListener("click",()=>{t=!0})});const c={toc:n,anchors:u,links:d,scrollOffset:s,collapseInactive:l};window.addEventListener("scroll",()=>e(c),{passive:!0}),window.addEventListener("hashchange",()=>e(c),{passive:!0}),e(c)}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",n):n()})()</script></div></div><div class="bf-article-main min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><h2 class="relative group">How to Get Started with Embodied AI Research<div id=how-to-get-started-with-embodied-ai-research class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#how-to-get-started-with-embodied-ai-research aria-label=锚点>#</a></span></h2><blockquote class="book-hint info"><div class=hint-content>转载时间：2025-11-13</div></blockquote><p>© PKU EPIC Lab. All rights reserved. Commercial distribution prohibited.<br>© PKU EPIC Lab. 版权所有。禁止商业传播。</p><p><a href=https://jiangranlv.notion.site/How-to-Get-Started-with-Embodied-AI-Research-252a467c9b60804d85dcfeffcc7771fb target=_blank>原文主页</a></p><p><strong>作者：吕江燃，张嘉曌，邓胜亮，陈嘉毅，严汨，李忆唐</strong><br><strong>指导老师：王鹤，弋力</strong><br></p><h2 class="relative group">O、前言<div id=o前言 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#o%e5%89%8d%e8%a8%80 aria-label=锚点>#</a></span></h2><p>随着具身智能的关注度不断升高，越来越多的研究者涌入这一领域，相关论文数量也呈现井喷式增长。然而，其中不少工作的质量令人担忧：有的只是单纯“讲故事”，有的则一味追求“刷榜”，但这些却反而获得了大量的关注和追捧。在这样的科研环境下，为新同学提供正确的引导，帮助他们少走弯路、健康成长，是一个重要的任务。</p><p>因此，我们撰写本文的目的，就是希望为刚进入具身智能科研领域的同学提供一个清晰的 guide，帮助大家理解具身智能究竟该研究什么，以及如何正确地入门。希望能帮助到初学者积累必要基础知识的同时，建立起正确的研究认知。</p><p>本文章也将作为 PKU EPIC Lab 本科生的入门材料，并会在实践和培养的过程中不断更新和完善。</p><h2 class="relative group">一、基础概念 (Basic Concepts)<div id=一基础概念-basic-concepts class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e4%b8%80%e5%9f%ba%e7%a1%80%e6%a6%82%e5%bf%b5-basic-concepts aria-label=锚点>#</a></span></h2><h3 class="relative group">1. 什么是具身智能 (What is Embodied AI)<div id=1-什么是具身智能-what-is-embodied-ai class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#1-%e4%bb%80%e4%b9%88%e6%98%af%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd-what-is-embodied-ai aria-label=锚点>#</a></span></h3><p>具身智能（Embodied AI）是指能够在物理或虚拟环境中<strong>通过感知、行动和交互</strong>来学习与完成任务的人工智能。不同于仅在静态数据（文本、图像、语音等）上进行训练和推理的传统 AI，具身智能的智能体（agent）往往有一个“身体”（body）或“化身”（avatar），它们可以与环境交互，改变环境，并随着环境的改变自己作出调整。</p><p>典型的具身智能研究对象包括机器人和虚拟环境中的智能体，本文主要面向机器人领域(Robotics)。</p><p><strong>核心特征：</strong></p><ul><li>拥有多模态感知能力（视觉、触觉、语音等）</li><li>能够执行动作并影响环境</li><li>学习可以通过<strong>与环境交互</strong>而不仅仅是被动监督完成</li></ul><h3 class="relative group">2. 具身智能与其他 AI 的区别 (Differences from Traditional AI)<div id=2-具身智能与其他-ai-的区别-differences-from-traditional-ai class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#2-%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e4%b8%8e%e5%85%b6%e4%bb%96-ai-%e7%9a%84%e5%8c%ba%e5%88%ab-differences-from-traditional-ai aria-label=锚点>#</a></span></h3><p>具身智能与传统 AI 的主要区别在于它的<strong>主动性、交互性，以及对动作数据的依赖</strong>。传统 AI 可以利用互联网上丰富的图像、文本、语音等大规模数据集进行训练（参考 LLM 的成功），而具身智能体所需的动作数据必须通过与环境的真实交互来收集，这使得数据获取代价高昂且规模有限。一言以蔽之，数据问题是具身智能目前最大的 bottleneck。那么很自然的两个关键问题是，</p><ul><li>如何 scale up 机器人数据？
例如：<a href=https://pku-epic.github.io/GraspVLA-web/ target=_blank>GraspVLA</a>（在仿真中以合成的方式猛猛造）, <a href=https://www.physicalintelligence.company/blog/pi0 target=_blank>pi0</a>和<a href=https://agibot-world.com/ target=_blank>AgiBot-World</a>（在真实世界猛猛遥操采）, <a href=https://umi-gripper.github.io/ target=_blank>UMI</a>和<a href=https://airexo.github.io/ target=_blank>AirExo</a>（可穿戴设备，如外骨骼的高效数据采集装置）</li><li>在不能 scale up 机器人数据的情况下，如何利用好已有的数据实现你的目的？
例如：<a href=https://diffusion-policy.cs.columbia.edu/ target=_blank>Diffusion Policy</a> (100 条机器人数据训一个特定任务的 policy）, <a href=https://arxiv.org/pdf/2507.15597 target=_blank>Being-H0</a>（利用 human video 参与 policy 训练），<a href=https://mimicgen.github.io/ target=_blank>MimicGen</a>、<a href=https://demo-generation.github.io/ target=_blank>DemoGen</a>、<a href=https://yangsizhe.github.io/robosplat/ target=_blank>Robosplat</a>（从一条机器人轨迹中 augment 得到更多数据）</li></ul><h3 class="relative group">3. 研究具身智能的核心原则 (Core Principles)<div id=3-研究具身智能的核心原则-core-principles class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#3-%e7%a0%94%e7%a9%b6%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd%e7%9a%84%e6%a0%b8%e5%bf%83%e5%8e%9f%e5%88%99-core-principles aria-label=锚点>#</a></span></h3><ul><li><p><strong>首先把任务定义（task formulation）想清楚，而不是一开始就盯着模型</strong>。在 CV 领域，研究者之所以可以直接关注模型，是因为任务往往已经被定义得很清晰，数据集也由他人整理好， 比如图像分类就是输入图片输出类别标签，检测就是输出四个数的 bounding box；</p><p>但在具身智能中，如何合理地建模任务、确定目标与评价指标，往往比模型选择更为关键。说白了，你得知道你想让机器人学会什么样的技能，输入是啥，输出是啥，用的什么传感器？你所研究的问题是否在合理的 setting 下？有没有有可能通过更好的 setting 来解决问题（比如机器人头部相机对场景观测不全，那我们可以考虑加装腕部相机，或者使用鱼眼相机）</p></li><li><p>必须认识到<strong>用学习（learning）来解决机器人问题并不是理所当然的选择</strong>。在许多场景中，传统的控制（Control）、规划（Planning）或优化方法（Optimization）依然高效且可靠，而学习方法更多是在任务复杂、环境多变(泛化性) 或缺乏解析建模手段时才展现优势。因此，做具身智能研究时，首先要想回答，为什么你研究的这件事传统 robotics 解决不了？为什么非得用 learning？</p></li></ul><h2 class="relative group">二、前置技能<div id=二前置技能 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e4%ba%8c%e5%89%8d%e7%bd%ae%e6%8a%80%e8%83%bd aria-label=锚点>#</a></span></h2><p>这些工具是一个当代 CS researcher 的必备技能（不局限于方向），主要学习资料可以参考
<a href=https://missing-semester-cn.github.io/ target=_blank>https://missing-semester-cn.github.io/</a></p><ul><li>Python, Conda and Pytorch</li><li>Linux Shell, Git, SSH</li><li>LLM, Cursor</li><li>Docker</li></ul><h2 class="relative group">三、AI and Robotics Basis<div id=三ai-and-robotics-basis class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e4%b8%89ai-and-robotics-basis aria-label=锚点>#</a></span></h2><p>以下三门课是基础课程，对于初学者希望能详细的掌握内容，不要“不求甚解”，对于课程 Lab 的 project 最好做到完整实现，而不仅局限于做“代码填空”。</p><h3 class="relative group">1. <a href=https://pku-epic.github.io/Intro2EAI_2025/schedule/ target=_blank>Intro-to-Embodied-AI</a><div id=1-intro-to-embodied-ai class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#1-intro-to-embodied-ai aria-label=锚点>#</a></span></h3><p>实验室内部课程，主要内容是 robotics 的基础概念和基于 learning 的 robotics，源于王鹤老师《具身智能导论》，外界同学自行寻找类似课程替代</p><h3 class="relative group">2. <a href=https://pku-epic.github.io/Intro2CV_2025/schedule/ target=_blank>Intro-to-CV</a><div id=2-intro-to-cv class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#2-intro-to-cv aria-label=锚点>#</a></span></h3><p>实验室内部课程，主要内容是 cv 基础和 deep learning，源于王鹤老师《计算机视觉导论》，外界同学可以学习 Stanford CS231N 替代</p><h3 class="relative group">3. (Optional) Deep Reinforcement Learning (CS285)<div id=3-optional-deep-reinforcement-learning-cs285 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#3-optional-deep-reinforcement-learning-cs285 aria-label=锚点>#</a></span></h3><p>Berkeley 的 RL 课程，涵盖了 Imitation Learning，Online RL, Offline RL 等 Policy Learning 范式</p><h2 class="relative group">四、研究平台与工具 (Research Platforms and Tools)<div id=四研究平台与工具-research-platforms-and-tools class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e5%9b%9b%e7%a0%94%e7%a9%b6%e5%b9%b3%e5%8f%b0%e4%b8%8e%e5%b7%a5%e5%85%b7-research-platforms-and-tools aria-label=锚点>#</a></span></h2><h3 class="relative group">1. 模拟环境 (Simulation Environments)<div id=1-模拟环境-simulation-environments class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#1-%e6%a8%a1%e6%8b%9f%e7%8e%af%e5%a2%83-simulation-environments aria-label=锚点>#</a></span></h3><p><strong>Simulation 的意义</strong>：在大多情况下，真机部署机器人是不方便的，所以 simulation 提供了一个很好的代替。主要有两大用途，1.作为高效的数据源，解决真实世界机器人数据少的问题 2.真机测试 policy 不方便，很难复现，作为一个更通用的 benchmark evaluation 平台</p><p><strong>对于 simulation 的掌握</strong>：需要至少一种 simulation 框架，通过阅读 tutorial 跑他的 example，加深对 robotics 的理解，不要等到上真机才发现有很多坑，会有很大的安全隐患</p><p>IssacLab (Recommend)</p><p><a href=https://isaac-sim.github.io/IsaacLab/main/index.html target=_blank>https://isaac-sim.github.io/IsaacLab/main/index.html</a></p><p><a href=https://playground.mujoco.org/ target=_blank>https://playground.mujoco.org/</a></p><h3 class="relative group">2. 机器人平台 (Robotic Platforms)<div id=2-机器人平台-robotic-platforms class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#2-%e6%9c%ba%e5%99%a8%e4%ba%ba%e5%b9%b3%e5%8f%b0-robotic-platforms aria-label=锚点>#</a></span></h3><p>真机实验下的机械臂通讯接口，至少熟悉一类常用的 API</p><p>基于 ros1 的 franka 通讯：</p><div class=github-card-wrapper><a id=github-57a169a505d4999db82b62512685b5a5 target=_blank href=https://github.com/rail-berkeley/serl_franka_controllers class=cursor-pointer><div class="w-full md:w-auto p-0 m-0 border border-neutral-200 dark:border-neutral-700 border rounded-md shadow-2xl"><div class="w-full nozoom"><img src=https://opengraph.githubassets.com/0/rail-berkeley/serl_franka_controllers alt="GitHub Repository Thumbnail" class="nozoom mt-0 mb-0 w-full h-full object-cover"></div><div class="w-full md:w-auto pt-3 p-5"><div class="flex items-center"><span class="text-2xl text-neutral-800 dark:text-neutral me-2"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><div id=github-57a169a505d4999db82b62512685b5a5-full_name class="m-0 font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral">rail-berkeley/serl_franka_controllers</div></div><p id=github-57a169a505d4999db82b62512685b5a5-description class="m-0 mt-2 text-md text-neutral-800 dark:text-neutral">Cartesian impedance controller with reference limiting for Franka Emika Robot</p><div class="m-0 mt-2 flex items-center"><span class="mr-1 inline-block h-3 w-3 rounded-full language-dot" data-language=C++></span><div class="m-0 mr-5 text-md text-neutral-800 dark:text-neutral">C++</div><span class="text-md mr-1 text-neutral-800 dark:text-neutral"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentColor" d="M287.9.0C297.1.0 305.5 5.25 309.5 13.52L378.1 154.8l153.3 22.7C540.4 178.8 547.8 185.1 550.7 193.7 553.5 202.4 551.2 211.9 544.8 218.2L433.6 328.4l26.3 155.5C461.4 492.9 457.7 502.1 450.2 507.4 442.8 512.7 432.1 513.4 424.9 509.1L287.9 435.9 150.1 509.1C142.9 513.4 133.1 512.7 125.6 507.4 118.2 502.1 114.5 492.9 115.1 483.9l27.1-155.5L31.11 218.2C24.65 211.9 22.36 202.4 25.2 193.7 28.03 185.1 35.5 178.8 44.49 177.5L197.7 154.8 266.3 13.52C270.4 5.249 278.7.0 287.9.0zm0 78.95L235.4 187.2C231.9 194.3 225.1 199.3 217.3 200.5L98.98 217.9 184.9 303C190.4 308.5 192.9 316.4 191.6 324.1L171.4 443.7l105.2-56.2C283.7 383.7 292.2 383.7 299.2 387.5l105.2 56.2-20.2-119.6C382.9 316.4 385.5 308.5 391 303l85.9-85.1-118.3-17.4C350.7 199.3 343.9 194.3 340.5 187.2L287.9 78.95z"/></svg></span></span><div id=github-57a169a505d4999db82b62512685b5a5-stargazers class="m-0 mr-5 text-md text-neutral-800 dark:text-neutral">187</div><span class="text-md mr-1 text-neutral-800 dark:text-neutral"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M80 104c13.3.0 24-10.7 24-24S93.3 56 80 56 56 66.7 56 80s10.7 24 24 24zm80-24c0 32.8-19.7 61-48 73.3V192c0 17.7 14.3 32 32 32H304c17.7.0 32-14.3 32-32V153.3C307.7 141 288 112.8 288 80c0-44.2 35.8-80 80-80s80 35.8 80 80c0 32.8-19.7 61-48 73.3V192c0 53-43 96-96 96H256v70.7c28.3 12.3 48 40.5 48 73.3.0 44.2-35.8 80-80 80s-80-35.8-80-80c0-32.8 19.7-61 48-73.3V288H144c-53 0-96-43-96-96V153.3C19.7 141 0 112.8.0 80 0 35.8 35.8.0 80 0s80 35.8 80 80zm208 24c13.3.0 24-10.7 24-24s-10.7-24-24-24-24 10.7-24 24 10.7 24 24 24zM248 432c0-13.3-10.7-24-24-24s-24 10.7-24 24 10.7 24 24 24 24-10.7 24-24z"/></svg></span></span><div id=github-57a169a505d4999db82b62512685b5a5-forks class="m-0 mr-5 text-md text-neutral-800 dark:text-neutral">27</div></div></div></div><script async type=text/javascript src=/js/fetch-repo.min.dc5533c50cefd50405344b235937142271f26229fe39cbee27fd4960e8bb897a0beebfad77a1091ca91cd0d1fb14e70fc37cc114dd9674fb2c32e0ab512ec8a4.js integrity="sha512-3FUzxQzv1QQFNEsjWTcUInHyYin+OcvuJ/1JYOi7iXoL7r+td6EJHKkc0NH7FOcPw3zBFN2WdPssMuCrUS7IpA==" data-repo-url=https://api.github.com/repos/rail-berkeley/serl_franka_controllers data-repo-id=github-57a169a505d4999db82b62512685b5a5></script></a></div><div class="expand-wrapper prose dark:prose-invert max-w-prose zen-mode-content"><input id=expand-2 class=expand-toggle type=checkbox>
<label for=expand-2 class=expand-title><span class=expand-icon><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-down"><polyline points="6 9 12 15 18 9"/></svg>
</span>原始地址</label><div class=expand-content><div class=expand-inner><a href=https://github.com/rail-berkeley/serl_franka_controllers target=_blank>https://github.com/rail-berkeley/serl_franka_controllers</a></div></div></div><h3 class="relative group">3. Embodied AI Daily ArXiv (Advanced)<div id=3-embodied-ai-daily-arxiv-advanced class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#3-embodied-ai-daily-arxiv-advanced aria-label=锚点>#</a></span></h3><p>具身智能每日最新的论文，按 manipulation，VLA， dexterous，humanoid 等关键词进行划分，推荐基础入门后的同学每日最终最新进展，丰富自己的认知和视野</p><p>你可以访问<a href=https://jiangranlv.github.io/robotics_arXiv_daily/ target=_blank>每日具身智能论文</a>查看最新进展。</p><div class=github-card-wrapper><a id=github-40c2f3100f7f2ed0a6ab0a1db55eaae6 target=_blank href=https://github.com/jiangranlv/robotics_arXiv_daily class=cursor-pointer><div class="w-full md:w-auto p-0 m-0 border border-neutral-200 dark:border-neutral-700 border rounded-md shadow-2xl"><div class="w-full nozoom"><img src=https://opengraph.githubassets.com/0/jiangranlv/robotics_arXiv_daily alt="GitHub Repository Thumbnail" class="nozoom mt-0 mb-0 w-full h-full object-cover"></div><div class="w-full md:w-auto pt-3 p-5"><div class="flex items-center"><span class="text-2xl text-neutral-800 dark:text-neutral me-2"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><div id=github-40c2f3100f7f2ed0a6ab0a1db55eaae6-full_name class="m-0 font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral">jiangranlv/robotics_arXiv_daily</div></div><p id=github-40c2f3100f7f2ed0a6ab0a1db55eaae6-description class="m-0 mt-2 text-md text-neutral-800 dark:text-neutral">Fetching Embodied AI Paper from ArXiv automatically</p><div class="m-0 mt-2 flex items-center"><span class="mr-1 inline-block h-3 w-3 rounded-full language-dot" data-language=Python></span><div class="m-0 mr-5 text-md text-neutral-800 dark:text-neutral">Python</div><span class="text-md mr-1 text-neutral-800 dark:text-neutral"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentColor" d="M287.9.0C297.1.0 305.5 5.25 309.5 13.52L378.1 154.8l153.3 22.7C540.4 178.8 547.8 185.1 550.7 193.7 553.5 202.4 551.2 211.9 544.8 218.2L433.6 328.4l26.3 155.5C461.4 492.9 457.7 502.1 450.2 507.4 442.8 512.7 432.1 513.4 424.9 509.1L287.9 435.9 150.1 509.1C142.9 513.4 133.1 512.7 125.6 507.4 118.2 502.1 114.5 492.9 115.1 483.9l27.1-155.5L31.11 218.2C24.65 211.9 22.36 202.4 25.2 193.7 28.03 185.1 35.5 178.8 44.49 177.5L197.7 154.8 266.3 13.52C270.4 5.249 278.7.0 287.9.0zm0 78.95L235.4 187.2C231.9 194.3 225.1 199.3 217.3 200.5L98.98 217.9 184.9 303C190.4 308.5 192.9 316.4 191.6 324.1L171.4 443.7l105.2-56.2C283.7 383.7 292.2 383.7 299.2 387.5l105.2 56.2-20.2-119.6C382.9 316.4 385.5 308.5 391 303l85.9-85.1-118.3-17.4C350.7 199.3 343.9 194.3 340.5 187.2L287.9 78.95z"/></svg></span></span><div id=github-40c2f3100f7f2ed0a6ab0a1db55eaae6-stargazers class="m-0 mr-5 text-md text-neutral-800 dark:text-neutral">190</div><span class="text-md mr-1 text-neutral-800 dark:text-neutral"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M80 104c13.3.0 24-10.7 24-24S93.3 56 80 56 56 66.7 56 80s10.7 24 24 24zm80-24c0 32.8-19.7 61-48 73.3V192c0 17.7 14.3 32 32 32H304c17.7.0 32-14.3 32-32V153.3C307.7 141 288 112.8 288 80c0-44.2 35.8-80 80-80s80 35.8 80 80c0 32.8-19.7 61-48 73.3V192c0 53-43 96-96 96H256v70.7c28.3 12.3 48 40.5 48 73.3.0 44.2-35.8 80-80 80s-80-35.8-80-80c0-32.8 19.7-61 48-73.3V288H144c-53 0-96-43-96-96V153.3C19.7 141 0 112.8.0 80 0 35.8 35.8.0 80 0s80 35.8 80 80zm208 24c13.3.0 24-10.7 24-24s-10.7-24-24-24-24 10.7-24 24 10.7 24 24 24zM248 432c0-13.3-10.7-24-24-24s-24 10.7-24 24 10.7 24 24 24 24-10.7 24-24z"/></svg></span></span><div id=github-40c2f3100f7f2ed0a6ab0a1db55eaae6-forks class="m-0 mr-5 text-md text-neutral-800 dark:text-neutral">14</div></div></div></div><script async type=text/javascript src=/js/fetch-repo.min.dc5533c50cefd50405344b235937142271f26229fe39cbee27fd4960e8bb897a0beebfad77a1091ca91cd0d1fb14e70fc37cc114dd9674fb2c32e0ab512ec8a4.js integrity="sha512-3FUzxQzv1QQFNEsjWTcUInHyYin+OcvuJ/1JYOi7iXoL7r+td6EJHKkc0NH7FOcPw3zBFN2WdPssMuCrUS7IpA==" data-repo-url=https://api.github.com/repos/jiangranlv/robotics_arXiv_daily data-repo-id=github-40c2f3100f7f2ed0a6ab0a1db55eaae6></script></a></div><div class="expand-wrapper prose dark:prose-invert max-w-prose zen-mode-content"><input id=expand-4 class=expand-toggle type=checkbox>
<label for=expand-4 class=expand-title><span class=expand-icon><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-down"><polyline points="6 9 12 15 18 9"/></svg>
</span>原始地址</label><div class=expand-content><div class=expand-inner><a href=https://github.com/jiangranlv/robotics_arXiv_daily target=_blank>https://github.com/jiangranlv/robotics_arXiv_daily</a></div></div></div><h2 class="relative group">五、对机器人技能的研究 (Research on Robot Skills)<div id=五对机器人技能的研究-research-on-robot-skills class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e4%ba%94%e5%af%b9%e6%9c%ba%e5%99%a8%e4%ba%ba%e6%8a%80%e8%83%bd%e7%9a%84%e7%a0%94%e7%a9%b6-research-on-robot-skills aria-label=锚点>#</a></span></h2><h3 class="relative group">1. Grasping<div id=1-grasping class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#1-grasping aria-label=锚点>#</a></span></h3><p>抓取（<strong>Grasping</strong>）是机器人学中最基础且最重要的任务之一，通常指让机器人末端牢牢抓紧物体以达到力闭合（<strong>force closure</strong>），成功完成抓取后可将物体视作机器人的一部分进行后续的移动和操作。</p><p>常见任务有（难度依次递增）：</p><ul><li><strong>Single object grasping（单物体抓取）</strong>：抓取一个物体，物体通常放在桌子上。</li><li><strong>Clutter scene grasping（堆叠场景抓取）</strong>：抓取堆叠场景中的物体，通常要求清台（全部抓完）。难点在物体的互相遮挡和干扰。</li><li><strong>Functional grasping（带语义抓取）</strong>：根据语言指令进行抓取。对于单物体抓取而言，语言通常指定物体要抓的 part 和抓取的手势；对于堆叠场景而言，还可以指定要抓的物体。难点在语言模态的引入。</li></ul><p>常用机械手末端有（难度依次递增）：</p><ul><li><strong>Suction cup（吸盘）</strong>：控制维度最低，除了末端整体的旋转和平移的自由度之外，只有是否施加吸力的 0/1 控制信号。</li><li><strong>Parallel gripper（平行夹抓）</strong>：类似吸盘。学术上通常认为吸盘/平行夹抓+堆叠场景抓取已经被<a href=https://arxiv.org/pdf/1703.09312 target=_blank>DexNet</a>和<a href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Fang_GraspNet-1Billion_A_Large-Scale_Benchmark_for_General_Object_Grasping_CVPR_2020_paper.pdf target=_blank>GraspNet</a>两个系列工作几乎解决（思路：大规模仿真抓取位姿 + 学习位姿预测网络 + sim2real）</li><li><strong>Multi-fingered hand（多指手）</strong>，又称<strong>Dexterous hand（灵巧手）</strong>：更高的可控自由度和更高的潜力，但也极大地增加了数据构造与学习的难度，导致其发展远落后于前两者。大规模仿真抓取位姿的进展/Dataset：<a href=https://pku-epic.github.io/DexGraspNet/ target=_blank>DexGraspNet</a>、<a href=https://arxiv.org/pdf/2504.18829 target=_blank>Dexonomy</a>（覆盖多样化手型）。</li></ul><p>常见的做法：</p><ul><li><strong>Open-loop methods（开环执行）</strong>：通过一次性预测抓取位姿并直接执行，不依赖执行过程中的感知反馈。可以直观理解为“看一次决定怎么抓”，执行时全程不再依赖视觉，仅依靠运动规划达到目标位姿。因此开环方法的核心是 <strong>grasping pose estimation</strong>。<strong>Data Source</strong>：Grasp Synthesis，如 <a href=https://arxiv.org/pdf/1703.09312 target=_blank>DexNet</a>、<a href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Fang_GraspNet-1Billion_A_Large-Scale_Benchmark_for_General_Object_Grasping_CVPR_2020_paper.pdf target=_blank>GraspNet-1B</a>. <strong>Learning Approaches</strong>：GSNet。</li><li><strong>Closed-loop methods（闭环执行）</strong>：在执行过程中持续使用视觉或触觉反馈进行动态调整，从而提升抓取的鲁棒性。这类闭环模型可视为 <strong>policy</strong>，持续输入视觉信息并输出机械臂动作。代表工作：<a href=https://pku-epic.github.io/GraspVLA-web/ target=_blank>GraspVLA</a>。</li></ul><h3 class="relative group">2. Manipulation<div id=2-manipulation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#2-manipulation aria-label=锚点>#</a></span></h3><p>操作（<strong>Manipulation</strong>）比抓取的含义更广，允许手和物体间有频繁的接触点变化，不像抓取任务中接触点形成后就固定不变了。通常只要是改变了物体状态的任务就可以叫操作。</p><ul><li><strong>Articulated Object Manipulation</strong>：铰链物体操作（如开门、拉抽屉、开柜子）。该类任务通常被简化成抓取任务来处理：1.Part 理解（<a href=https://pku-epic.github.io/GAPartNet/ target=_blank>GAPartNet</a>）2.抓取（Grasping）3.抓取后的操作轨迹规划 4.拉取力度控制（Impedance Control）</li><li><strong>Deformable Object Manipulation</strong>：柔性物体操作（如叠衣服、挂衣服）。难点在于柔性物体自由度极高、难以精确建模和仿真。常见做法通常基于人工设计的原子操作（action primitives），最近也有一些公司（pai，dyna）开始用数采+端到端学习的方式来直接做。</li><li><strong>Non-prehensile Manipulation</strong>：非抓握操作，指通过推、拨、翻转等方式在无抓握的情况下操控物体至指定姿态。难点在于 <strong>contact-rich</strong> 的动力学特性，机器人、物体与环境存在多重接触与碰撞，如何生成成功的操作轨迹是当前研究重点。</li><li><strong>Dexterous Manipulation</strong>：灵巧操作，与 non-prehensile 类似，但通常有更多的 contact 和更高的控制维度。一个经典的任务是 in-hand reorientation，虽然它已经几乎被 RL 解决，但如何提升学习效率、拓展到更一般的灵巧操作任务上依旧是研究难点。</li><li><strong>Bimanual Manipulation</strong>：双臂操作，重点在于如何实现双臂的协调与配合。</li><li><strong>Mobile Manipulation</strong>：移动操作，强调移动系统为操作提供更大、更灵活的工作空间，移动如何为操作服务，两者如何协同</li></ul><h3 class="relative group">3. Navigation<div id=3-navigation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#3-navigation aria-label=锚点>#</a></span></h3><p><strong>Navigation</strong> 导航研究机器人如何在物理环境中移动，以完成给定任务。导航能力是一种综合能力，从高层次来看，包括对视觉、深度信息和指令的理解，以及对历史信息（如地图、Tokens 等）的建模；从低层次来看，还包含路径规划与避障。导航通常涉及场景级别的移动，是硬件、传感器与控制算法综合能力的体现。</p><p>常见任务包括：</p><ul><li><strong>Point Goal Navigation (PointNav)</strong>：给定目标点坐标或相对方向，机器人需从起始位置导航至目标点。不涉及语义理解，属于低层任务。</li><li><strong>Object Goal Navigation (ObjectNav)</strong>：根据目标物体类别（如“椅子”），在未知环境中寻找并导航至目标物体。</li><li><strong>Vision-Language Navigation (VLN)</strong>：根据自然语言指令（如“走到厨房的桌子旁”），结合视觉感知完成导航任务。</li><li><strong>Embodied Question Answering (EQA)</strong>：机器人需在环境中探索、感知并回答与场景相关的问题（如“卧室里有几张床？”）。</li><li><strong>Tracking</strong>：机器人持续感知并跟随动态目标（如人或移动物体）。</li></ul><p>常见做法：</p><ul><li><strong>Map-based Navigation</strong>, 基于地图的导航算法会利用深度图，里程计等信息构建地图，从而基于地图规划路径完成导航任务。基于地图的方法在静态或者易结构化的场景下表现非常好。相关工作包括: <a href=https://arxiv.org/abs/2007.00643 target=_blank>Object Goal Navigation using Goal-Oriented Semantic Exploration</a></li><li><strong>Prompting-Large-Model Navigation</strong>，通过对物理世界进行解释得到 prompting，然后以现成（off-the-shelf）的大模型作为规划决策的中心。这种方法不需要训练复杂的大模型，且可以利用大模型的智能优势实现复杂的导航任务。相关工作包括: <a href=https://arxiv.org/abs/2305.16986 target=_blank>NavGPT</a>, <a href=https://yhancao.github.io/CogNav/ target=_blank>CogNav</a></li><li><strong>Video-based VLM Navigation</strong>, 通过端到端训练基于视频输入的视觉语言大模型，通过 tokens 来建模导航历史，和用 VLM 直接输出未来导航动作。相关工作<a href=https://pku-epic.github.io/NaVid/ target=_blank>NaVid</a></li></ul><p><strong>Unified Embodied Navigation</strong>：最新研究趋势是将多种导航任务统一建模，常使用纯 RGB 输入，并将目标描述转换为语言指令。代表性工作：<strong><a href=https://pku-epic.github.io/Uni-NaVid/ target=_blank>Uni-Navid</a></strong>，统一多种导航任务。<strong><a href=https://pku-epic.github.io/NavFoM-Web/ target=_blank>NavFoM</a></strong>,统一导航任务和 embodiment。</p><h3 class="relative group">4. Locomotion<div id=4-locomotion class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#4-locomotion aria-label=锚点>#</a></span></h3><p><strong>Locomotion</strong> 强调机器人在多样环境中的运动与机动能力。狭义上通常指基于 <strong>Whole-body Control (WBC)</strong> 的控制方法，用于实现 <strong>四足（Quadrupedal）</strong> 与 <strong>双足（Bipedal / Humanoid）</strong> 运动。</p><p>技术路线上，2019 年以前主要靠传统的 MPC 控制实现（例如波士顿动力），目前主流的方法是 Sim2Real RL, 以下主要讨论这类主流范式。
既然谈及 RL，又分为</p><ul><li><strong>Learning from manually designed reward</strong> 自己写 reward 提供 desired behavior, 例如<a href=https://arxiv.org/pdf/2406.06005 target=_blank>WoCoCo</a>&mdash;-任务目的：通过 reward 设计让机器人完成某些特定任务</li><li><strong>Learning from human data</strong> (data 提供 desired behavior，也叫做 tracking，例如<a href=https://agile.human2humanoid.com/ target=_blank>ASAP</a>)&mdash;-任务目的：模仿某一段人类数据中的动作（输入：现在的 state 和目标的 state；输出这一步的 action）</li></ul><p>如果人形机器人能完成对特定人类动作的 tracking，那么接下来就有了一个很主流的研究方向，general motion tracking -> whole-body teleopration，人在做任何一段动作的时候，机器人可以复现人的动作（这里的难点就很多了，动作输入形式的多样性，减少延时，长程复现人的动作，复现的精准度）
这一系列的工作是 H2O, OmniH2O, HOMIE, TWIST, CLONE, HOVER, GMT, Unitrack 等等，至此 Control 最基本的问题应该 well-defined 了</p><p>下一个阶段会涉及到一点除了 control 之外的东西，就是</p><ul><li>引入【视觉】实现户外自主化（perceptive locomotion）；例如，根据视觉来进行上楼梯，迈台阶，难点：vision sim2real <a href=https://visualmimic.github.io/ target=_blank>VisualMimic</a></li><li>引入【物体】实现 loco-manipulation；例如人型机器人搬箱子，难点：物体的 dynamics. <a href=https://hdmi-humanoid.github.io/#/ target=_blank>HDMI</a></li><li>对上述两种 task 的组合</li><li>强调【语义的泛化性】，希望能根据各种各样的场景/物体【自主决策】做出相应的动作（whole body VLA）<a href=https://ember-lab-berkeley.github.io/LeVERB-Website/ target=_blank>LeVERB</a></li><li>强调一些特殊的 capability（比如<a href=https://hub-robot.github.io/ target=_blank>HuB</a>做极端平衡，<a href=https://zzk273.github.io/Any2Track/ target=_blank>Any2Track</a>受很大的力干扰摔不倒, <a href=https://humanoid-table-tennis.github.io/ target=_blank>HITTER</a>做一个特殊的乒乓球 task）</li></ul><h2 class="relative group">六、基于 learning 的主要研究方向<div id=六基于-learning-的主要研究方向 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e5%85%ad%e5%9f%ba%e4%ba%8e-learning-%e7%9a%84%e4%b8%bb%e8%a6%81%e7%a0%94%e7%a9%b6%e6%96%b9%e5%90%91 aria-label=锚点>#</a></span></h2><h3 class="relative group">1. Few-shot Imitation Learning<div id=1-few-shot-imitation-learning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#1-few-shot-imitation-learning aria-label=锚点>#</a></span></h3><p>该方向主要聚焦于 <strong>小模型 (small-model)</strong> 场景：给定一个特定任务，以及数量有限的专家轨迹数据集（比如 50 条轨迹），学习一个策略来模仿专家轨迹完成任务。能够在一定范围内实现泛化，例如在同一张桌面上对同一物体的不同初始位置泛化。</p><ul><li><p><strong>传统方法</strong>：<a href=https://cgi.cse.unsw.edu.au/~claude/papers/MI15.pdf target=_blank>Behavior Cloning</a>、<a href=https://arxiv.org/abs/1011.0686 target=_blank>DAgger</a></p></li><li><p><strong>当前主流方法</strong>：<a href=https://tonyzhaozh.github.io/aloha/ target=_blank>ACT</a>、<a href=https://diffusion-policy.cs.columbia.edu/ target=_blank>Diffusion Policy</a></p><p>这些方法通过引入时序建模与生成式策略学习，有效提升了模仿学习在视觉控制任务中的表现。</p></li></ul><hr><h3 class="relative group">2. Robot Foundation Model<div id=2-robot-foundation-model class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#2-robot-foundation-model aria-label=锚点>#</a></span></h3><p>该方向属于 <strong>大模型 (foundation model)</strong> 范式，旨在通过统一的模型架构与大规模数据学习，使机器人具备跨任务、跨场景、跨模态的泛化能力。不同于传统在特定任务上单独训练的策略模型，这类模型试图构建“通用机器人智能（generalist robot）”，让机器人能够像语言模型一样，通过大规模预训练与下游微调实现“涌现式”的智能行为。<br>目前主流的做法是 Vision-Language-Action Models (VLA), 借助 VLM 的预训练知识将视觉、语言与动作建模统一在同一框架下。代表性工作：</p><ul><li><a href=https://openvla.github.io/ target=_blank>OpenVLA</a>：第一个开源且易于 follow 的 VLA。</li><li><a href=https://www.physicalintelligence.company/blog/pi0 target=_blank>Pi0</a> / <a href=https://www.physicalintelligence.company/blog/pi05 target=_blank>Pi0.5</a>：目前公认最 work 的 VLA，10K+ hours teleop data 训练的。</li><li><a href=https://pku-epic.github.io/GraspVLA-web/ target=_blank>GraspVLA</a>：基于纯仿真数据的抓取任务的 VLA。</li></ul><p>还有少量工作没有借助 VLM，单纯靠机器人数据做 scaling，代表有 RDT-1B 和 Large Behavior Model (LBM)</p><hr><h3 class="relative group">3. Sim-to-Real Reinforcement Learning (Distillation)<div id=3-sim-to-real-reinforcement-learning-distillation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#3-sim-to-real-reinforcement-learning-distillation aria-label=锚点>#</a></span></h3><p><strong>从仿真到真实 (Sim-to-Real)</strong> 是强化学习在具身智能中的关键挑战之一。</p><p>目前最成功的落地应用集中在 <strong>Locomotion（运动控制）</strong>，而在 <strong>Manipulation（操作任务）</strong> 上仍面临 sim2real Gap 过大的问题。</p><p>核心思路通常包括 <strong>策略蒸馏 (policy distillation)</strong>、<strong>域随机化 (domain randomization)</strong> 与 <strong>现实校准 (real calibration)</strong> 等技术。</p><hr><h3 class="relative group">4. Real-World Reinforcement Learning<div id=4-real-world-reinforcement-learning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#4-real-world-reinforcement-learning aria-label=锚点>#</a></span></h3><p><strong>Real-world RL</strong> 指直接在现实环境中进行探索式学习。</p><p>这类方法通常用于解决高度挑战性的具体任务（如插入 USB），目标是将成功率优化至接近 100%。</p><ul><li><strong>从零开始的真实世界强化学习</strong>：<strong>Hil-Serl</strong></li><li><strong>基于 VLA 的真实世界微调 (Fine-tuning)</strong>：部分近期工作尝试利用预训练 VLA 进行现实强化学习微调，但仍处于早期探索阶段。</li></ul><hr><h3 class="relative group">5. World Models<div id=5-world-models class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#5-world-models aria-label=锚点>#</a></span></h3><p><strong>World Model</strong> 最早起源于 <strong>基于模型的强化学习 (Model-based RL)</strong>，旨在通过内部世界建模来提升采样效率。</p><p>代表性工作包括 <strong>Dreamer 系列</strong>（Dreamer, DreamerV2, DreamerV3），通过学习潜在动态模型，实现“在脑中想象未来”式的策略更新。</p><p>在具身智能的最新语境中，<strong>World Model</strong> 的概念被拓展为 <strong>条件视频生成模型 (conditioned video generation model)</strong>，用于模拟未来观测、预测任务后果，并与规划模块或语言模型结合以实现长期推理。</p><h2 class="relative group">七、相关领域<div id=七相关领域 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e4%b8%83%e7%9b%b8%e5%85%b3%e9%a2%86%e5%9f%9f aria-label=锚点>#</a></span></h2><h3 class="relative group">1. Graphics<div id=1-graphics class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#1-graphics aria-label=锚点>#</a></span></h3><p>图形学在机器人与具身智能中的两大重要应用是 <strong>simulation（仿真）</strong> 与 <strong>rendering（渲染）</strong>。</p><ul><li><strong>Simulation</strong>：用于搭建虚拟的物理交互环境，是机器人强化学习、控制算法和策略验证的重要工具。如上述 IsaacLab 等</li><li><strong>Rendering</strong>：用于生成高质量的图像或视频，支撑感知模型（如视觉 Transformer）的训练与评估。例如：<strong>Blender</strong>：开源的三维建模与渲染软件。</li><li>系统性学习图形学推荐课程：<strong>Games 101, 103</strong></li></ul><h3 class="relative group">2. Hardware<div id=2-hardware class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#2-hardware aria-label=锚点>#</a></span></h3><p>硬件是具身智能的“身体基础”，涵盖操作、感知与反馈等环节。</p><ul><li><p><strong>Tele-operation（遥操作）</strong></p><ul><li><strong>末端操作设备</strong>：如 <em>Space Mouse</em>，用于控制机械臂的末端姿态。</li><li><strong>主从臂系统</strong>：如 <em>Gello</em>，实现高精度的力控遥操作。</li><li><strong>可穿戴设备</strong>：如 <em>AirExo</em> 或 <em>UMI</em>，通过外骨骼或手部设备实现自然交互与示教。</li></ul></li><li><p><strong>Sensors（传感器）</strong></p><ul><li><strong>Camera（视觉）</strong>：RGB / RGB-D 相机，如 RealSense、ZED、Azure Kinect。</li><li><strong>Force Sensor（力传感器）</strong>：用于检测接触力矩，常安装于末端。</li><li><strong>Tactile Sensor（触觉传感器）</strong>：如 GelSight、DIGIT，用于捕捉表面接触信息。</li></ul></li><li><p><strong>Mocap System（动作捕捉系统）</strong></p><p>用于精确追踪人体或机器人位姿，常用于收集示教数据或标定</p></li></ul><h3 class="relative group">3. Mainstream Models<div id=3-mainstream-models class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#3-mainstream-models aria-label=锚点>#</a></span></h3><ul><li><p><strong>Transformer</strong></p></li><li><p><strong>Diffusion、Flow Matching</strong>
由于能够有效建模多峰分布的生成模型 sota。</p></li></ul><h3 class="relative group">4. Foundation Models<div id=4-foundation-models class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#4-foundation-models aria-label=锚点>#</a></span></h3><ul><li><p><strong>LLM（Large Language Model）</strong>
通过大规模文本训练获得强大的语言理解与推理能力，是具身智能中语言规划与高层决策的重要基石。代表模型包括：<strong>GPT / Claude / Gemini</strong>：通用语言推理模型。</p></li><li><p><strong>Vision Encoder</strong></p><ul><li><a href=https://dinov2.metademolab.com/ target=_blank>DINO 系列</a>：通过大规模的<strong>自监督学习 (self-supervised learning)</strong> 提取图像的细粒度语义表示，在机器人视觉任务中常用于特征提取与场景理解。</li><li><a href=https://arxiv.org/pdf/2103.00020 target=_blank>CLIP</a>：通过大规模的图文匹配对上的 <strong>对比学习 (contrastive learning)</strong> ，将图像与文本映射到共享的多模态语义空间，成为视觉语言理解的核心模型。</li></ul></li><li><p><strong>VLM（Vision-Language Model）</strong>
通过大规模的图文理解数据进行训练，获得强大的视觉语言理解能力，在机器人视觉任务中常用于 VLA 模型的初始化，或用于场景理解与任务规划。代表模型包括：<a href=https://github.com/QwenLM/Qwen-VL target=_blank>Qwen-VL 系列</a>、<a href=https://arxiv.org/pdf/2410.21276 target=_blank>GPT4-o</a>、<a href=https://arxiv.org/pdf/2403.05530 target=_blank>Gemini</a>。</p></li></ul><h3 class="relative group">5. 3D Vision<div id=5-3d-vision class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#5-3d-vision aria-label=锚点>#</a></span></h3><p>详见 Intro-to-CV 课程，此处仅给出一些具身任务中常用的三维视觉技术。</p><ul><li><strong>三维生成与重建</strong><ul><li>相机标定：利用标定版构建多组约束，从而求解相机参数，常用于获取机器人坐标系与相机坐标系之间的变换矩阵。</li><li>单目三维生成：根据单张 RGB 图片生成对应物体的三维几何，在 real-to-sim 中是一种常用的获得物体几何的方法。</li><li>单目深度估计：通过单张 RGB 图片估计场景深度，常用于将互联网或是二维生成模型的输出结果转换为三维视觉信号。</li><li>位姿估计与追踪：通过单张或多张 RGB 图片估计物体或相机的位姿，常用于提取二维图片或视频中的物体或是人手位姿，进一步作为 action 的一种表征。</li></ul></li><li><strong>三维表示</strong><ul><li>网格（Mesh）：通过三角形网格表示三维几何，物理仿真中最常用的三维表示方式。</li><li>点云（Point Cloud）：通过物体表面的点的集合来表示三维几何。现有的点云处理网络具有很好的捕捉局部几何的能力，因此 GraspNet 使用点云作为输入，实现了非常鲁棒的抓取位姿预测。</li><li>Gaussian Splatting：通过高斯分布表示三维几何，由于其可微渲染与快速计算的特点，成为沟通二维与三维的桥梁。在 real-to-sim 中是一种常用的重建场景几何的表示。</li></ul></li><li><strong>三维理解</strong><ul><li>包括三维分类、场景分割、实例检测、空间推理等任务，常用于机器人视觉任务中的场景理解与任务规划。</li></ul></li></ul><h2 class="relative group">八、(Optional) 科研工作中的必备能力<div id=八optional-科研工作中的必备能力 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e5%85%aboptional-%e7%a7%91%e7%a0%94%e5%b7%a5%e4%bd%9c%e4%b8%ad%e7%9a%84%e5%bf%85%e5%a4%87%e8%83%bd%e5%8a%9b aria-label=锚点>#</a></span></h2><ul><li>Sharp Mind：戳穿别人工作的包装，看到本质<br></li><li>Writing and Presentation： 包装自己的工作，别让别人拆穿<br></li><li>Warm Mind：不要一味的批评别人的工作，能够欣赏到别人的亮点<br></li></ul><h2 class="relative group">相关仓库<div id=相关仓库 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e7%9b%b8%e5%85%b3%e4%bb%93%e5%ba%93 aria-label=锚点>#</a></span></h2><div class=github-card-wrapper><a id=github-35b16211100f8c12d4ffeea82d1eb796 target=_blank href=https://github.com/TianxingChen/Embodied-AI-Guide class=cursor-pointer><div class="w-full md:w-auto p-0 m-0 border border-neutral-200 dark:border-neutral-700 border rounded-md shadow-2xl"><div class="w-full nozoom"><img src=https://opengraph.githubassets.com/0/TianxingChen/Embodied-AI-Guide alt="GitHub Repository Thumbnail" class="nozoom mt-0 mb-0 w-full h-full object-cover"></div><div class="w-full md:w-auto pt-3 p-5"><div class="flex items-center"><span class="text-2xl text-neutral-800 dark:text-neutral me-2"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><div id=github-35b16211100f8c12d4ffeea82d1eb796-full_name class="m-0 font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral">TianxingChen/Embodied-AI-Guide</div></div><p id=github-35b16211100f8c12d4ffeea82d1eb796-description class="m-0 mt-2 text-md text-neutral-800 dark:text-neutral">[Lumina Embodied AI] 具身智能技术指南 Embodied-AI-Guide</p><div class="m-0 mt-2 flex items-center"><span class="mr-1 inline-block h-3 w-3 rounded-full language-dot" data-language=default></span><div class="m-0 mr-5 text-md text-neutral-800 dark:text-neutral">null</div><span class="text-md mr-1 text-neutral-800 dark:text-neutral"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentColor" d="M287.9.0C297.1.0 305.5 5.25 309.5 13.52L378.1 154.8l153.3 22.7C540.4 178.8 547.8 185.1 550.7 193.7 553.5 202.4 551.2 211.9 544.8 218.2L433.6 328.4l26.3 155.5C461.4 492.9 457.7 502.1 450.2 507.4 442.8 512.7 432.1 513.4 424.9 509.1L287.9 435.9 150.1 509.1C142.9 513.4 133.1 512.7 125.6 507.4 118.2 502.1 114.5 492.9 115.1 483.9l27.1-155.5L31.11 218.2C24.65 211.9 22.36 202.4 25.2 193.7 28.03 185.1 35.5 178.8 44.49 177.5L197.7 154.8 266.3 13.52C270.4 5.249 278.7.0 287.9.0zm0 78.95L235.4 187.2C231.9 194.3 225.1 199.3 217.3 200.5L98.98 217.9 184.9 303C190.4 308.5 192.9 316.4 191.6 324.1L171.4 443.7l105.2-56.2C283.7 383.7 292.2 383.7 299.2 387.5l105.2 56.2-20.2-119.6C382.9 316.4 385.5 308.5 391 303l85.9-85.1-118.3-17.4C350.7 199.3 343.9 194.3 340.5 187.2L287.9 78.95z"/></svg></span></span><div id=github-35b16211100f8c12d4ffeea82d1eb796-stargazers class="m-0 mr-5 text-md text-neutral-800 dark:text-neutral">10158</div><span class="text-md mr-1 text-neutral-800 dark:text-neutral"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M80 104c13.3.0 24-10.7 24-24S93.3 56 80 56 56 66.7 56 80s10.7 24 24 24zm80-24c0 32.8-19.7 61-48 73.3V192c0 17.7 14.3 32 32 32H304c17.7.0 32-14.3 32-32V153.3C307.7 141 288 112.8 288 80c0-44.2 35.8-80 80-80s80 35.8 80 80c0 32.8-19.7 61-48 73.3V192c0 53-43 96-96 96H256v70.7c28.3 12.3 48 40.5 48 73.3.0 44.2-35.8 80-80 80s-80-35.8-80-80c0-32.8 19.7-61 48-73.3V288H144c-53 0-96-43-96-96V153.3C19.7 141 0 112.8.0 80 0 35.8 35.8.0 80 0s80 35.8 80 80zm208 24c13.3.0 24-10.7 24-24s-10.7-24-24-24-24 10.7-24 24 10.7 24 24 24zM248 432c0-13.3-10.7-24-24-24s-24 10.7-24 24 10.7 24 24 24 24-10.7 24-24z"/></svg></span></span><div id=github-35b16211100f8c12d4ffeea82d1eb796-forks class="m-0 mr-5 text-md text-neutral-800 dark:text-neutral">691</div></div></div></div><script async type=text/javascript src=/js/fetch-repo.min.dc5533c50cefd50405344b235937142271f26229fe39cbee27fd4960e8bb897a0beebfad77a1091ca91cd0d1fb14e70fc37cc114dd9674fb2c32e0ab512ec8a4.js integrity="sha512-3FUzxQzv1QQFNEsjWTcUInHyYin+OcvuJ/1JYOi7iXoL7r+td6EJHKkc0NH7FOcPw3zBFN2WdPssMuCrUS7IpA==" data-repo-url=https://api.github.com/repos/TianxingChen/Embodied-AI-Guide data-repo-id=github-35b16211100f8c12d4ffeea82d1eb796></script></a></div><div class="expand-wrapper prose dark:prose-invert max-w-prose zen-mode-content"><input id=expand-6 class=expand-toggle type=checkbox>
<label for=expand-6 class=expand-title><span class=expand-icon><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-down"><polyline points="6 9 12 15 18 9"/></svg>
</span>原始地址</label><div class=expand-content><div class=expand-inner><a href=https://github.com/TianxingChen/Embodied-AI-Guide target=_blank>https://github.com/TianxingChen/Embodied-AI-Guide</a></div></div></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full me-4" width=96 height=96 alt=xiadengma src=/img/author_hu_76f37802f31e48ac.jpg data-zoom-src=/img/author_hu_9efbc1c1dad84e42.jpg><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">作者</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">xiadengma</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/xiadengma target=_blank aria-label=Github title=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://xiadengma.com/ target=_blank aria-label=Link title=Link rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 640 512"><path fill="currentColor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></span></span></a></div></div></div></div><div class=mb-10></div></div><script>var oid="views_posts/如何入门具身智能研究/index.md",oid_likes="likes_posts/如何入门具身智能研究/index.md"</script><script type=text/javascript src=/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span class="flex flex-col"><a class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" href=/posts/mit-6.s184-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/><span class=leading-6><span class="inline-block rtl:rotate-180">&larr;</span>&ensp;MIT 6.S184 课程笔记
</span></a><span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-11-06T10:54:08+08:00>2025年11月6日</time>
</span></span><span class="flex flex-col items-end"></span></div></div></footer></article><div id=scroll-to-top class="fixed bottom-6 end-6 z-50 transform translate-y-4 opacity-0 duration-200"><a href=#the-top class="bf-scroll-to-top__btn pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label=返回顶部 title=返回顶部><span class=sr-only>返回顶部</span>
<svg class="bf-scroll-to-top__icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true"><path d="M12 19V5"/><path d="m5 12 7-7 7 7"/></svg></a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">© 2025 <a href=https://xiadengma.com target=_blank>XIADENGMA</a> | All rights reserved | <a href=https://beian.miit.gov.cn target=_blank>浙ICP备20005376号-2</a></p><p class="text-xs text-neutral-500 dark:text-neutral-400">🛸 by Hugo & Blowfish</p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script><div style=display:none class="bf-no-justify bf-algorithm bf-algorithm__caption bf-algorithm__body bf-figure-group bf-figure-group__grid bf-fig-16 bf-fig-single"></div><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script src=https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js></script><script>function css(e){return"rgb("+getComputedStyle(document.documentElement).getPropertyValue(e)+")"}mermaid.initialize({startOnLoad:!0,theme:"base",themeVariables:{background:css("--color-neutral"),primaryColor:css("--color-primary-200"),secondaryColor:css("--color-secondary-200"),tertiaryColor:css("--color-neutral-100"),primaryBorderColor:css("--color-primary-400"),secondaryBorderColor:css("--color-secondary-400"),tertiaryBorderColor:css("--color-neutral-400"),lineColor:"#ffffff",fontFamily:"ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,segoe ui,Roboto,helvetica neue,Arial,noto sans,sans-serif",fontSize:"16px"}}),document.addEventListener("DOMContentLoaded",function(){setTimeout(function(){const e=document.querySelectorAll(".mermaid svg");e.forEach(e=>{const n=(new XMLSerializer).serializeToString(e),t=document.createElement("img"),s=new Blob([n],{type:"image/svg+xml"}),o=URL.createObjectURL(s);t.src=o,t.style.width="100%",t.style.maxWidth=e.getAttribute("width")||"100%",t.style.height="auto",t.alt="Mermaid diagram",e.parentNode.replaceChild(t,e),mediumZoom(t,{margin:24,background:"rgba(0,0,0,0.8)",scrollOffset:99999})})},1e3)})</script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500" data-url=https://blog.xiadengma.com/><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=搜索 tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="关闭 (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>