#!/usr/bin/env python

# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
ä½¿ç”¨ Web æµè§ˆå™¨å¯è§†åŒ– LeRobotDataset ä¸­ä»»æ„ episode çš„æ‰€æœ‰å¸§æ•°æ®ã€‚
Visualize data of **all** frames of any episode of a dataset of type LeRobotDataset in a web browser.

æ³¨æ„ Note:
    - Episode çš„æœ€åä¸€å¸§ä¸ä¸€å®šå¯¹åº”æœ€ç»ˆçŠ¶æ€ / The last frame doesn't always correspond to a final state
    - å›¾åƒå¯èƒ½å­˜åœ¨å‹ç¼©ä¼ªå½± / Images may show compression artifacts from mp4 encoding

è®¿é—® Access:
    æµè§ˆå™¨æ‰“å¼€ / Open in browser: http://localhost:PORT
"""

import argparse
import gc
import logging
from collections.abc import Iterator
from pathlib import Path

import numpy as np
import rerun as rr
import torch
import torch.utils.data
import tqdm

from lerobot.datasets.lerobot_dataset import LeRobotDataset
from lerobot.utils.constants import ACTION, DONE, OBS_STATE, REWARD


class EpisodeSampler(torch.utils.data.Sampler):
    """ç”¨äºé‡‡æ ·å•ä¸ª episode çš„æ‰€æœ‰å¸§ / Sampler for all frames of a single episode."""

    def __init__(self, dataset: LeRobotDataset, episode_index: int):
        from_idx = dataset.meta.episodes["dataset_from_index"][episode_index]
        to_idx = dataset.meta.episodes["dataset_to_index"][episode_index]
        self.frame_ids = range(from_idx, to_idx)

    def __iter__(self) -> Iterator:
        return iter(self.frame_ids)

    def __len__(self) -> int:
        return len(self.frame_ids)


def to_hwc_uint8_numpy(chw_float32_torch: torch.Tensor) -> np.ndarray:
    """
    å°† PyTorch CHW float32 å›¾åƒè½¬æ¢ä¸º NumPy HWC uint8 æ ¼å¼ã€‚
    Convert PyTorch CHW float32 image to NumPy HWC uint8 format.
    """
    assert chw_float32_torch.dtype == torch.float32
    assert chw_float32_torch.ndim == 3
    c, h, w = chw_float32_torch.shape
    assert c < h and c < w, (
        f"æœŸæœ›é€šé“ä¼˜å…ˆæ ¼å¼ï¼Œä½†å¾—åˆ° / expect channel first images, but got {chw_float32_torch.shape}"
    )
    hwc_uint8_numpy = (chw_float32_torch * 255).type(torch.uint8).permute(1, 2, 0).numpy()
    return hwc_uint8_numpy


def visualize_episode(
    dataset: LeRobotDataset,
    episode_index: int,
    batch_size: int = 32,
    num_workers: int = 0,
) -> None:
    """
    åœ¨ Rerun ä¸­å¯è§†åŒ–å•ä¸ª episode çš„æ‰€æœ‰å¸§ã€‚
    Visualize all frames of a single episode in Rerun.

    Args:
        dataset: LeRobot æ•°æ®é›† / LeRobot dataset
        episode_index: Episode ç´¢å¼• / Episode index
        batch_size: æ‰¹å¤„ç†å¤§å° / Batch size for dataloader
        num_workers: æ•°æ®åŠ è½½è¿›ç¨‹æ•° / Number of worker processes
    """
    logging.info(
        f"ğŸ“Š åŠ è½½ Episode {episode_index} çš„æ•°æ®åŠ è½½å™¨ / Loading dataloader for Episode {episode_index}"
    )

    episode_sampler = EpisodeSampler(dataset, episode_index)
    dataloader = torch.utils.data.DataLoader(
        dataset,
        num_workers=num_workers,
        batch_size=batch_size,
        sampler=episode_sampler,
    )

    total_frames = len(episode_sampler)
    logging.info(
        f"ğŸ“ˆ Episode {episode_index} å…±æœ‰ {total_frames} å¸§ / Episode {episode_index} has {total_frames} frames"
    )

    # è®°å½•æ•°æ®åˆ° Rerun / Log data to Rerun
    for batch in tqdm.tqdm(dataloader, total=len(dataloader), desc=f"Episode {episode_index}"):
        # éå†æ‰¹æ¬¡ä¸­çš„æ¯ä¸€å¸§ / iterate over the batch
        for i in range(len(batch["index"])):
            rr.set_time_sequence("frame_index", batch["frame_index"][i].item())
            rr.set_time_seconds("timestamp", batch["timestamp"][i].item())

            # æ˜¾ç¤ºç›¸æœºå›¾åƒ / display camera images
            for key in dataset.meta.camera_keys:
                rr.log(f"cameras/{key}", rr.Image(to_hwc_uint8_numpy(batch[key][i])))

            # æ˜¾ç¤ºåŠ¨ä½œç©ºé—´çš„æ¯ä¸ªç»´åº¦ / display each dimension of action space
            if ACTION in batch:
                for dim_idx, val in enumerate(batch[ACTION][i]):
                    rr.log(f"{ACTION}/dim_{dim_idx}", rr.Scalar(val.item()))

            # æ˜¾ç¤ºè§‚æµ‹çŠ¶æ€ç©ºé—´çš„æ¯ä¸ªç»´åº¦ / display each dimension of observed state space
            if OBS_STATE in batch:
                for dim_idx, val in enumerate(batch[OBS_STATE][i]):
                    rr.log(f"state/dim_{dim_idx}", rr.Scalar(val.item()))

            # æ˜¾ç¤ºå®Œæˆæ ‡å¿— / display done flag
            if DONE in batch:
                rr.log(DONE, rr.Scalar(batch[DONE][i].item()))

            # æ˜¾ç¤ºå¥–åŠ± / display reward
            if REWARD in batch:
                rr.log(REWARD, rr.Scalar(batch[REWARD][i].item()))

            # æ˜¾ç¤ºæˆåŠŸæ ‡å¿— / display success flag
            if "next.success" in batch:
                rr.log("success", rr.Scalar(batch["next.success"][i].item()))

    logging.info(f"âœ… Episode {episode_index} å¯è§†åŒ–å®Œæˆ / Episode {episode_index} visualization complete")


def visualize_dataset_web(
    dataset: LeRobotDataset,
    episode_indices: list[int],
    batch_size: int = 32,
    num_workers: int = 0,
    port: int = 9090,
    open_browser: bool = True,
    memory_limit: str = "25%",
) -> None:
    """
    ä½¿ç”¨ Web ç•Œé¢å¯è§†åŒ–æ•°æ®é›†ã€‚
    Visualize dataset using web interface.

    Args:
        dataset: LeRobot æ•°æ®é›† / LeRobot dataset
        episode_indices: è¦å¯è§†åŒ–çš„ episode ç´¢å¼•åˆ—è¡¨ / List of episode indices to visualize
        batch_size: æ‰¹å¤„ç†å¤§å° / Batch size for dataloader
        num_workers: æ•°æ®åŠ è½½è¿›ç¨‹æ•° / Number of worker processes
        port: Web æœåŠ¡å™¨ç«¯å£ / Web server port
        open_browser: æ˜¯å¦è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ / Whether to automatically open browser
        memory_limit: Rerun å†…å­˜é™åˆ¶ / Memory limit for Rerun
    """
    repo_id = dataset.repo_id

    # åˆå§‹åŒ– Rerun Web ç•Œé¢ / Initialize Rerun web interface
    logging.info("ğŸŒ å¯åŠ¨ Rerun Web ç•Œé¢ / Starting Rerun Web interface")
    logging.info(f"ğŸ“ è®¿é—®åœ°å€ / Access URL: http://localhost:{port}")
    logging.info(f"ğŸ’¾ å†…å­˜é™åˆ¶ / Memory limit: {memory_limit}")

    rr.init(f"{repo_id}_web_viz", spawn=False)

    # æ‰‹åŠ¨è§¦å‘åƒåœ¾å›æ”¶ï¼Œé¿å…é˜»å¡ / Manually call garbage collector to avoid blocking
    gc.collect()

    # å¯åŠ¨ Web æœåŠ¡å™¨ / Start web server
    rr.serve_web(
        open_browser=open_browser,
        web_port=port,
        server_memory_limit=memory_limit,
    )

    # å¯è§†åŒ–æ¯ä¸ª episode / Visualize each episode
    for episode_idx in episode_indices:
        if episode_idx >= len(dataset.meta.episodes):
            logging.warning(
                f"âš ï¸  Episode {episode_idx} ä¸å­˜åœ¨ï¼Œè·³è¿‡ / Episode {episode_idx} does not exist, skipping"
            )
            continue

        # ä¸ºæ¯ä¸ª episode åˆ›å»ºè®°å½•è·¯å¾„ / Create recording path for each episode
        rr.log(f"episode_{episode_idx}/info", rr.TextLog(f"Episode {episode_idx}"), static=True)

        # è®¾ç½®æ—¶é—´åºåˆ—æ ‡è®°å½“å‰ episode / Set time sequence for current episode
        rr.set_time_sequence("episode", episode_idx)

        visualize_episode(
            dataset=dataset,
            episode_index=episode_idx,
            batch_size=batch_size,
            num_workers=num_workers,
        )

    logging.info("âœ¨ æ‰€æœ‰ episode å¯è§†åŒ–å®Œæˆ / All episodes visualization complete")
    logging.info("ğŸŒ Web æœåŠ¡å™¨æŒç»­è¿è¡Œä¸­ï¼ŒæŒ‰ Ctrl+C é€€å‡º / Web server running, press Ctrl+C to exit")

    # ä¿æŒæœåŠ¡å™¨è¿è¡Œ / Keep server running
    try:
        import time

        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        logging.info("ğŸ‘‹ æ”¶åˆ° Ctrl-Cï¼Œæ­£åœ¨é€€å‡º / Ctrl-C received, exiting")


def main():
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ Web æµè§ˆå™¨å¯è§†åŒ– LeRobot æ•°æ®é›† / Visualize LeRobot dataset in web browser",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument(
        "--repo-id",
        type=str,
        required=True,
        help="æ•°æ®é›†ä»“åº“ ID / Dataset repository ID (e.g. `lerobot/pusht` or `xiadengma/record-test-so101`)",
    )

    # Episode é€‰æ‹©å‚æ•° / Episode selection arguments
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        "--episode-index",
        type=int,
        help="è¦å¯è§†åŒ–çš„å•ä¸ª episode ç´¢å¼• / Single episode index to visualize",
    )
    group.add_argument(
        "--episodes",
        type=int,
        nargs="+",
        help="è¦å¯è§†åŒ–çš„å¤šä¸ª episode ç´¢å¼• / Multiple episode indices to visualize (e.g. 0 1 2 3)",
    )

    parser.add_argument(
        "--root",
        type=Path,
        default=None,
        help="æœ¬åœ°æ•°æ®é›†æ ¹ç›®å½• / Root directory for local dataset (e.g. `--root ./data/datasets/xiadengma/record-test-so101`)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=32,
        help="DataLoader æ‰¹å¤„ç†å¤§å° / Batch size for DataLoader (default: 32)",
    )
    parser.add_argument(
        "--num-workers",
        type=int,
        default=4,
        help="DataLoader è¿›ç¨‹æ•° / Number of DataLoader worker processes (default: 4)",
    )
    parser.add_argument(
        "--port",
        type=int,
        default=9090,
        help="Web æœåŠ¡å™¨ç«¯å£ / Web server port (default: 9090)",
    )
    parser.add_argument(
        "--open-browser",
        type=lambda x: str(x).lower() in ("true", "1", "yes"),
        default=True,
        help="æ˜¯å¦è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ / Whether to automatically open browser (default: True)",
    )
    parser.add_argument(
        "--memory-limit",
        type=str,
        default="25%",
        help="Rerun å†…å­˜é™åˆ¶ / Memory limit for Rerun (default: 25%%)",
    )
    parser.add_argument(
        "--tolerance-s",
        type=float,
        default=1e-4,
        help="æ—¶é—´æˆ³å®¹å·®ï¼ˆç§’ï¼‰/ Tolerance in seconds for timestamps (default: 1e-4)",
    )

    args = parser.parse_args()

    # é…ç½®æ—¥å¿— / Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    # ç¡®å®šè¦å¯è§†åŒ–çš„ episode åˆ—è¡¨ / Determine episode list
    if args.episode_index is not None:
        episode_indices = [args.episode_index]
    else:
        episode_indices = args.episodes

    logging.info("=" * 80)
    logging.info("ğŸ¤– LeRobot æ•°æ®é›† Web å¯è§†åŒ–å·¥å…· / LeRobot Dataset Web Visualizer")
    logging.info("=" * 80)
    logging.info(f"ğŸ“¦ æ•°æ®é›† / Dataset: {args.repo_id}")
    logging.info(f"ğŸ“‚ æ ¹ç›®å½• / Root: {args.root if args.root else 'HuggingFace Cache'}")
    logging.info(f"ğŸ“Š Episodes: {episode_indices}")
    logging.info("=" * 80)

    # åŠ è½½æ•°æ®é›† / Load dataset
    logging.info("ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®é›† / Loading dataset...")
    dataset = LeRobotDataset(
        repo_id=args.repo_id,
        episodes=episode_indices,
        root=args.root,
        tolerance_s=args.tolerance_s,
    )

    logging.info("âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ / Dataset loaded successfully")
    logging.info(f"ğŸ“ˆ æ•°æ®é›†æ€»å¸§æ•° / Total frames: {len(dataset)}")
    logging.info(f"ğŸ“¹ ç›¸æœºæ•°é‡ / Number of cameras: {len(dataset.meta.camera_keys)}")
    logging.info(f"ğŸ¥ ç›¸æœºåˆ—è¡¨ / Camera keys: {dataset.meta.camera_keys}")

    # å¯åŠ¨ Web å¯è§†åŒ– / Start web visualization
    visualize_dataset_web(
        dataset=dataset,
        episode_indices=episode_indices,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        port=args.port,
        open_browser=args.open_browser,
        memory_limit=args.memory_limit,
    )


if __name__ == "__main__":
    main()
