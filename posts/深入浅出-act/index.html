<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=dark data-auto-appearance=false><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><meta name=theme-color><title>深入浅出 ACT &#183; MyBlog</title><meta name=title content="深入浅出 ACT &#183; MyBlog"><meta name=description content="My awesome website"><meta name=keywords content="Embodied AI,ACT,IL,"><link rel=canonical href=https://blog.xiadengma.com/posts/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-act/><meta name=author content="xiadengma"><link href=https://github.com/xiadengma rel=me><link href=https://xiadengma.com/ rel=me><meta property="og:url" content="https://blog.xiadengma.com/posts/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-act/"><meta property="og:site_name" content="MyBlog"><meta property="og:title" content="深入浅出 ACT"><meta property="og:description" content="My awesome website"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-20T00:02:03+08:00"><meta property="article:modified_time" content="2025-12-25T13:40:32+08:00"><meta property="article:tag" content="Embodied-Ai"><meta property="article:tag" content="ACT"><meta property="article:tag" content="IL"><meta property="og:see_also" content="https://blog.xiadengma.com/posts/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-vae--cvae/"><meta property="og:see_also" content="https://blog.xiadengma.com/posts/%E4%B8%80%E4%BA%9B%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"><meta name=twitter:card content="summary"><meta name=twitter:title content="深入浅出 ACT"><meta name=twitter:description content="My awesome website"><link type=text/css rel=stylesheet href=/css/main.bundle.min.80e191426fa299808166e9d09dd48085cd7b62f9a4c616750392885feb860dcdbc7da53b64846e6042b50930f56ccfeeb46795073cf668cf56c1e7cf8652bf65.css integrity="sha512-gOGRQm+imYCBZunQndSAhc17YvmkxhZ1A5KIX+uGDc28faU7ZIRuYEK1CTD1bM/utGeVBzz2aM9WwefPhlK/ZQ=="><script type=text/javascript src=/js/appearance.min.6f41174b3a05b680820fe08cadbfa5fb7a7ca347b76a0955cdc68b9d8aca1ce24f0547e138cea33bcc7904d551a90afcb1cc7f2d9fe8557075d501419046c08c.js integrity="sha512-b0EXSzoFtoCCD+CMrb+l+3p8o0e3aglVzcaLnYrKHOJPBUfhOM6jO8x5BNVRqQr8scx/LZ/oVXB11QFBkEbAjA=="></script><script src=/lib/zoom/zoom.min.umd.a527109b68c082a70f3697716dd72a9d5aa8b545cf800cecbbc7399f2ca6f6e0ce3e431f2062b48bbfa47c9ea42822714060bef309be073f49b9c0e30d318d7b.js integrity="sha512-pScQm2jAgqcPNpdxbdcqnVqotUXPgAzsu8c5nyym9uDOPkMfIGK0i7+kfJ6kKCJxQGC+8wm+Bz9JucDjDTGNew=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.a4a7dfccb3436d38f7e4b42726453b8acb7e154b0b6ecad4afcbede29623d36d0463273c19cd6867ac37af7853561bb8f323133653545b1157c47a2360052976.js integrity="sha512-pKffzLNDbTj35LQnJkU7ist+FUsLbsrUr8vt4pYj020EYyc8Gc1oZ6w3r3hTVhu48yMTNlNUWxFXxHojYAUpdg==" data-copy=复制 data-copied=已复制></script><script src=/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"文章","name":"深入浅出 ACT","headline":"深入浅出 ACT","inLanguage":"en","url":"https://blog.xiadengma.com/posts/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-act/","author":{"@type":"Person","name":"xiadengma"},"copyrightYear":"2025","dateCreated":"2025-10-20T00:02:03\u002b08:00","datePublished":"2025-10-20T00:02:03\u002b08:00","dateModified":"2025-12-25T13:40:32\u002b08:00","keywords":["Embodied AI","ACT","IL"],"mainEntityOfPage":"true","wordCount":"15974"}]</script><script data-id=umami-script async src=https://analytics.umami.is/script.js data-website-id=678aa4f9-0d3a-485e-88ea-9a4e51d5aa9f></script><script type=text/javascript>document.querySelector('script[data-id="umami-script"]').addEventListener("load",function(){const e=document.head.querySelector('meta[property = "og:type"]').getAttribute("content");let t=document.head.querySelector('meta[property = "og:title"]').getAttribute("content"),n=document.head.querySelector('meta[property = "og:url"]').getAttribute("content");umami.track(e+":"+t,{url:n})})</script><script>(()=>{const n=/[A-Za-z]{2,}[^\S\r\n]+[A-Za-z]{2,}/,s=/[A-Za-z]/g,o=/[\u4E00-\u9FFF]/g,i=/https?:\/\/\S+/i,a=/[A-Za-z0-9_./:-]{20,}/;function e(e,t){const n=t.match(e);return n?n.length:0}function t(){const t=document.querySelectorAll(".article-content p, .article-content li");for(const c of t){const r=(c.textContent||"").trim();if(!r)continue;const d=getComputedStyle(c);if(d.textAlign!=="justify")continue;if(i.test(r)||a.test(r)){c.classList.add("bf-no-justify");continue}if(!n.test(r))continue;const l=e(s,r);if(l<12)continue;const u=e(o,r);if(l<=u)continue;c.classList.add("bf-no-justify")}}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",t):t()})()</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script><script defer src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js id=MathJax-script></script><script>(()=>{function s(){const e=window.matchMedia("(max-width: 768px)").matches,t=.7,n=document.querySelectorAll('.article-content mjx-container[display="true"]');for(const o of n){const s=o.querySelector("mjx-math");if(!s)continue;s.style.transform="",s.style.transformOrigin="",s.style.display="",o.classList.remove("bf-math-scroll");const l=o.parentElement||o,a=l.getBoundingClientRect().width,r=o.getBoundingClientRect().width,i=a&&r?Math.min(a,r):a||r;if(!i||i<=0)continue;const c=Math.max(s.scrollWidth||0,s.getBoundingClientRect().width);if(c>i+1){const n=i/c;e&&n<t?(o.classList.add("bf-math-scroll"),s.style.display="inline-block"):(s.style.transform=`scale(${n})`,s.style.transformOrigin="0 0",s.style.display="inline-block")}}}let e=0;function t(){e&&cancelAnimationFrame(e),e=requestAnimationFrame(()=>{e=0,s()})}function n(){window.MathJax?.startup?.promise?MathJax.startup.promise.then(()=>{t(),setTimeout(t,300)}):t(),window.addEventListener("resize",t,{passive:!0})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",n):n()})()</script></head><body class="flex flex-col h-screen m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32 text-lg bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 pe-2 dark:text-primary-400">&darr;</span>
跳过正文</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 min-h-[130px] opacity-65 bg-gradient-to-b from-neutral from-60% dark:from-neutral-800 to-transparent mix-blend-normal z-80"></div><div class="fixed inset-x-0 z-100"><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative m-auto leading-7 max-w-7xl px-6 sm:px-14 md:px-24 lg:px-32"><div class="main-menu flex items-center justify-between py-6 md:justify-start gap-x-3 pt-[2px] pr-2 md:pr-4 pb-[3px] pl-0"><div><a href=/ class=flex><span class=sr-only>MyBlog</span>
<img src=/img/logo.png width=90 height=90 class="logo max-h-[5rem] max-w-[5rem] object-scale-down object-left nozoom" alt></a></div><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium">MyBlog</a></nav><nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12"><a href=/posts/ class="flex items-center hover:text-primary-600 dark:hover:text-primary-400" aria-label=文章 title=文章><span class=mr-1><span class="relative block icon"><svg height="1em" viewBox="0 0 512 512"><path fill="currentColor" d="M441 58.9 453.1 71c9.4 9.4 9.4 24.6.0 33.9L424 134.1 377.9 88 407 58.9c9.4-9.4 24.6-9.4 33.9.0zM209.8 256.2 344 121.9 390.1 168 255.8 302.2c-2.9 2.9-6.5 5-10.4 6.1L186.9 325l16.7-58.5c1.1-3.9 3.2-7.5 6.1-10.4zM373.1 25 175.8 222.2c-8.7 8.7-15 19.4-18.3 31.1l-28.6 1e2c-2.4 8.4-.1 17.4 6.1 23.6s15.2 8.5 23.6 6.1l1e2-28.6c11.8-3.4 22.5-9.7 31.1-18.3L487 138.9c28.1-28.1 28.1-73.7.0-101.8L474.9 25C446.8-3.1 401.2-3.1 373.1 25zM88 64C39.4 64 0 103.4.0 152V424c0 48.6 39.4 88 88 88H360c48.6.0 88-39.4 88-88V312c0-13.3-10.7-24-24-24s-24 10.7-24 24V424c0 22.1-17.9 40-40 40H88c-22.1.0-40-17.9-40-40V152c0-22.1 17.9-40 40-40H2e2c13.3.0 24-10.7 24-24s-10.7-24-24-24H88z"/></svg></span></span><p class="text-base font-medium">文章</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title="搜索 (/)">
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button></nav><div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title="搜索 (/)">
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button></div></div><div class="-my-2 md:hidden"><div id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 pt-[5px]"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none text-end max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/posts/ class="flex items-center hover:text-primary-600 dark:hover:text-primary-400" aria-label=文章 title=文章><div class=mr-2><span class="relative block icon"><svg height="1em" viewBox="0 0 512 512"><path fill="currentColor" d="M441 58.9 453.1 71c9.4 9.4 9.4 24.6.0 33.9L424 134.1 377.9 88 407 58.9c9.4-9.4 24.6-9.4 33.9.0zM209.8 256.2 344 121.9 390.1 168 255.8 302.2c-2.9 2.9-6.5 5-10.4 6.1L186.9 325l16.7-58.5c1.1-3.9 3.2-7.5 6.1-10.4zM373.1 25 175.8 222.2c-8.7 8.7-15 19.4-18.3 31.1l-28.6 1e2c-2.4 8.4-.1 17.4 6.1 23.6s15.2 8.5 23.6 6.1l1e2-28.6c11.8-3.4 22.5-9.7 31.1-18.3L487 138.9c28.1-28.1 28.1-73.7.0-101.8L474.9 25C446.8-3.1 401.2-3.1 373.1 25zM88 64C39.4 64 0 103.4.0 152V424c0 48.6 39.4 88 88 88H360c48.6.0 88-39.4 88-88V312c0-13.3-10.7-24-24-24s-24 10.7-24 24V424c0 22.1-17.9 40-40 40H88c-22.1.0-40-17.9-40-40V152c0-22.1 17.9-40 40-40H2e2c13.3.0 24-10.7 24-24s-10.7-24-24-24H88z"/></svg></span></div><p class="text-bg font-bg">文章</p></a></li></ul></div></div></div></div></div></div><script type=text/javascript src=/js/background-blur.min.605b3b942818f0ab5a717ae446135ec46b8ee5a2ad12ae56fb90dc2a76ce30c388f9fec8bcc18db15bd47e3fa8a09d779fa12aa9c184cf614a315bc72c6c163d.js integrity="sha512-YFs7lCgY8KtacXrkRhNexGuO5aKtEq5W+5DcKnbOMMOI+f7IvMGNsVvUfj+ooJ13n6EqqcGEz2FKMVvHLGwWPQ==" data-blur-id=menu-blur></script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom"><img id=background-image src=/img/background_1.svg role=presentation loading=eager decoding=async fetchpriority=high class="absolute inset-0 w-full h-full object-cover"><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-xl bg-neutral-100/75 dark:bg-neutral-800/60"></div><script type=text/javascript src=/js/background-blur.min.605b3b942818f0ab5a717ae446135ec46b8ee5a2ad12ae56fb90dc2a76ce30c388f9fec8bcc18db15bd47e3fa8a09d779fa12aa9c184cf614a315bc72c6c163d.js integrity="sha512-YFs7lCgY8KtacXrkRhNexGuO5aKtEq5W+5DcKnbOMMOI+f7IvMGNsVvUfj+ooJ13n6EqqcGEz2FKMVvHLGwWPQ==" data-blur-id=background-blur data-image-id=background-image data-image-url=/img/background_1.svg></script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class=hidden><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/>MyBlog</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/posts/>文章</a><span class="px-1 text-primary-500">/</span></li><li class=hidden><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/posts/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-act/>深入浅出 ACT</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">深入浅出 ACT</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-10-20T00:02:03+08:00>2025年10月20日</time><span class="px-2 text-primary-500">&#183;</span><time datetime=2025-12-25T13:40:32+08:00>更新:2025年12月25日</time><span class="px-2 text-primary-500">&#183;</span><span>47278 字</span><span class="px-2 text-primary-500">&#183;</span><span title=预计阅读>32 分钟</span></div><div class="flex flex-row flex-wrap items-center"><a class="relative mt-[0.5rem] me-2" href=/tags/embodied-ai/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Embodied-Ai
</span></span></a><a class="relative mt-[0.5rem] me-2" href=/tags/act/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ACT
</span></span></a><a class="relative mt-[0.5rem] me-2" href=/tags/il/><span class="flex cursor-pointer"><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">IL</span></span></a></div></div></header><section class="bf-single-layout bf-has-toc flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="bf-article-toc order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-auto overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg -ms-5 ps-5 pe-2 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 -ms-5 ps-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">目录</summary><div class="min-w-[220px] py-2 border-dotted border-s-1 -ms-5 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#0-写在开头>0. 写在开头</a></li><li><a href=#实践系统介绍>实践系统介绍</a><ul><li><a href=#数据收集流程>数据收集流程</a></li></ul></li><li><a href=#act-模型>ACT 模型</a><ul><li><a href=#act-模型核心创新点>ACT 模型核心创新点</a><ul><li><a href=#action-chunking-policy动作分块策略>Action Chunking Policy（动作分块策略）</a></li><li><a href=#temporal-ensemble时间集成>Temporal Ensemble（时间集成）</a></li></ul></li><li><a href=#act-伪代码与算法流程>ACT 伪代码与算法流程</a><ul><li><a href=#一act-训练>一、ACT 训练</a></li><li><a href=#二act-推理>二、ACT 推理</a></li><li><a href=#三损失函数>三、损失函数</a></li></ul></li><li><a href=#act-模型架构图>ACT 模型架构图</a><ul><li><a href=#act-训练>ACT 训练</a></li><li><a href=#act-推理>ACT 推理</a></li></ul></li><li><a href=#act-消融实验>ACT 消融实验</a></li></ul></li><li><a href=#实践>实践</a><ul><li><a href=#lerobot-act-代码阅读>LeRobot ACT 代码阅读</a></li><li><a href=#configuration_actpyact-配置><code>configuration_act.py</code>：ACT 配置</a></li><li><a href=#processor_actpyact-流水线><code>processor_act.py</code>：ACT 流水线</a></li><li><a href=#modeling_actpyact-模型><code>modeling_act.py</code>：ACT 模型</a></li><li><a href=#act-模型微调>ACT 模型微调</a><ul><li><a href=#关键参数>关键参数</a></li><li><a href=#参考命令>参考命令</a></li></ul></li></ul></li><li><a href=#结语>结语</a></li><li><a href=#参考资料><em>参考资料</em></a></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg -ms-5 ps-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 -ms-5 ps-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">目录</summary><div class="py-2 border-dotted border-neutral-300 border-s-1 -ms-5 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#0-写在开头>0. 写在开头</a></li><li><a href=#实践系统介绍>实践系统介绍</a><ul><li><a href=#数据收集流程>数据收集流程</a></li></ul></li><li><a href=#act-模型>ACT 模型</a><ul><li><a href=#act-模型核心创新点>ACT 模型核心创新点</a><ul><li><a href=#action-chunking-policy动作分块策略>Action Chunking Policy（动作分块策略）</a></li><li><a href=#temporal-ensemble时间集成>Temporal Ensemble（时间集成）</a></li></ul></li><li><a href=#act-伪代码与算法流程>ACT 伪代码与算法流程</a><ul><li><a href=#一act-训练>一、ACT 训练</a></li><li><a href=#二act-推理>二、ACT 推理</a></li><li><a href=#三损失函数>三、损失函数</a></li></ul></li><li><a href=#act-模型架构图>ACT 模型架构图</a><ul><li><a href=#act-训练>ACT 训练</a></li><li><a href=#act-推理>ACT 推理</a></li></ul></li><li><a href=#act-消融实验>ACT 消融实验</a></li></ul></li><li><a href=#实践>实践</a><ul><li><a href=#lerobot-act-代码阅读>LeRobot ACT 代码阅读</a></li><li><a href=#configuration_actpyact-配置><code>configuration_act.py</code>：ACT 配置</a></li><li><a href=#processor_actpyact-流水线><code>processor_act.py</code>：ACT 流水线</a></li><li><a href=#modeling_actpyact-模型><code>modeling_act.py</code>：ACT 模型</a></li><li><a href=#act-模型微调>ACT 模型微调</a><ul><li><a href=#关键参数>关键参数</a></li><li><a href=#参考命令>参考命令</a></li></ul></li></ul></li><li><a href=#结语>结语</a></li><li><a href=#参考资料><em>参考资料</em></a></li></ul></nav></div></details><script>(function(){"use strict";const r=.33,m="#TableOfContents",h=".anchor",o='a[href^="#"]',a="li ul",i="active";let e=!1,n=0;function c(){return window.scrollY||document.documentElement.scrollTop||document.body.scrollTop||0}function l(){return window.innerHeight||document.documentElement.clientHeight||document.body.clientHeight||0}function d(t,n,s){const o=c(),i=o+l()*s;if(e)for(let s=0;s<t.length;s++){const i=t[s];if(!n.has(i.id))continue;const a=i.getBoundingClientRect().top+o;if(Math.abs(o-a)<100)return e=!1,i.id}for(let e=t.length-1;e>=0;e--){const s=t[e];if(!n.has(s.id))continue;const a=s.getBoundingClientRect().top+o;if(a<=i)return s.id}for(let e=0;e<t.length;e++)if(n.has(t[e].id))return t[e].id;return""}function u({toc:e,anchors:t,links:n,tocIds:s,scrollOffset:o,collapseInactive:a}){const r=d(t,s,o);if(!r)return;if(n.forEach(e=>{const t=e.getAttribute("href")===`#${r}`;if(e.classList.toggle(i,t),a){const n=e.closest("li")?.querySelector("ul");n&&(n.style.display=t?"":"none")}}),a){const n=e.querySelector(`a[href="#${CSS.escape(r)}"]`);let t=n;for(;t&&t!==e;)t.tagName==="UL"&&(t.style.display=""),t.tagName==="LI"&&t.querySelector("ul")?.style.setProperty("display",""),t=t.parentElement}}function t(e){if(n)return;n=requestAnimationFrame(()=>{n=0,u(e)})}function s(){const n=document.querySelector(m);if(!n)return;const l=!0,d=[...document.querySelectorAll(h)],s=[...n.querySelectorAll(o)],u=new Set(s.map(e=>(e.getAttribute("href")||"").trim()).filter(e=>e.startsWith("#")&&e.length>1).map(e=>e.slice(1)));l&&n.querySelectorAll(a).forEach(e=>e.style.display="none"),s.forEach(t=>{t.addEventListener("click",()=>{e=!0})});const i={toc:n,anchors:d,links:s,tocIds:u,scrollOffset:r,collapseInactive:l},c=()=>t(i);window.addEventListener("scroll",c,{passive:!0}),document.body?.addEventListener("scroll",c,{passive:!0}),document.documentElement?.addEventListener("scroll",c,{passive:!0}),window.addEventListener("hashchange",()=>t(i),{passive:!0}),t(i)}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",s):s()})()</script></div></div><div class="bf-article-main min-w-0 min-h-0 max-w-fit"><details class="mt-2 mb-5 overflow-hidden rounded-lg ms-0 ps-5"><summary class="py-1 text-lg font-semibold cursor-pointer bg-primary-200 text-neutral-800 -ms-5 ps-5 dark:bg-primary-800 dark:text-neutral-100">深入浅出具身智能 -
系列文章</summary><div class="py-1 border-dotted border-neutral-300 border-s-1 -ms-5 ps-5 dark:border-neutral-600">§ :
本文</div><div class="py-1 border-dotted border-neutral-300 border-s-1 -ms-5 ps-5 dark:border-neutral-600"><a href=/posts/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-vae--cvae/>§ :
深入浅出 VAE & CVAE</a></div><div class="py-1 border-dotted border-neutral-300 border-s-1 -ms-5 ps-5 dark:border-neutral-600"><a href=/posts/%E4%B8%80%E4%BA%9B%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/>§ :
一些数学基础概念</a></div></details><div class="article-content max-w-prose mb-20"><h2 class="relative group">0. 写在开头<div id=0-写在开头 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#0-%e5%86%99%e5%9c%a8%e5%bc%80%e5%a4%b4 aria-label=锚点>#</a></span></h2><p>作为 SO-101 基础实践的理论部分，本文将深入浅出地介绍 ACT（Action Chunking with Transformers）模型。</p><h2 class="relative group">实践系统介绍<div id=实践系统介绍 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e5%ae%9e%e8%b7%b5%e7%b3%bb%e7%bb%9f%e4%bb%8b%e7%bb%8d aria-label=锚点>#</a></span></h2><ol><li><p>主臂（Leader Arm）：</p><ul><li>由人控制；</li><li>记录关节姿态，称为“动作（Action）”，训练时模型需要预测该动作。</li></ul></li><li><p>从臂（Follower Arm）：</p><ul><li>装有摄像头；</li><li>由程序控制：<ul><li>数据采集时：使用 PID 跟随已记录的 Action；</li><li>测试时：使用 PID 跟随模型预测的 Action；</li></ul></li><li>记录关节姿态与相机图像，统称为“观测（Observation）”，作为模型输入。</li></ul></li></ol><h3 class="relative group">数据收集流程<div id=数据收集流程 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e6%95%b0%e6%8d%ae%e6%94%b6%e9%9b%86%e6%b5%81%e7%a8%8b aria-label=锚点>#</a></span></h3><p>操作员通过主臂执行演示，系统以固定频率同步记录三类信息：</p><ol><li>场景视觉——机载/外置摄像头的当前图像；</li><li>从臂状态——七自由度从臂的当前关节位置向量（含末端夹爪开合自由度）；</li><li>主臂控制——同为七维的主臂关节目标（作为动作标签/ground truth）。</li></ol><p>采集中，主从之间存在通信时延与 PID 跟踪误差，因此从臂在时刻 $t$ 的实际位置更接近于主臂在 $t-1$ 的目标。为提升预测稳定性，观测（Observation）选取“当前从臂关节 + 当前图像”，而非“上一帧主臂动作”，因为前者能直接反映已执行结果、首帧不缺失，且允许模型自适应学习主从系统性细微差异。</p><p>训练阶段以（图像、从臂关节）为输入、以主臂关节目标为动作标签；测试/推理阶段在相同观测输入上预测虚拟主臂动作，并下发该动作，由从臂通过 PID 跟随完成实际执行。</p><h2 class="relative group">ACT 模型<div id=act-模型 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#act-%e6%a8%a1%e5%9e%8b aria-label=锚点>#</a></span></h2><h3 class="relative group">ACT 模型核心创新点<div id=act-模型核心创新点 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#act-%e6%a8%a1%e5%9e%8b%e6%a0%b8%e5%bf%83%e5%88%9b%e6%96%b0%e7%82%b9 aria-label=锚点>#</a></span></h3><h4 class="relative group">Action Chunking Policy（动作分块策略）<div id=action-chunking-policy动作分块策略 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#action-chunking-policy%e5%8a%a8%e4%bd%9c%e5%88%86%e5%9d%97%e7%ad%96%e7%95%a5 aria-label=锚点>#</a></span></h4><ul><li>ACT 的核心思想：缩短有效决策时域（reduce the effective horizon of a long trajectory）。</li><li>Action Chunking Policy：在时刻 $t$ 观测一次后，一次性输出接下来 $K$ 个时间步的动作序列；系统每经过 $K$ 步才进行一次新的决策。</li></ul><ol><li>痛点：累计误差（Compounding Error）。在长时程、多步控制任务中，早期或连续的小偏差会相互叠加，最终放大为不可挽回的偏差，导致任务失败。<ul><li>传统做法：<ul><li>单步策略（Single-Step Policy）在每个时间步都重新观测并决策。一旦连续多次出现轻微预测偏差，误差会以高频方式累积。</li></ul></li><li>为什么 Action Chunking Policy 能缓解累计误差？<ol><li>降低决策频率：由“每步决策”降为“每 $K$ 步决策”，有效决策次数显著减少，误差触发与积累机会随之降低。</li><li>抑制连续小错：单次偏差通常可在下一次（块级）决策时被纠正；真正致命的是高频、连续的小错。减少决策频率可直接降低“连续错误链”出现概率。</li><li>提高轨迹稳定性：成块输出的动作在时间上更一致，抑制逐步预测抖动，降低轨迹漂移风险。</li></ol></li></ul></li><li>痛点：非马尔可夫环境（Non-Markovian）。在马尔可夫环境中，最优策略可仅依赖当前状态；但现实任务常含隐含时间依赖。以炒菜为例：倒油 → 等待加热一段时间（在达到目标油温前后外观几乎无明显变化）→ 投入食材。<ul><li>传统做法：<ul><li>单步策略（Single-Step Policy）：由于关键信息无法从单帧观测直接辨识，难以判断“何时执行下一步”。</li><li>历史条件化策略（History-Conditioned Policy）：通过引入过往观测与动作来弥补非马尔可夫信息，但其条件域从“当前观测”扩展到“较长历史”，容易产生因果误识（causal misidentification）。<ul><li>因果误识（Causal Misidentification）：指将与行动高度共现、却非因果的线索当作真正原因，从而学习到错误的决策规则。比如：刹车时刹车灯会亮，但“灯亮”并非“刹车原因”，若据此做决策即为因果误识。</li></ul></li></ul></li><li>为什么 Action Chunking Policy 能提高非马尔可夫环境中的表现？<ol><li>将“倒油 → 等待加热 → 投入食材”封装为一个 action chunk，使模型能够显式/隐式地建模其中的等待时长与触发条件。</li><li>用面向未来的动作序列直接对时间依赖进行结构化约束，更容易学到真正的时序因果（如“等待时长”）而非表面共现。</li></ol></li></ul></li></ol><h4 class="relative group">Temporal Ensemble（时间集成）<div id=temporal-ensemble时间集成 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#temporal-ensemble%e6%97%b6%e9%97%b4%e9%9b%86%e6%88%90 aria-label=锚点>#</a></span></h4><figure style=width:40%;max-width:100%><img src=/posts/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-act/img/Temporal-Ensemble.png style=display:block;max-width:100%;height:auto alt="Temporal Ensemble" loading=lazy><figcaption><p>Temporal Ensemble</p></figcaption></figure><ol><li>为避免 Action Chunking “每 $K$ 步一次决策”带来的卡顿执行，使用 Temporal Ensemble 在每个时间步都预测一个长度为 $K$ 的动作块。</li><li>每次执行到第 $t$ 步时，将所有历史时刻对“第 $t$ 步”的预测进行加权融合，得到最终动作，相当于一种滑动加权平均的平滑器。</li></ol><ul><li>权重通常对更早产生的预测赋予更大权重（更稳定、噪声更小），从而显著提升轨迹的连续性与顺滑度。</li><li>代价是需要额外的前向推理次数（计算量上升），但无需改动训练目标或系统结构。</li></ul><h3 class="relative group">ACT 伪代码与算法流程<div id=act-伪代码与算法流程 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#act-%e4%bc%aa%e4%bb%a3%e7%a0%81%e4%b8%8e%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b aria-label=锚点>#</a></span></h3><blockquote class="book-hint warning"><div class=hint-content>请先学习 <a href=../%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-vae--cvae/>VAE 和 CVAE 模型</a>，再继续阅读。</div></blockquote><p>记号约定：令 $a_{t:t+K}\!\triangleq\!(a_t,\dots,a_{t+K-1})$ 表示长度为 $K$ 的动作序列；$o_t$ 为时刻 $t$ 的观测，$\bar{o}_t$ 为去除图像模态后的观测；$z$ 为潜变量。</p><h4 class="relative group">一、ACT 训练<div id=一act-训练 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e4%b8%80act-%e8%ae%ad%e7%bb%83 aria-label=锚点>#</a></span></h4><p>给定：演示数据集 $\mathcal{D}$、块大小（预测跨度）$K$、正则权重 $\beta$。</p><ol><li>初始化编码器（后验）$q_\phi\!\left(z\,\middle|\,a_{t:t+K},\bar{o}_t\right)$。</li><li>初始化解码器（策略）$\pi_\theta\!\left(\hat{a}_{t:t+K}\,\middle|\,o_t,z\right)$。</li><li>对迭代轮次 $n=1,2,\ldots$ 重复：<ol><li>从 $\mathcal{D}$ 采样样本对 $(o_t, a_{t:t+K})$；</li><li>从 $q_\phi\!\left(z\,\middle|\,a_{t:t+K},\bar{o}_t\right)$ 采样 $z$；</li><li>用 $\pi_\theta\!\left(\hat{a}_{t:t+K}\,\middle|\,o_t,z\right)$ 预测 $\hat{a}_{t:t+K}$；</li><li>计算重构损失
$$
\mathcal{L}_{\text{reconst}}=\mathrm{MSE}\!\left(\hat{a}_{t:t+K},\,a_{t:t+K}\right)
$$</li><li>计算正则（KL）项
$$
\mathcal{L}_{\text{reg}}=D_{\mathrm{KL}}\!\bigl(q_\phi(z\,|\,a_{t:t+K},\bar{o}_t)\,\|\,\mathcal{N}(0,I)\bigr)
$$</li><li>用 Adam 优化器更新 $\theta,\phi$，总损失
$$
\mathcal{L}=\mathcal{L}_{\text{reconst}}+\beta\,\mathcal{L}_{\text{reg}}\,.
$$</li></ol></li></ol><blockquote><p>说明：重构项对应最大似然思想——在给定观测 $o_t$ 与潜变量 $z$ 下，使真实动作序列 $a_{t:t+K}$ 的生成概率最大。工程上常用距离度量近似该原则，常见为 $MSE$ 或 $L_1$；经验上 $L_1$ 往往更稳健，许多工作报告 $L_1$ 优于 $MSE$。</p></blockquote><h4 class="relative group">二、ACT 推理<div id=二act-推理 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e4%ba%8cact-%e6%8e%a8%e7%90%86 aria-label=锚点>#</a></span></h4><p>给定：训练好的策略 $\pi_\theta$，轨迹长度（episode 长度）$T$，指数衰减系数 $m$。</p><ol><li>初始化先入先出（FIFO）缓冲区 $\mathcal{B}[0{:}T]$，其中 $\mathcal{B}[t]$ 存储对时刻 $t$ 的多次候选动作预测（来自不同起点的重叠块）。</li><li>对 $t=1,2,\ldots,T$：<ol><li>令 $z=\mathbf{0}$（使用先验均值进行推理），用 $\pi_\theta\!\left(\hat{a}_{t:t+K}\,\middle|\,o_t,z\right)$ 预测长度为 $K$ 的动作序列；</li><li>将 $\hat{a}_{t:t+K}$ 的各元素分别追加进对应槽位的缓冲区：$\mathcal{B}[t{:}t+K]$；</li><li>设 $\mathcal{B}[t]=\{A_t[i]\}_{i=0}^{N_t-1}$ 为当前步 $t$ 聚合到的候选动作集合（$N_t$ 为该槽累计的候选数）；</li><li>用指数加权平均得到最终执行动作
$$
a^{\text{exec}}_t=\frac{\sum_{i=0}^{N_t-1}w_i\,A_t[i]}{\sum_{i=0}^{N_t-1}w_i}\,,\quad
w_i=\exp(-m\cdot i)\,.
$$<blockquote><p>注：$i$ 按加入缓冲区的先后次序编号，$i=0$ 表示最早预测，$m>0$ 控制对旧候选的衰减强度。</p></blockquote></li></ol></li></ol><h4 class="relative group">三、损失函数<div id=三损失函数 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e4%b8%89%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 aria-label=锚点>#</a></span></h4><ul><li>重构损失：$\displaystyle \mathcal{L}_{\text{reconst}}=\mathrm{MSE}\!\left(\hat{a}_{t:t+K},\,a_{t:t+K}\right)$</li><li>正则损失：$\displaystyle \mathcal{L}_{\text{reg}}=D_{\mathrm{KL}}\!\bigl(q_\phi(z\,|\,a_{t:t+K},\bar{o}_t)\,\|\,\mathcal{N}(0,I)\bigr)$</li><li>总损失：$\displaystyle \mathcal{L}=\mathcal{L}_{\text{reconst}}+\beta\,\mathcal{L}_{\text{reg}}$。</li></ul><h3 class="relative group">ACT 模型架构图<div id=act-模型架构图 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#act-%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84%e5%9b%be aria-label=锚点>#</a></span></h3><blockquote class="book-hint warning"><div class=hint-content>请先学习 <a href=../%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-vae--cvae/>VAE 和 CVAE 模型</a>，再继续阅读。</div></blockquote><h4 class="relative group">ACT 训练<div id=act-训练 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#act-%e8%ae%ad%e7%bb%83 aria-label=锚点>#</a></span></h4><figure style=width:40%;max-width:100%><img src=/posts/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-act/img/ACT-Train.png style=display:block;max-width:100%;height:auto alt="ACT Training" loading=lazy><figcaption><p>ACT Training</p></figcaption></figure><p>总体思想与 CVAE 相似，但在 ACT 中，输入与输出均为动作序列（action sequence）：将 CVAE 中的“图像”角色替换为“动作序列”。同时，我们在 encoder 与 decoder 中都使用 condition（条件），即 observation（观测），包含摄像头图像与从臂的关节信息。为加快训练，encoder 的条件中不使用图像，仅使用从臂关节信息。</p><blockquote><p>为什么条件要包含从臂的关节信息？</p><ul><li>让模型专注于增量（在已知关节状态基础上的变化），而非直接从图像回归绝对位姿。</li><li>让模型感知主臂与从臂之间细微而系统性的差异。</li></ul></blockquote><h5 class="relative group">Step 1：数据与监督信号<div id=step-1数据与监督信号 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#step-1%e6%95%b0%e6%8d%ae%e4%b8%8e%e7%9b%91%e7%9d%a3%e4%bf%a1%e5%8f%b7 aria-label=锚点>#</a></span></h5><p>采集数据集。每个 batch（批次）的 input（输入）为 observation（观测，包含摄像头图像与从臂关节信息），ground-truth label（真实值标签）为 action sequence（动作序列）。该动作序列的维度等于“每步 7 维主臂动作”与 action chunk 大小 $K$ 的组合（$7\times K$ 维）。</p><h5 class="relative group">Step 2：潜变量后验（encoder）与采样<div id=step-2潜变量后验encoder与采样 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#step-2%e6%bd%9c%e5%8f%98%e9%87%8f%e5%90%8e%e9%aa%8cencoder%e4%b8%8e%e9%87%87%e6%a0%b7 aria-label=锚点>#</a></span></h5><ul><li>CLS token（分类标记）：一个可学习的 512 维向量（可视作模型参数）。</li><li>embedded joints（关节信息嵌入）：从臂 7 个关节值，经线性层映射为 512 维向量。</li><li>embedded action sequence（动作序列嵌入）：将长度为 $K$ 的动作序列逐步经线性层映射为 512 维向量，并加上 Sinusoidal Positional Encoding（正弦位置编码），得到 $K$ 个 512 维 token。</li></ul><p>因此，送入 Transformer Encoder 的共为 $K+2$ 个 512 维 token（含 1 个 CLS、1 个 joints、以及 $K$ 个 action 序列 token）。Encoder 通过 self-attention（自注意力）融合全局信息，我们取 CLS 对应的 512 维输出，经线性层得到 32 维高斯的均值与方差，随后用 reparameterization trick（重参数化技巧）采样潜变量 $z\in\mathbb{R}^{32}$。</p><h5 class="relative group">Step 3：条件解码与动作预测（decoder）<div id=step-3条件解码与动作预测decoder class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#step-3%e6%9d%a1%e4%bb%b6%e8%a7%a3%e7%a0%81%e4%b8%8e%e5%8a%a8%e4%bd%9c%e9%a2%84%e6%b5%8bdecoder aria-label=锚点>#</a></span></h5><ul><li>图像分支：4 个摄像头、每帧分辨率 480×640、RGB 三通道，经 ResNet-18 得到特征张量（例如 15×20×$C$，通常 $C\approx512$），flatten 为 $300\times C$，再经线性层映射到 512 维，并加上 Sinusoidal Positional Encoding。四路拼接共得到 $1200$ 个 512 维向量（cam1 ～ cam4 合计）。</li><li>joints 分支：与 encoder 侧相同处理，经线性层得到 512 维向量。</li><li>latent 分支：将潜变量 $z$ 经线性层映射为 512 维向量。</li></ul><p>最终，Transformer Encoder 的输入 token 数为 $1200+2=1202$ 个 512 维向量。其输出作为 key/value 供 Transformer Decoder 做 cross-attention（交叉注意力）。Transformer Decoder 接收 $K$ 个 query 并输出长度为 $K$ 的 predicted action sequence（预测动作序列）。</p><h4 class="relative group">ACT 推理<div id=act-推理 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#act-%e6%8e%a8%e7%90%86 aria-label=锚点>#</a></span></h4><figure style=width:40%;max-width:100%><img src=/posts/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-act/img/ACT-Test.png style=display:block;max-width:100%;height:auto alt="ACT Testing" loading=lazy><figcaption><p>ACT Testing</p></figcaption></figure><p>推理流程与训练阶段的 Step 3 类似，但此处直接将潜变量设为零向量 $\mathbf{0}$：在先验假设 $z\sim\mathcal{N}(0,I)$ 下，$\mathbf{0}$ 是对称中心和众数（实作中常见的推理近似），据此可直接生成 predicted action sequence（预测动作序列）作为输出。</p><h3 class="relative group">ACT 消融实验<div id=act-消融实验 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#act-%e6%b6%88%e8%9e%8d%e5%ae%9e%e9%aa%8c aria-label=锚点>#</a></span></h3><figure style=width:40%;max-width:100%><img src=/posts/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-act/img/Ablation.png style=display:block;max-width:100%;height:auto alt=Ablation loading=lazy><figcaption><p>Ablation</p></figcaption></figure><ul><li>Action Chunk（动作分块）的有效性
将“单步预测”扩展为“一次预测 $K$ 步”能显著提升成功率。随着 chunk size 从 1 增至接近整段（≈400 步），性能总体上升；当 chunk 过长时略有回落，可能由于一次性输出过长、与环境交互与反馈不足。</li><li>Temporal Ensemble（时间集成）
对相邻输出片段在重叠区加权平均，可带来稳定的小幅增益，使动作更连贯。</li><li>CVAE 目标（$KL$ 对齐）的作用
对潜变量 $z$ 施加 $KL$ 正则，使其与标准正态对齐：<ul><li>脚本演示数据：去掉 $KL$ 影响较小，因数据更规整、噪声少。</li><li>人类演示数据：去掉 $KL$ 后性能显著下降，说明 $KL$ 正则对建模人类噪声/多样性至关重要。</li></ul></li><li>远程操作的控制频率与数据采集效率
提高示教/遥操作频率可显著缩短单次演示时长：例如从 5 Hz 提升至 50 Hz，采集同一任务由约 30–40 s 降至 ≈20 s。作者建议高控制频率对高效采集至关重要。</li></ul><h2 class="relative group">实践<div id=实践 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e5%ae%9e%e8%b7%b5 aria-label=锚点>#</a></span></h2><p>标准 ACT 通常按单任务训练，跨任务泛化有限，且不内置语言接口；若需多任务或语言条件，可在更高层做扩展。</p><p>ACT 相对其他模仿学习方法，具有：</p><ul><li>减少复合误差：通过预测动作块降低误差累积；</li><li>提高成功率：在精细操作任务上表现优异；</li><li>端到端训练：无需手工设计特征；</li><li>多模态融合：有效融合视觉与状态信息。</li></ul><h3 class="relative group">LeRobot ACT 代码阅读<div id=lerobot-act-代码阅读 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#lerobot-act-%e4%bb%a3%e7%a0%81%e9%98%85%e8%af%bb aria-label=锚点>#</a></span></h3><blockquote class="book-hint danger"><div class=hint-content><a href=https://github.com/huggingface/lerobot target=_blank>LeRobot 仓库</a>经常性变动，请务必参考官方教程和仓库代码；本文撰写时版本为 <code>d57d1aa1</code>（2025-10-31）。</div></blockquote><div class=highlight-wrapper data-code-lang=shell><div class=codeblock-title><span class=codeblock-lang>SHELL</span><button type=button class=codeblock-wrap-toggle data-code-wrap-toggle aria-label=切换代码块换行模式 aria-pressed=true>不换行</button></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl> lerobot/src/lerobot/policies/act/
</span></span><span class=line><span class=cl>├──  configuration_act.py
</span></span><span class=line><span class=cl>├──  modeling_act.py
</span></span><span class=line><span class=cl>├──  processor_act.py
</span></span><span class=line><span class=cl>└── 󰂺 README.md -&gt; ../../../../docs/source/policy_act_README.md</span></span></code></pre></div></div><p>LeRobot 的 ACT 由 <code>configuration_act.py</code>、<code>modeling_act.py</code>、<code>processor_act.py</code> 三个文件实现。</p><h3 class="relative group"><code>configuration_act.py</code>：ACT 配置<div id=configuration_actpyact-配置 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#configuration_actpyact-%e9%85%8d%e7%bd%ae aria-label=锚点>#</a></span></h3><div class=highlight-wrapper data-code-lang=python><div class=codeblock-title><span class=codeblock-lang>PYTHON</span><button type=button class=codeblock-wrap-toggle data-code-wrap-toggle aria-label=切换代码块换行模式 aria-pressed=true>不换行</button></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=ch>#!/usr/bin/env python</span>
</span></span><span class=line><span class=cl><span class=c1># Copyright 2024 Tony Z. Zhao and The HuggingFace Inc. team. All rights reserved.</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1># Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);</span>
</span></span><span class=line><span class=cl><span class=c1># 以上是版权与开源许可声明，表明本代码遵循 Apache 2.0 许可证</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dataclasses</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>dataclass</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>field</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>  <span class=c1># dataclass 用于简化配置类的定义，field 可定义默认值/工厂</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>lerobot.configs.policies</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>PreTrainedConfig</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>  <span class=c1># 项目内基类：预训练策略的通用配置抽象</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>lerobot.configs.types</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>NormalizationMode</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>  <span class=c1># 枚举：输入/输出的归一化模式（如 MEAN_STD、MIN_MAX）</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>lerobot.optim.optimizers</span> <span class=kn>import</span> <span class=n>AdamWConfig</span>  <span class=c1># 优化器配置对象（AdamW 的超参集合）</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 使用注册器将该配置类注册为 &#34;act&#34; 类型，便于通过字符串查找/构造对应配置。</span>
</span></span><span class=line><span class=cl><span class=nd>@PreTrainedConfig.register_subclass</span><span class=p>(</span><span class=s2>&#34;act&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nd>@dataclass</span>  <span class=c1># dataclass 会自动生成 __init__/__repr__/__eq__ 等，从字段定义中推导构造参数</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ACTConfig</span><span class=p>(</span><span class=n>PreTrainedConfig</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Configuration class for the Action Chunking Transformers policy.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Defaults are configured for training on bimanual Aloha tasks like &#34;insertion&#34; or &#34;transfer&#34;.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    The parameters you will most likely need to change are the ones which depend on the environment / sensors.
</span></span></span><span class=line><span class=cl><span class=s2>    Those are: `input_shapes` and &#39;output_shapes`.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Notes on the inputs and outputs:
</span></span></span><span class=line><span class=cl><span class=s2>        - Either:
</span></span></span><span class=line><span class=cl><span class=s2>            - At least one key starting with &#34;observation.image is required as an input.
</span></span></span><span class=line><span class=cl><span class=s2>              AND/OR
</span></span></span><span class=line><span class=cl><span class=s2>            - The key &#34;observation.environment_state&#34; is required as input.
</span></span></span><span class=line><span class=cl><span class=s2>        - If there are multiple keys beginning with &#34;observation.images.&#34; they are treated as multiple camera
</span></span></span><span class=line><span class=cl><span class=s2>          views. Right now we only support all images having the same shape.
</span></span></span><span class=line><span class=cl><span class=s2>        - May optionally work without an &#34;observation.state&#34; key for the proprioceptive robot state.
</span></span></span><span class=line><span class=cl><span class=s2>        - &#34;action&#34; is required as an output key.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        n_obs_steps: Number of environment steps worth of observations to pass to the policy (takes the
</span></span></span><span class=line><span class=cl><span class=s2>            current step and additional steps going back).
</span></span></span><span class=line><span class=cl><span class=s2>        chunk_size: The size of the action prediction &#34;chunks&#34; in units of environment steps.
</span></span></span><span class=line><span class=cl><span class=s2>        n_action_steps: The number of action steps to run in the environment for one invocation of the policy.
</span></span></span><span class=line><span class=cl><span class=s2>            This should be no greater than the chunk size. For example, if the chunk size size 100, you may
</span></span></span><span class=line><span class=cl><span class=s2>            set this to 50. This would mean that the model predicts 100 steps worth of actions, runs 50 in the
</span></span></span><span class=line><span class=cl><span class=s2>            environment, and throws the other 50 out.
</span></span></span><span class=line><span class=cl><span class=s2>        input_shapes: A dictionary defining the shapes of the input data for the policy. The key represents
</span></span></span><span class=line><span class=cl><span class=s2>            the input data name, and the value is a list indicating the dimensions of the corresponding data.
</span></span></span><span class=line><span class=cl><span class=s2>            For example, &#34;observation.image&#34; refers to an input from a camera with dimensions [3, 96, 96],
</span></span></span><span class=line><span class=cl><span class=s2>            indicating it has three color channels and 96x96 resolution. Importantly, `input_shapes` doesn&#39;t
</span></span></span><span class=line><span class=cl><span class=s2>            include batch dimension or temporal dimension.
</span></span></span><span class=line><span class=cl><span class=s2>        output_shapes: A dictionary defining the shapes of the output data for the policy. The key represents
</span></span></span><span class=line><span class=cl><span class=s2>            the output data name, and the value is a list indicating the dimensions of the corresponding data.
</span></span></span><span class=line><span class=cl><span class=s2>            For example, &#34;action&#34; refers to an output shape of [14], indicating 14-dimensional actions.
</span></span></span><span class=line><span class=cl><span class=s2>            Importantly, `output_shapes` doesn&#39;t include batch dimension or temporal dimension.
</span></span></span><span class=line><span class=cl><span class=s2>        input_normalization_modes: A dictionary with key representing the modality (e.g. &#34;observation.state&#34;),
</span></span></span><span class=line><span class=cl><span class=s2>            and the value specifies the normalization mode to apply. The two available modes are &#34;mean_std&#34;
</span></span></span><span class=line><span class=cl><span class=s2>            which subtracts the mean and divides by the standard deviation and &#34;min_max&#34; which rescale in a
</span></span></span><span class=line><span class=cl><span class=s2>            [-1, 1] range.
</span></span></span><span class=line><span class=cl><span class=s2>        output_normalization_modes: Similar dictionary as `normalize_input_modes`, but to unnormalize to the
</span></span></span><span class=line><span class=cl><span class=s2>            original scale. Note that this is also used for normalizing the training targets.
</span></span></span><span class=line><span class=cl><span class=s2>        vision_backbone: Name of the torchvision resnet backbone to use for encoding images.
</span></span></span><span class=line><span class=cl><span class=s2>        pretrained_backbone_weights: Pretrained weights from torchvision to initialize the backbone.
</span></span></span><span class=line><span class=cl><span class=s2>            `None` means no pretrained weights.
</span></span></span><span class=line><span class=cl><span class=s2>        replace_final_stride_with_dilation: Whether to replace the ResNet&#39;s final 2x2 stride with a dilated
</span></span></span><span class=line><span class=cl><span class=s2>            convolution.
</span></span></span><span class=line><span class=cl><span class=s2>        pre_norm: Whether to use &#34;pre-norm&#34; in the transformer blocks.
</span></span></span><span class=line><span class=cl><span class=s2>        dim_model: The transformer blocks&#39; main hidden dimension.
</span></span></span><span class=line><span class=cl><span class=s2>        n_heads: The number of heads to use in the transformer blocks&#39; multi-head attention.
</span></span></span><span class=line><span class=cl><span class=s2>        dim_feedforward: The dimension to expand the transformer&#39;s hidden dimension to in the feed-forward
</span></span></span><span class=line><span class=cl><span class=s2>            layers.
</span></span></span><span class=line><span class=cl><span class=s2>        feedforward_activation: The activation to use in the transformer block&#39;s feed-forward layers.
</span></span></span><span class=line><span class=cl><span class=s2>        n_encoder_layers: The number of transformer layers to use for the transformer encoder.
</span></span></span><span class=line><span class=cl><span class=s2>        n_decoder_layers: The number of transformer layers to use for the transformer decoder.
</span></span></span><span class=line><span class=cl><span class=s2>        use_vae: Whether to use a variational objective during training. This introduces another transformer
</span></span></span><span class=line><span class=cl><span class=s2>            which is used as the VAE&#39;s encoder (not to be confused with the transformer encoder - see
</span></span></span><span class=line><span class=cl><span class=s2>            documentation in the policy class).
</span></span></span><span class=line><span class=cl><span class=s2>        latent_dim: The VAE&#39;s latent dimension.
</span></span></span><span class=line><span class=cl><span class=s2>        n_vae_encoder_layers: The number of transformer layers to use for the VAE&#39;s encoder.
</span></span></span><span class=line><span class=cl><span class=s2>        temporal_ensemble_coeff: Coefficient for the exponential weighting scheme to apply for temporal
</span></span></span><span class=line><span class=cl><span class=s2>            ensembling. Defaults to None which means temporal ensembling is not used. `n_action_steps` must be
</span></span></span><span class=line><span class=cl><span class=s2>            1 when using this feature, as inference needs to happen at every step to form an ensemble. For
</span></span></span><span class=line><span class=cl><span class=s2>            more information on how ensembling works, please see `ACTTemporalEnsembler`.
</span></span></span><span class=line><span class=cl><span class=s2>        dropout: Dropout to use in the transformer layers (see code for details).
</span></span></span><span class=line><span class=cl><span class=s2>        kl_weight: The weight to use for the KL-divergence component of the loss if the variational objective
</span></span></span><span class=line><span class=cl><span class=s2>            is enabled. Loss is then calculated as: `reconstruction_loss + kl_weight * kld_loss`.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 以上三引号是类的文档字符串（docstring），用于解释该配置类的用途和各参数含义。</span>
</span></span><span class=line><span class=cl>    <span class=c1># 文档内提到的 `input_shapes` / `output_shapes` 等键，属于父类/策略使用的约定。</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Input / output structure.</span>
</span></span><span class=line><span class=cl>    <span class=c1># 观测/动作的时序结构配置</span>
</span></span><span class=line><span class=cl>    <span class=n>n_obs_steps</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>1</span>  <span class=c1># 传入策略的观测步数（时间维度），目前实现只支持 1（当前步）</span>
</span></span><span class=line><span class=cl>    <span class=n>chunk_size</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>100</span>  <span class=c1># 一次预测的“动作块”的长度（以环境步数计）</span>
</span></span><span class=line><span class=cl>    <span class=n>n_action_steps</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=mi>100</span>  <span class=c1># 每次调用策略实际执行到环境中的动作步数，不能超过 chunk_size</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 归一化模式的默认映射：按模态选择 NormalizationMode</span>
</span></span><span class=line><span class=cl>    <span class=n>normalization_mapping</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>NormalizationMode</span><span class=p>]</span> <span class=o>=</span> <span class=n>field</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>default_factory</span><span class=o>=</span><span class=k>lambda</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;VISUAL&#34;</span><span class=p>:</span> <span class=n>NormalizationMode</span><span class=o>.</span><span class=n>MEAN_STD</span><span class=p>,</span>  <span class=c1># 图像模态：减均值/除方差</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;STATE&#34;</span><span class=p>:</span> <span class=n>NormalizationMode</span><span class=o>.</span><span class=n>MEAN_STD</span><span class=p>,</span>  <span class=c1># 状态模态：减均值/除方差</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;ACTION&#34;</span><span class=p>:</span> <span class=n>NormalizationMode</span><span class=o>.</span><span class=n>MEAN_STD</span><span class=p>,</span>  <span class=c1># 动作模态：减均值/除方差（训练目标也会用到）</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Architecture.</span>
</span></span><span class=line><span class=cl>    <span class=c1># Vision backbone.</span>
</span></span><span class=line><span class=cl>    <span class=n>vision_backbone</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;resnet18&#34;</span>  <span class=c1># 图像编码使用的 ResNet 主干名称（需为 torchvision 的 resnet 变体）</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>pretrained_backbone_weights</span><span class=p>:</span> <span class=nb>str</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;ResNet18_Weights.IMAGENET1K_V1&#34;</span>  <span class=c1># 主干的预训练权重标识；None 表示不加载</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>replace_final_stride_with_dilation</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=kc>False</span>  <span class=c1># 是否用空洞卷积替换 ResNet 最后一个 2x2 stride（类型标注为 int，但实际布尔使用）</span>
</span></span><span class=line><span class=cl>    <span class=c1># Transformer layers.</span>
</span></span><span class=line><span class=cl>    <span class=n>pre_norm</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span>  <span class=c1># Transformer 是否使用 pre-norm 结构（LayerNorm 在子层之前）</span>
</span></span><span class=line><span class=cl>    <span class=n>dim_model</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>512</span>  <span class=c1># Transformer 主通道隐藏维度 d_model</span>
</span></span><span class=line><span class=cl>    <span class=n>n_heads</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>8</span>  <span class=c1># 多头注意力的头数</span>
</span></span><span class=line><span class=cl>    <span class=n>dim_feedforward</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>3200</span>  <span class=c1># 前馈网络的扩展维度（通常为 d_model 的若干倍）</span>
</span></span><span class=line><span class=cl>    <span class=n>feedforward_activation</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;relu&#34;</span>  <span class=c1># 前馈网络的激活函数类型</span>
</span></span><span class=line><span class=cl>    <span class=n>n_encoder_layers</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>4</span>  <span class=c1># Transformer 编码器层数</span>
</span></span><span class=line><span class=cl>    <span class=c1># Note: Although the original ACT implementation has 7 for `n_decoder_layers`, there is a bug in the code</span>
</span></span><span class=line><span class=cl>    <span class=c1># that means only the first layer is used. Here we match the original implementation by setting this to 1.</span>
</span></span><span class=line><span class=cl>    <span class=c1># See this issue https://github.com/tonyzhaozh/act/issues/25#issue-2258740521.</span>
</span></span><span class=line><span class=cl>    <span class=n>n_decoder_layers</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=mi>1</span>  <span class=c1># Transformer 解码器层数（按原实现的实际效果设置为 1，以对齐行为）</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># VAE.</span>
</span></span><span class=line><span class=cl>    <span class=n>use_vae</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=kc>True</span>  <span class=c1># 训练时是否使用 VAE 目标（引入额外 Transformer 作为 VAE 编码器）</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>latent_dim</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>32</span>  <span class=c1># VAE 潜变量维度</span>
</span></span><span class=line><span class=cl>    <span class=n>n_vae_encoder_layers</span><span class=p>:</span> <span class=nb>int</span> <span class=o>=</span> <span class=mi>4</span>  <span class=c1># VAE 编码器的 Transformer 层数</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Inference.</span>
</span></span><span class=line><span class=cl>    <span class=c1># Note: the value used in ACT when temporal ensembling is enabled is 0.01.</span>
</span></span><span class=line><span class=cl>    <span class=n>temporal_ensemble_coeff</span><span class=p>:</span> <span class=nb>float</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=kc>None</span>  <span class=c1># 时间集成（temporal ensembling）的指数加权系数；None 表示关闭</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Training and loss computation.</span>
</span></span><span class=line><span class=cl>    <span class=n>dropout</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>0.1</span>  <span class=c1># Transformer 层内的 dropout 比例（防止过拟合）</span>
</span></span><span class=line><span class=cl>    <span class=n>kl_weight</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=mf>10.0</span>  <span class=c1># 使用 VAE 时，KL 散度项的损失权重（总损失 = 重构损失 + kl_weight * KL）</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Training preset</span>
</span></span><span class=line><span class=cl>    <span class=c1># 训练预设：优化器相关超参数</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer_lr</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>1e-5</span>  <span class=c1># 主体学习率</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer_weight_decay</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=mf>1e-4</span>  <span class=c1># 权重衰减（L2 正则）</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer_lr_backbone</span><span class=p>:</span> <span class=nb>float</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=mf>1e-5</span>  <span class=c1># 视觉主干的学习率（可能与主体不同，用于微调/冻结策略）</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>__post_init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># dataclass 的钩子：在 __init__ 之后自动调用。</span>
</span></span><span class=line><span class=cl>        <span class=c1># 这里首先调用父类的 __post_init__ 来完成通用初始化（如解析输入/输出特征等）。</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=n>__post_init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Input validation (not exhaustive).&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># ——以下是对配置进行基本校验的逻辑（非穷尽）——</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 校验视觉主干名称：必须是 ResNet 家族，否则抛出 ValueError</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>vision_backbone</span><span class=o>.</span><span class=n>startswith</span><span class=p>(</span><span class=s2>&#34;resnet&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=sa>f</span><span class=s2>&#34;`vision_backbone` must be one of the ResNet variants. Got </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>vision_backbone</span><span class=si>}</span><span class=s2>.&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 若启用时间集成（temporal_ensemble_coeff 非 None），则 n_action_steps 必须为 1</span>
</span></span><span class=line><span class=cl>        <span class=c1># 原因：时间集成需要每一步都查询策略以形成集成</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>temporal_ensemble_coeff</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>n_action_steps</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>NotImplementedError</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;`n_action_steps` must be 1 when using temporal ensembling. This is &#34;</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;because the policy needs to be queried every step to compute the ensembled action.&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># n_action_steps 不能超过 chunk_size（一次调用预测的最大可用步数上限）</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>n_action_steps</span> <span class=o>&gt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>chunk_size</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=sa>f</span><span class=s2>&#34;The chunk size is the upper bound for the number of action steps per model invocation. Got &#34;</span>
</span></span><span class=line><span class=cl>                <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>n_action_steps</span><span class=si>}</span><span class=s2> for `n_action_steps` and </span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>chunk_size</span><span class=si>}</span><span class=s2> for `chunk_size`.&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 目前实现不支持多观测步（时间窗口 &gt; 1）</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>n_obs_steps</span> <span class=o>!=</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=sa>f</span><span class=s2>&#34;Multiple observation steps not handled yet. Got `nobs_steps=</span><span class=si>{</span><span class=bp>self</span><span class=o>.</span><span class=n>n_obs_steps</span><span class=si>}</span><span class=s2>`&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_optimizer_preset</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>AdamWConfig</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 返回一个 AdamW 优化器的配置预设，供上层训练器构造实际优化器实例</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>AdamWConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>lr</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>optimizer_lr</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>weight_decay</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>optimizer_weight_decay</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_scheduler_preset</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 返回 None 表示不使用学习率调度器（或由外部训练脚本自行指定）</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>validate_features</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 检查特征输入是否满足最小要求：</span>
</span></span><span class=line><span class=cl>        <span class=c1># 必须至少提供一种图像特征（来自摄像头）或环境状态特征。</span>
</span></span><span class=line><span class=cl>        <span class=c1># 这些属性（image_features、env_state_feature）通常在父类 __post_init__ 中解析并赋值。</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>image_features</span> <span class=ow>and</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>env_state_feature</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>raise</span> <span class=ne>ValueError</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;You must provide at least one image or the environment state among the inputs.&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>observation_delta_indices</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 观测的“增量索引”定义（若用于计算时间差分等）。这里返回 None，表示不定义/不使用。</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>action_delta_indices</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 动作的“增量索引”定义：这里返回 [0, 1, ..., chunk_size-1]</span>
</span></span><span class=line><span class=cl>        <span class=c1># 常见用途：指示哪些时间步上的动作需要被预测/计算，或用于构建目标序列的索引。</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>list</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>chunk_size</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@property</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>reward_delta_indices</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 奖励的“增量索引”定义：此处不使用奖励差分（返回 None）</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>None</span></span></span></code></pre></div></div><h3 class="relative group"><code>processor_act.py</code>：ACT 流水线<div id=processor_actpyact-流水线 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#processor_actpyact-%e6%b5%81%e6%b0%b4%e7%ba%bf aria-label=锚点>#</a></span></h3><div class=highlight-wrapper data-code-lang=python><div class=codeblock-title><span class=codeblock-lang>PYTHON</span><button type=button class=codeblock-wrap-toggle data-code-wrap-toggle aria-label=切换代码块换行模式 aria-pressed=true>不换行</button></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=ch>#!/usr/bin/env python</span>
</span></span><span class=line><span class=cl><span class=c1># Copyright 2024 Tony Z. Zhao and The HuggingFace Inc. team. All rights reserved.</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1># Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);</span>
</span></span><span class=line><span class=cl><span class=c1># 以上是版权与开源许可声明，表明本代码遵循 Apache 2.0 许可证</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 小结：</span>
</span></span><span class=line><span class=cl><span class=c1># - 本文件的核心函数 make_act_pre_post_processors 会根据 ACTConfig 和可选的数据集统计，构造两个流水线对象：</span>
</span></span><span class=line><span class=cl><span class=c1>#   1) 前处理流水线（PRE）：重命名 -&gt; 加 batch 维 -&gt; 移到设备 -&gt; 归一化</span>
</span></span><span class=line><span class=cl><span class=c1>#   2) 后处理流水线（POST）：反归一化 -&gt; 移回 CPU</span>
</span></span><span class=line><span class=cl><span class=c1># - 这样做的好处：将数据工程与模型推理解耦，保证输入输出的形状、设备与数值尺度都符合模型与下游使用方的预期</span>
</span></span><span class=line><span class=cl><span class=c1># - features 与 norm_map 的一致性非常重要：保证前处理与后处理的变换可逆且匹配</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># limitations under the License.</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>Any</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>  <span class=c1># 从 typing 导入 Any，表示“任意类型”，常用于类型提示中表示通用容器</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>  <span class=c1># 导入 PyTorch，用于张量（Tensor）及设备（device）管理</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>lerobot.policies.act.configuration_act</span> <span class=kn>import</span> <span class=n>ACTConfig</span>  <span class=c1># 导入 ACT 策略的配置类</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 从 lerobot.processor 导入一系列“处理步骤（ProcessorStep）”与管道（Pipeline）相关类</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>lerobot.processor</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>AddBatchDimensionProcessorStep</span><span class=p>,</span>  <span class=c1># 处理步骤：为输入添加 batch 维度（例如从 [C,H,W] 变为 [B,C,H,W]）</span>
</span></span><span class=line><span class=cl>    <span class=n>DeviceProcessorStep</span><span class=p>,</span>  <span class=c1># 处理步骤：将数据移动到指定设备（如 &#34;cuda:0&#34; 或 &#34;cpu&#34;）</span>
</span></span><span class=line><span class=cl>    <span class=n>NormalizerProcessorStep</span><span class=p>,</span>  <span class=c1># 处理步骤：对特征做归一化（根据统计量，如均值/方差）</span>
</span></span><span class=line><span class=cl>    <span class=n>PolicyAction</span><span class=p>,</span>  <span class=c1># 策略输出动作的数据结构（类型别名/封装）</span>
</span></span><span class=line><span class=cl>    <span class=n>PolicyProcessorPipeline</span><span class=p>,</span>  <span class=c1># 通用的策略处理“流水线”定义，包含一系列有序步骤</span>
</span></span><span class=line><span class=cl>    <span class=n>RenameObservationsProcessorStep</span><span class=p>,</span>  <span class=c1># 处理步骤：重命名观测字典中的键（key），以适配模型预期的输入名</span>
</span></span><span class=line><span class=cl>    <span class=n>UnnormalizerProcessorStep</span><span class=p>,</span>  <span class=c1># 处理步骤：把归一化后的输出反归一化回原始尺度</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>lerobot.processor.converters</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>policy_action_to_transition</span><span class=p>,</span>  <span class=c1># 转换函数：将策略动作结构转为“transition”结构（过渡/样本格式）</span>
</span></span><span class=line><span class=cl>    <span class=n>transition_to_policy_action</span><span class=p>,</span>  <span class=c1># 转换函数：与上相反，将 transition 转回策略动作结构</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>lerobot.utils.constants</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>POLICY_POSTPROCESSOR_DEFAULT_NAME</span><span class=p>,</span>  <span class=c1># 常量：后处理流水线的默认命名</span>
</span></span><span class=line><span class=cl>    <span class=n>POLICY_PREPROCESSOR_DEFAULT_NAME</span><span class=p>,</span>  <span class=c1># 常量：前处理流水线的默认命名</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>make_act_pre_post_processors</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>config</span><span class=p>:</span> <span class=n>ACTConfig</span><span class=p>,</span>  <span class=c1># ACT 策略配置对象，内含设备、特征配置、归一化映射关系等信息</span>
</span></span><span class=line><span class=cl>    <span class=n>dataset_stats</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>    <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>  <span class=c1># 数据集统计信息（如 mean/std），按特征名组织；可为 None</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>tuple</span><span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>PolicyProcessorPipeline</span><span class=p>[</span><span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>],</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>    <span class=n>PolicyProcessorPipeline</span><span class=p>[</span><span class=n>PolicyAction</span><span class=p>,</span> <span class=n>PolicyAction</span><span class=p>],</span>
</span></span><span class=line><span class=cl><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Creates the pre- and post-processing pipelines for the ACT policy.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    The pre-processing pipeline handles normalization, batching, and device placement for the model inputs.
</span></span></span><span class=line><span class=cl><span class=s2>    The post-processing pipeline handles unnormalization and moves the model outputs back to the CPU.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        config (ACTConfig): The ACT policy configuration object.
</span></span></span><span class=line><span class=cl><span class=s2>        dataset_stats (dict[str, dict[str, torch.Tensor]] | None): A dictionary containing dataset
</span></span></span><span class=line><span class=cl><span class=s2>            statistics (e.g., mean and std) used for normalization. Defaults to None.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        tuple[PolicyProcessorPipeline[dict[str, Any], dict[str, Any]], PolicyProcessorPipeline[PolicyAction, PolicyAction]]: A tuple containing the
</span></span></span><span class=line><span class=cl><span class=s2>        pre-processor pipeline and the post-processor pipeline.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 上面的英文文档说明：</span>
</span></span><span class=line><span class=cl>    <span class=c1># - 本函数构造并返回“前处理（pre）”与“后处理（post）”两个流水线，用于 ACT 策略的输入和输出数据处理。</span>
</span></span><span class=line><span class=cl>    <span class=c1># - 前处理：将原始观测做重命名、补齐 batch 维度、移动到指定设备、并按数据集统计进行归一化，使之适配模型输入。</span>
</span></span><span class=line><span class=cl>    <span class=c1># - 后处理：对模型输出做反归一化（还原到原尺度），并移动回 CPU（便于后续使用或与非 GPU 代码交互）。</span>
</span></span><span class=line><span class=cl>    <span class=c1># - dataset_stats：通常包含每个特征的 mean/std，用于 Normalizer/Unnormalizer；为 None 时可能使用默认策略或跳过部分操作。</span>
</span></span><span class=line><span class=cl>    <span class=c1># - 返回值是一个元组：(前处理流水线, 后处理流水线)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 定义前处理流水线中包含的“步骤”列表（按顺序执行）</span>
</span></span><span class=line><span class=cl>    <span class=n>input_steps</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>RenameObservationsProcessorStep</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>rename_map</span><span class=o>=</span><span class=p>{}</span>
</span></span><span class=line><span class=cl>        <span class=p>),</span>  <span class=c1># 重命名观测键的步骤：这里给了空映射，表示当前不需要改名</span>
</span></span><span class=line><span class=cl>        <span class=n>AddBatchDimensionProcessorStep</span><span class=p>(),</span>  <span class=c1># 添加 batch 维：当输入是单样本时，变为批大小为 1 的张量，便于模型统一处理</span>
</span></span><span class=line><span class=cl>        <span class=n>DeviceProcessorStep</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>device</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>device</span>
</span></span><span class=line><span class=cl>        <span class=p>),</span>  <span class=c1># 设备迁移：将（可能包含张量的）输入移动到 config.device（如 &#34;cuda&#34; 或 &#34;cpu&#34;）</span>
</span></span><span class=line><span class=cl>        <span class=n>NormalizerProcessorStep</span><span class=p>(</span>  <span class=c1># 归一化步骤：对输入/输出特征集合按 norm_map 和 stats 做标准化/归一化</span>
</span></span><span class=line><span class=cl>            <span class=n>features</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=o>**</span><span class=n>config</span><span class=o>.</span><span class=n>input_features</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=o>**</span><span class=n>config</span><span class=o>.</span><span class=n>output_features</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>},</span>  <span class=c1># 指定需要归一化的特征集合：将输入与输出特征合并</span>
</span></span><span class=line><span class=cl>            <span class=n>norm_map</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>normalization_mapping</span><span class=p>,</span>  <span class=c1># 指定特征名到“归一化配置/方式”的映射（例如使用哪组统计量）</span>
</span></span><span class=line><span class=cl>            <span class=n>stats</span><span class=o>=</span><span class=n>dataset_stats</span><span class=p>,</span>  <span class=c1># 数据集统计（如 mean/std），用于归一化参数</span>
</span></span><span class=line><span class=cl>            <span class=n>device</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>device</span><span class=p>,</span>  <span class=c1># 将统计量和运算放在相同设备上，避免跨设备拷贝/错误</span>
</span></span><span class=line><span class=cl>        <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=c1># 以上前处理的意图：</span>
</span></span><span class=line><span class=cl>    <span class=c1># 1) RenameObservationsProcessorStep：有些数据集的键名与模型期望不一致，通过重命名统一接口（此处为空映射，意味着保持原样）</span>
</span></span><span class=line><span class=cl>    <span class=c1># 2) AddBatchDimensionProcessorStep：即便是单条数据也要添加 batch 维度，满足大多数深度学习模型形状要求</span>
</span></span><span class=line><span class=cl>    <span class=c1># 3) DeviceProcessorStep：统一把数据移动到 config 指定的设备（GPU/CPU），确保后续张量运算在同一设备上</span>
</span></span><span class=line><span class=cl>    <span class=c1># 4) NormalizerProcessorStep：将输入（甚至包含模型要预测的目标特征）进行标准化，使训练/推理更稳定</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 定义后处理流水线步骤：将模型输出从标准化空间映射回原空间，并迁移到 CPU</span>
</span></span><span class=line><span class=cl>    <span class=n>output_steps</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>UnnormalizerProcessorStep</span><span class=p>(</span>  <span class=c1># 反归一化：把模型输出（先前按统计量标准化过）还原到原始数值范围</span>
</span></span><span class=line><span class=cl>            <span class=n>features</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>output_features</span><span class=p>,</span>  <span class=c1># 仅对输出相关的特征进行反归一化（不会动输入特征）</span>
</span></span><span class=line><span class=cl>            <span class=n>norm_map</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>normalization_mapping</span><span class=p>,</span>  <span class=c1># 使用与前处理一致的归一化映射表，保证前后处理对齐</span>
</span></span><span class=line><span class=cl>            <span class=n>stats</span><span class=o>=</span><span class=n>dataset_stats</span><span class=p>,</span>  <span class=c1># 使用相同的数据集统计参数进行反变换</span>
</span></span><span class=line><span class=cl>        <span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>DeviceProcessorStep</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>device</span><span class=o>=</span><span class=s2>&#34;cpu&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>),</span>  <span class=c1># 将最终结果统一移回 CPU：便于日志记录、与非 GPU 组件交互或序列化</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 返回前/后处理两个 PolicyProcessorPipeline 实例：</span>
</span></span><span class=line><span class=cl>    <span class=c1># - 对于前处理流水线：输入与输出都是字典（键到任意类型），因为输入通常是多模态观测的字典结构</span>
</span></span><span class=line><span class=cl>    <span class=c1># - 对于后处理流水线：输入与输出是 PolicyAction（策略动作）结构/对象</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>PolicyProcessorPipeline</span><span class=p>[</span><span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>],</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Any</span><span class=p>]](</span>
</span></span><span class=line><span class=cl>            <span class=n>steps</span><span class=o>=</span><span class=n>input_steps</span><span class=p>,</span>  <span class=c1># 指定流水线包含的步骤序列</span>
</span></span><span class=line><span class=cl>            <span class=n>name</span><span class=o>=</span><span class=n>POLICY_PREPROCESSOR_DEFAULT_NAME</span><span class=p>,</span>  <span class=c1># 使用默认的“前处理”名称，便于日志或调试</span>
</span></span><span class=line><span class=cl>        <span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>PolicyProcessorPipeline</span><span class=p>[</span><span class=n>PolicyAction</span><span class=p>,</span> <span class=n>PolicyAction</span><span class=p>](</span>
</span></span><span class=line><span class=cl>            <span class=n>steps</span><span class=o>=</span><span class=n>output_steps</span><span class=p>,</span>  <span class=c1># 指定后处理步骤序列</span>
</span></span><span class=line><span class=cl>            <span class=n>name</span><span class=o>=</span><span class=n>POLICY_POSTPROCESSOR_DEFAULT_NAME</span><span class=p>,</span>  <span class=c1># 使用默认的“后处理”名称</span>
</span></span><span class=line><span class=cl>            <span class=n>to_transition</span><span class=o>=</span><span class=n>policy_action_to_transition</span><span class=p>,</span>  <span class=c1># 指定如何把 PolicyAction 转换为 transition（内部可能用于统一接口）</span>
</span></span><span class=line><span class=cl>            <span class=n>to_output</span><span class=o>=</span><span class=n>transition_to_policy_action</span><span class=p>,</span>  <span class=c1># 指定如何把 transition 转回 PolicyAction（与上相反的方向）</span>
</span></span><span class=line><span class=cl>        <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span></span></span></code></pre></div></div><h3 class="relative group"><code>modeling_act.py</code>：ACT 模型<div id=modeling_actpyact-模型 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#modeling_actpyact-%e6%a8%a1%e5%9e%8b aria-label=锚点>#</a></span></h3><div class=highlight-wrapper data-code-lang=python><div class=codeblock-title><span class=codeblock-lang>PYTHON</span><button type=button class=codeblock-wrap-toggle data-code-wrap-toggle aria-label=切换代码块换行模式 aria-pressed=true>不换行</button></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=ch>#!/usr/bin/env python</span>
</span></span><span class=line><span class=cl><span class=c1># Copyright 2024 Tony Z. Zhao and The HuggingFace Inc. team. All rights reserved.</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1># Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);</span>
</span></span><span class=line><span class=cl><span class=c1># you may not use this file except in compliance with the License.</span>
</span></span><span class=line><span class=cl><span class=c1># You may obtain a copy of the License at</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1>#     http://www.apache.org/licenses/LICENSE-2.0</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1># Unless required by applicable law or agreed to in writing, software</span>
</span></span><span class=line><span class=cl><span class=c1># distributed under the License is distributed on an &#34;AS IS&#34; BASIS,</span>
</span></span><span class=line><span class=cl><span class=c1># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
</span></span><span class=line><span class=cl><span class=c1># See the License for the specific language governing permissions and</span>
</span></span><span class=line><span class=cl><span class=c1># limitations under the License.</span>
</span></span><span class=line><span class=cl><span class=c1># 以上是版权与开源许可声明，表明本代码遵循 Apache 2.0 许可证</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;Action Chunking Transformer Policy
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>As per Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware (https://huggingface.co/papers/2304.13705).
</span></span></span><span class=line><span class=cl><span class=s2>The majority of changes here involve removing unused code, unifying naming, and adding helpful comments.
</span></span></span><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 说明：</span>
</span></span><span class=line><span class=cl><span class=c1># 本文件实现 ACT（Action Chunking Transformer）策略与其底层神经网络。ACT 旨在在机器人强化学习/模仿学习中，</span>
</span></span><span class=line><span class=cl><span class=c1># 一次性预测一段连续的动作序列（“动作块”/chunk），以减少每步都要前向一次模型的开销，并改善时序一致性。</span>
</span></span><span class=line><span class=cl><span class=c1># 代码支持两种模式：</span>
</span></span><span class=line><span class=cl><span class=c1>#   1) 纯 Transformer 预测动作序列；</span>
</span></span><span class=line><span class=cl><span class=c1>#   2) 可选 VAE（变分自编码器）训练目标：使用一个 VAE encoder 产生隐变量，再用 Transformer（此时相当于 VAE 的解码器）预测动作序列。</span>
</span></span><span class=line><span class=cl><span class=c1># 另外支持可选的“时间集成（Temporal Ensembling）”在推理时对多步动作进行指数加权平均，从而平滑并提升鲁棒性。</span>
</span></span><span class=line><span class=cl><span class=c1># 模块结构总览：</span>
</span></span><span class=line><span class=cl><span class=c1># - ACTPolicy：策略封装（选择动作、训练前向、优化参数分组、时间集成/动作队列）</span>
</span></span><span class=line><span class=cl><span class=c1># - ACTTemporalEnsembler：在线指数加权时间集成器</span>
</span></span><span class=line><span class=cl><span class=c1># - ACT：核心模型（视觉骨干网络 + Transformer 编码器/解码器 + 各种投影/位置编码 + 动作回归头）</span>
</span></span><span class=line><span class=cl><span class=c1># - ACTEncoder / ACTDecoder / 对应 Layer：标准 Transformer 层（支持 pre-norm/post-norm）</span>
</span></span><span class=line><span class=cl><span class=c1># - 位置编码（1D/2D 正弦位置编码）</span>
</span></span><span class=line><span class=cl><span class=c1># - 实用函数：get_activation_fn、create_sinusoidal_pos_embedding</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1># 术语与张量形状约定：</span>
</span></span><span class=line><span class=cl><span class=c1># - B: batch size</span>
</span></span><span class=line><span class=cl><span class=c1># - S: 序列长度（这里多指 chunk_size，即要预测的动作步数）</span>
</span></span><span class=line><span class=cl><span class=c1># - D: 隐藏维度（dim_model）</span>
</span></span><span class=line><span class=cl><span class=c1># - L: 潜变量维度（latent_dim）</span>
</span></span><span class=line><span class=cl><span class=c1># - action_dim: 动作维度</span>
</span></span><span class=line><span class=cl><span class=c1># - 图片特征：从视觉骨干网络输出的 feature map (B, C, H, W)，随后会被重排为序列。</span>
</span></span><span class=line><span class=cl><span class=c1># - Transformer 采用 PyTorch 标准接口，序列维度在最前（Seq, Batch, Channel）。</span>
</span></span><span class=line><span class=cl><span class=c1>#</span>
</span></span><span class=line><span class=cl><span class=c1># 限制：仅添加注释，不修改任何原始代码逻辑或接口。</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>collections</span> <span class=kn>import</span> <span class=n>deque</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>collections.abc</span> <span class=kn>import</span> <span class=n>Callable</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>itertools</span> <span class=kn>import</span> <span class=n>chain</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>einops</span>  <span class=c1># 张量重排工具库，便于通道/维度变换</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>  <span class=c1># noqa: N812</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torchvision</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>lerobot.policies.act.configuration_act</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>ACTConfig</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>  <span class=c1># 配置对象，集中管理所有超参数</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>lerobot.policies.pretrained</span> <span class=kn>import</span> <span class=n>PreTrainedPolicy</span>  <span class=c1># 通用策略基类</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>lerobot.utils.constants</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>ACTION</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>OBS_ENV_STATE</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>OBS_IMAGES</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>OBS_STATE</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>  <span class=c1># 约定的数据字典键名</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>Tensor</span><span class=p>,</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchvision.models._utils</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>IntermediateLayerGetter</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>  <span class=c1># 从骨干网络中提取中间层输出</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torchvision.ops.misc</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>FrozenBatchNorm2d</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>  <span class=c1># 冻结的 BN，常用于迁移学习避免数值漂移</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ACTPolicy</span><span class=p>(</span><span class=n>PreTrainedPolicy</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Action Chunking Transformer Policy as per Learning Fine-Grained Bimanual Manipulation with Low-Cost
</span></span></span><span class=line><span class=cl><span class=s2>    Hardware (paper: https://huggingface.co/papers/2304.13705, code: https://github.com/tonyzhaozh/act)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>config_class</span> <span class=o>=</span> <span class=n>ACTConfig</span>
</span></span><span class=line><span class=cl>    <span class=n>name</span> <span class=o>=</span> <span class=s2>&#34;act&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>config</span><span class=p>:</span> <span class=n>ACTConfig</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            config: Policy configuration class instance or None, in which case the default instantiation of
</span></span></span><span class=line><span class=cl><span class=s2>                    the configuration class is used.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>config</span><span class=o>.</span><span class=n>validate_features</span><span class=p>()</span>  <span class=c1># 校验特征配置是否一致/可用（例如是否提供所需的键）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>config</span> <span class=o>=</span> <span class=n>config</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span> <span class=o>=</span> <span class=n>ACT</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>  <span class=c1># 核心模型：视觉 + Transformer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>temporal_ensemble_coeff</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 如果配置了时间集成，就用指数加权的在线方法平滑动作序列</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>temporal_ensembler</span> <span class=o>=</span> <span class=n>ACTTemporalEnsembler</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>config</span><span class=o>.</span><span class=n>temporal_ensemble_coeff</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>chunk_size</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>  <span class=c1># 初始化动作队列或时间集成器状态</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_optim_params</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>dict</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># TODO(aliberts, rcadene): As of now, lr_backbone == lr</span>
</span></span><span class=line><span class=cl>        <span class=c1># Should we remove this and just `return self.parameters()`?</span>
</span></span><span class=line><span class=cl>        <span class=c1># 将参数分组以设置不同学习率（例如对视觉骨干设置较小 LR）</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;params&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>                    <span class=n>p</span>
</span></span><span class=line><span class=cl>                    <span class=k>for</span> <span class=n>n</span><span class=p>,</span> <span class=n>p</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                    <span class=k>if</span> <span class=ow>not</span> <span class=n>n</span><span class=o>.</span><span class=n>startswith</span><span class=p>(</span><span class=s2>&#34;model.backbone&#34;</span><span class=p>)</span> <span class=ow>and</span> <span class=n>p</span><span class=o>.</span><span class=n>requires_grad</span>
</span></span><span class=line><span class=cl>                <span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=p>},</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;params&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>                    <span class=n>p</span>
</span></span><span class=line><span class=cl>                    <span class=k>for</span> <span class=n>n</span><span class=p>,</span> <span class=n>p</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                    <span class=k>if</span> <span class=n>n</span><span class=o>.</span><span class=n>startswith</span><span class=p>(</span><span class=s2>&#34;model.backbone&#34;</span><span class=p>)</span> <span class=ow>and</span> <span class=n>p</span><span class=o>.</span><span class=n>requires_grad</span>
</span></span><span class=line><span class=cl>                <span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;lr&#34;</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>optimizer_lr_backbone</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>reset</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;This should be called whenever the environment is reset.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 环境重置时需清空时间集成器或动作队列，以免使用旧状态</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>temporal_ensemble_coeff</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>temporal_ensembler</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 无时间集成时，使用一个定长队列缓存已预测的动作块，逐步弹出</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_action_queue</span> <span class=o>=</span> <span class=n>deque</span><span class=p>([],</span> <span class=n>maxlen</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>n_action_steps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@torch.no_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>select_action</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Tensor</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Select a single action given environment observations.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        This method wraps `select_actions` in order to return one action at a time for execution in the
</span></span></span><span class=line><span class=cl><span class=s2>        environment. It works by managing the actions in a queue and only calling `select_actions` when the
</span></span></span><span class=line><span class=cl><span class=s2>        queue is empty.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 选择动作时确保 eval 模式（禁用 Dropout/BN 统计更新）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>  <span class=c1># keeping the policy in eval mode as it could be set to train mode while queue is consumed</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>temporal_ensemble_coeff</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 使用时间集成：每次对最新的动作块进行在线融合，并返回当前应执行的单步动作</span>
</span></span><span class=line><span class=cl>            <span class=n>actions</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>predict_action_chunk</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>action</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>temporal_ensembler</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>actions</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>action</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 无时间集成：维护一个动作队列（n_action_steps 个），当队列为空时，再预测新的动作块填充</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>_action_queue</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>actions</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>predict_action_chunk</span><span class=p>(</span><span class=n>batch</span><span class=p>)[:,</span> <span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>n_action_steps</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 模型输出形状为 (B, n_action_steps, action_dim)，而队列按时间步推进，等价 (n_action_steps, B, *)</span>
</span></span><span class=line><span class=cl>            <span class=c1># 因此需要转置再按时间步扩展到队列</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>_action_queue</span><span class=o>.</span><span class=n>extend</span><span class=p>(</span><span class=n>actions</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>_action_queue</span><span class=o>.</span><span class=n>popleft</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@torch.no_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predict_action_chunk</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Tensor</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Predict a chunk of actions given environment observations.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>image_features</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 若配置了图像特征，将用户提供的多路图像拼到统一键 OBS_IMAGES 下（浅拷贝避免改动原 batch）</span>
</span></span><span class=line><span class=cl>            <span class=n>batch</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>batch</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>  <span class=c1># shallow copy so that adding a key doesn&#39;t modify the original</span>
</span></span><span class=line><span class=cl>            <span class=n>batch</span><span class=p>[</span><span class=n>OBS_IMAGES</span><span class=p>]</span> <span class=o>=</span> <span class=p>[</span><span class=n>batch</span><span class=p>[</span><span class=n>key</span><span class=p>]</span> <span class=k>for</span> <span class=n>key</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>image_features</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>actions</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>batch</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>  <span class=c1># 仅取预测的动作（忽略 VAE 参数返回）</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>actions</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Tensor</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=nb>tuple</span><span class=p>[</span><span class=n>Tensor</span><span class=p>,</span> <span class=nb>dict</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Run the batch through the model and compute the loss for training or validation.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 训练/验证前向：输出预测动作与损失</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>image_features</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>batch</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>batch</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>  <span class=c1># shallow copy so that adding a key doesn&#39;t modify the original</span>
</span></span><span class=line><span class=cl>            <span class=n>batch</span><span class=p>[</span><span class=n>OBS_IMAGES</span><span class=p>]</span> <span class=o>=</span> <span class=p>[</span><span class=n>batch</span><span class=p>[</span><span class=n>key</span><span class=p>]</span> <span class=k>for</span> <span class=n>key</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>image_features</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>actions_hat</span><span class=p>,</span> <span class=p>(</span><span class=n>mu_hat</span><span class=p>,</span> <span class=n>log_sigma_x2_hat</span><span class=p>)</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># L1 行为克隆损失（对 padding 掩码为 True 的时间步不计入损失）</span>
</span></span><span class=line><span class=cl>        <span class=n>l1_loss</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>F</span><span class=o>.</span><span class=n>l1_loss</span><span class=p>(</span><span class=n>batch</span><span class=p>[</span><span class=n>ACTION</span><span class=p>],</span> <span class=n>actions_hat</span><span class=p>,</span> <span class=n>reduction</span><span class=o>=</span><span class=s2>&#34;none&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=o>*</span> <span class=o>~</span><span class=n>batch</span><span class=p>[</span><span class=s2>&#34;action_is_pad&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>loss_dict</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;l1_loss&#34;</span><span class=p>:</span> <span class=n>l1_loss</span><span class=o>.</span><span class=n>item</span><span class=p>()}</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>use_vae</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 当使用 VAE 目标时，额外计算 KL 散度（对潜变量逐维求和，再对 batch 求均值）</span>
</span></span><span class=line><span class=cl>            <span class=c1># log_sigma_x2 是 2*log(sigma)，保持与原实现一致</span>
</span></span><span class=line><span class=cl>            <span class=n>mean_kld</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=o>-</span><span class=mf>0.5</span>
</span></span><span class=line><span class=cl>                    <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>log_sigma_x2_hat</span> <span class=o>-</span> <span class=n>mu_hat</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span> <span class=o>-</span> <span class=p>(</span><span class=n>log_sigma_x2_hat</span><span class=p>)</span><span class=o>.</span><span class=n>exp</span><span class=p>())</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>loss_dict</span><span class=p>[</span><span class=s2>&#34;kld_loss&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=n>mean_kld</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>l1_loss</span> <span class=o>+</span> <span class=n>mean_kld</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>kl_weight</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>l1_loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>loss</span><span class=p>,</span> <span class=n>loss_dict</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ACTTemporalEnsembler</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>temporal_ensemble_coeff</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span> <span class=n>chunk_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Temporal ensembling as described in Algorithm 2 of https://huggingface.co/papers/2304.13705.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        The weights are calculated as wᵢ = exp(-temporal_ensemble_coeff * i) where w₀ is the oldest action.
</span></span></span><span class=line><span class=cl><span class=s2>        They are then normalized to sum to 1 by dividing by Σwᵢ. Here&#39;s some intuition around how the
</span></span></span><span class=line><span class=cl><span class=s2>        coefficient works:
</span></span></span><span class=line><span class=cl><span class=s2>            - Setting it to 0 uniformly weighs all actions.
</span></span></span><span class=line><span class=cl><span class=s2>            - Setting it positive gives more weight to older actions.
</span></span></span><span class=line><span class=cl><span class=s2>            - Setting it negative gives more weight to newer actions.
</span></span></span><span class=line><span class=cl><span class=s2>        NOTE: The default value for `temporal_ensemble_coeff` used by the original ACT work is 0.01. This
</span></span></span><span class=line><span class=cl><span class=s2>        results in older actions being weighed more highly than newer actions (the experiments documented in
</span></span></span><span class=line><span class=cl><span class=s2>        https://github.com/huggingface/lerobot/pull/319 hint at why highly weighing new actions might be
</span></span></span><span class=line><span class=cl><span class=s2>        detrimental: doing so aggressively may diminish the benefits of action chunking).
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Here we use an online method for computing the average rather than caching a history of actions in
</span></span></span><span class=line><span class=cl><span class=s2>        order to compute the average offline. For a simple 1D sequence it looks something like:
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        ```
</span></span></span><span class=line><span class=cl><span class=s2>        import torch
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        seq = torch.linspace(8, 8.5, 100)
</span></span></span><span class=line><span class=cl><span class=s2>        print(seq)
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        m = 0.01
</span></span></span><span class=line><span class=cl><span class=s2>        exp_weights = torch.exp(-m * torch.arange(len(seq)))
</span></span></span><span class=line><span class=cl><span class=s2>        print(exp_weights)
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        # Calculate offline
</span></span></span><span class=line><span class=cl><span class=s2>        avg = (exp_weights * seq).sum() / exp_weights.sum()
</span></span></span><span class=line><span class=cl><span class=s2>        print(&#34;offline&#34;, avg)
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        # Calculate online
</span></span></span><span class=line><span class=cl><span class=s2>        for i, item in enumerate(seq):
</span></span></span><span class=line><span class=cl><span class=s2>            if i == 0:
</span></span></span><span class=line><span class=cl><span class=s2>                avg = item
</span></span></span><span class=line><span class=cl><span class=s2>                continue
</span></span></span><span class=line><span class=cl><span class=s2>            avg *= exp_weights[:i].sum()
</span></span></span><span class=line><span class=cl><span class=s2>            avg += item * exp_weights[i]
</span></span></span><span class=line><span class=cl><span class=s2>            avg /= exp_weights[: i + 1].sum()
</span></span></span><span class=line><span class=cl><span class=s2>        print(&#34;online&#34;, avg)
</span></span></span><span class=line><span class=cl><span class=s2>        ```
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 中文补充：时间集成器对一个动作块内的每个时间步位置 i（0 为最旧）分配权重 w_i = exp(-m * i)。</span>
</span></span><span class=line><span class=cl>        <span class=c1># 在线更新避免缓存历史所有动作，大幅节省内存/计算，适合推理时逐步滑动窗口融合。</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>chunk_size</span> <span class=o>=</span> <span class=n>chunk_size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ensemble_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=o>-</span><span class=n>temporal_ensemble_coeff</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>chunk_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 累积和用于在线“归一化”更新</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ensemble_weights_cumsum</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cumsum</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ensemble_weights</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>reset</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Resets the online computation variables.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 清零内部缓存：当前融合的动作序列与对应的计数（每个时间步融合了多少次）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>        <span class=c1># (chunk_size,) count of how many actions are in the ensemble for each time step in the sequence.</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions_count</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>update</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>actions</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Takes a (batch, chunk_size, action_dim) sequence of actions, update the temporal ensemble for all
</span></span></span><span class=line><span class=cl><span class=s2>        time steps, and pop/return the next batch of actions in the sequence.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 将权重张量放到与输入相同的 device 上（CPU/GPU 兼容）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ensemble_weights</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ensemble_weights</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=o>=</span><span class=n>actions</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ensemble_weights_cumsum</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ensemble_weights_cumsum</span><span class=o>.</span><span class=n>to</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>device</span><span class=o>=</span><span class=n>actions</span><span class=o>.</span><span class=n>device</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 第一次调用：直接把预测的动作块克隆为当前融合序列</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions</span> <span class=o>=</span> <span class=n>actions</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=c1># 记录每个时间步目前的“融合次数”=1（形状对齐为 (S,1)，便于广播）</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions_count</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>chunk_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>long</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>device</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions</span><span class=o>.</span><span class=n>device</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 对已有的融合序列（除了最后一个时间步）进行在线更新：</span>
</span></span><span class=line><span class=cl>            <span class=c1># old_avg * sum(w[:i]) + new * w[i] 再除以 sum(w[:i+1])，形如“带权平均”的递推公式</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions</span> <span class=o>*=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ensemble_weights_cumsum</span><span class=p>[</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions_count</span> <span class=o>-</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions</span> <span class=o>+=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>actions</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>ensemble_weights</span><span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions_count</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions</span> <span class=o>/=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ensemble_weights_cumsum</span><span class=p>[</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions_count</span>
</span></span><span class=line><span class=cl>            <span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=c1># 融合计数自增，封顶为 chunk_size</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions_count</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>clamp</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions_count</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=nb>max</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>chunk_size</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># 将“最新一步”的原始动作直接拼到末尾（该位置没有历史平均）</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions</span><span class=p>,</span> <span class=n>actions</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>:]],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># 对应的计数也拼接 1</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions_count</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=p>[</span>
</span></span><span class=line><span class=cl>                    <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions_count</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>torch</span><span class=o>.</span><span class=n>ones_like</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions_count</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>:]),</span>
</span></span><span class=line><span class=cl>                <span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 消费/弹出融合序列的第一个动作（当前要执行的动作），并滑动窗口</span>
</span></span><span class=line><span class=cl>        <span class=n>action</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions_count</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>:],</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>ensembled_actions_count</span><span class=p>[</span><span class=mi>1</span><span class=p>:],</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>action</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ACT</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Action Chunking Transformer: The underlying neural network for ACTPolicy.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Note: In this code we use the terms `vae_encoder`, &#39;encoder&#39;, `decoder`. The meanings are as follows.
</span></span></span><span class=line><span class=cl><span class=s2>        - The `vae_encoder` is, as per the literature around variational auto-encoders (VAE), the part of the
</span></span></span><span class=line><span class=cl><span class=s2>          model that encodes the target data (a sequence of actions), and the condition (the robot
</span></span></span><span class=line><span class=cl><span class=s2>          joint-space).
</span></span></span><span class=line><span class=cl><span class=s2>        - A transformer with an `encoder` (not the VAE encoder) and `decoder` (not the VAE decoder) with
</span></span></span><span class=line><span class=cl><span class=s2>          cross-attention is used as the VAE decoder. For these terms, we drop the `vae_` prefix because we
</span></span></span><span class=line><span class=cl><span class=s2>          have an option to train this model without the variational objective (in which case we drop the
</span></span></span><span class=line><span class=cl><span class=s2>          `vae_encoder` altogether, and nothing about this model has anything to do with a VAE).
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>                                 Transformer
</span></span></span><span class=line><span class=cl><span class=s2>                                 Used alone for inference
</span></span></span><span class=line><span class=cl><span class=s2>                                 (acts as VAE decoder
</span></span></span><span class=line><span class=cl><span class=s2>                                  during training)
</span></span></span><span class=line><span class=cl><span class=s2>                                ┌───────────────────────┐
</span></span></span><span class=line><span class=cl><span class=s2>                                │             Outputs   │
</span></span></span><span class=line><span class=cl><span class=s2>                                │                ▲      │
</span></span></span><span class=line><span class=cl><span class=s2>                                │     ┌─────►┌───────┐  │
</span></span></span><span class=line><span class=cl><span class=s2>                   ┌──────┐     │     │      │Transf.│  │
</span></span></span><span class=line><span class=cl><span class=s2>                   │      │     │     ├─────►│decoder│  │
</span></span></span><span class=line><span class=cl><span class=s2>              ┌────┴────┐ │     │     │      │       │  │
</span></span></span><span class=line><span class=cl><span class=s2>              │         │ │     │ ┌───┴───┬─►│       │  │
</span></span></span><span class=line><span class=cl><span class=s2>              │ VAE     │ │     │ │       │  └───────┘  │
</span></span></span><span class=line><span class=cl><span class=s2>              │ encoder │ │     │ │Transf.│             │
</span></span></span><span class=line><span class=cl><span class=s2>              │         │ │     │ │encoder│             │
</span></span></span><span class=line><span class=cl><span class=s2>              └───▲─────┘ │     │ │       │             │
</span></span></span><span class=line><span class=cl><span class=s2>                  │       │     │ └▲──▲─▲─┘             │
</span></span></span><span class=line><span class=cl><span class=s2>                  │       │     │  │  │ │               │
</span></span></span><span class=line><span class=cl><span class=s2>                inputs    └─────┼──┘  │ image emb.      │
</span></span></span><span class=line><span class=cl><span class=s2>                                │    state emb.         │
</span></span></span><span class=line><span class=cl><span class=s2>                                └───────────────────────┘
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>ACTConfig</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># BERT 风格的 VAE 编码器输入： [CLS, 机器人当前关节状态（可选）, 动作序列]。</span>
</span></span><span class=line><span class=cl>        <span class=c1># CLS token 经过投影后输出潜变量分布参数（mean 与 log_sigma_x2）。</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>config</span> <span class=o>=</span> <span class=n>config</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>use_vae</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># VAE 编码器（仅在使用 VAE 目标且训练阶段时启用）</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>vae_encoder</span> <span class=o>=</span> <span class=n>ACTEncoder</span><span class=p>(</span><span class=n>config</span><span class=p>,</span> <span class=n>is_vae_encoder</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>vae_encoder_cls_embed</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># 机器人关节状态投影到 Transformer 隐藏维度</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>robot_state_feature</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>vae_encoder_robot_state_input_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>robot_state_feature</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># 动作（目标关节位姿/速度等）投影到隐藏维度</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>vae_encoder_action_input_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>action_feature</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># 将 VAE 编码器的 CLS 输出映射为潜变量分布参数（均值和对数方差 * 2）</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>vae_encoder_latent_output_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>latent_dim</span> <span class=o>*</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># 固定正弦位置编码（1D），长度 = 1(CLS) + S(动作步) + [1(关节状态，可选)]</span>
</span></span><span class=line><span class=cl>            <span class=n>num_input_token_encoder</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>+</span> <span class=n>config</span><span class=o>.</span><span class=n>chunk_size</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>robot_state_feature</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>num_input_token_encoder</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;vae_encoder_pos_enc&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>create_sinusoidal_pos_embedding</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>num_input_token_encoder</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 视觉骨干网络（例如 ResNet），用于提取图像特征</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>image_features</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>backbone_model</span> <span class=o>=</span> <span class=nb>getattr</span><span class=p>(</span><span class=n>torchvision</span><span class=o>.</span><span class=n>models</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>vision_backbone</span><span class=p>)(</span>
</span></span><span class=line><span class=cl>                <span class=n>replace_stride_with_dilation</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>                    <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>config</span><span class=o>.</span><span class=n>replace_final_stride_with_dilation</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=n>weights</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>pretrained_backbone_weights</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>norm_layer</span><span class=o>=</span><span class=n>FrozenBatchNorm2d</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># 使用 IntermediateLayerGetter 从指定层（这里为 layer4）获取特征图</span>
</span></span><span class=line><span class=cl>            <span class=c1># 输出为字典 {&#34;feature_map&#34;: output}</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>backbone</span> <span class=o>=</span> <span class=n>IntermediateLayerGetter</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>backbone_model</span><span class=p>,</span> <span class=n>return_layers</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;layer4&#34;</span><span class=p>:</span> <span class=s2>&#34;feature_map&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Transformer：在使用 VAE 时相当于解码器（cross-attend 到条件输入）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>ACTEncoder</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>ACTDecoder</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Transformer 编码器的输入投影与位置编码：</span>
</span></span><span class=line><span class=cl>        <span class=c1># token 顺序：[latent, (robot_state), (env_state), (image_feature_map_pixels...)]</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>robot_state_feature</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>encoder_robot_state_input_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>robot_state_feature</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>env_state_feature</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>encoder_env_state_input_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>env_state_feature</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder_latent_input_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>latent_dim</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>image_features</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 将骨干网络的通道维投影到 dim_model（1x1 卷积作为线性投影）</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>encoder_img_feat_input_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>backbone_model</span><span class=o>.</span><span class=n>fc</span><span class=o>.</span><span class=n>in_features</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 1D token（latent/robot_state/env_state）的可学习位置嵌入</span>
</span></span><span class=line><span class=cl>        <span class=n>n_1d_tokens</span> <span class=o>=</span> <span class=mi>1</span>  <span class=c1># latent</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>robot_state_feature</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>n_1d_tokens</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>env_state_feature</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>n_1d_tokens</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder_1d_feature_pos_embed</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>n_1d_tokens</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>image_features</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 2D 正弦位置编码（对 feature map 的每个像素提供位置信息）</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>encoder_cam_feat_pos_embed</span> <span class=o>=</span> <span class=n>ACTSinusoidalPositionEmbedding2d</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span> <span class=o>//</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Transformer 解码器：为 S 个要预测的时间步提供可学习查询（DETR 风格）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder_pos_embed</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>chunk_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 最终线性头：将 decoder 输出映射为动作维度</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>action_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>action_feature</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_reset_parameters</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_reset_parameters</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Xavier-uniform initialization of the transformer parameters as in the original code.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 对 Transformer 层进行 Xavier 均匀初始化，提升训练稳定性</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>chain</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=o>.</span><span class=n>parameters</span><span class=p>()):</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>p</span><span class=o>.</span><span class=n>dim</span><span class=p>()</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>xavier_uniform_</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span> <span class=n>batch</span><span class=p>:</span> <span class=nb>dict</span><span class=p>[</span><span class=nb>str</span><span class=p>,</span> <span class=n>Tensor</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>tuple</span><span class=p>[</span><span class=n>Tensor</span><span class=p>,</span> <span class=nb>tuple</span><span class=p>[</span><span class=n>Tensor</span><span class=p>,</span> <span class=n>Tensor</span><span class=p>]</span> <span class=o>|</span> <span class=nb>tuple</span><span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=kc>None</span><span class=p>]]:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;A forward pass through the Action Chunking Transformer (with optional VAE encoder).
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        `batch` should have the following structure:
</span></span></span><span class=line><span class=cl><span class=s2>        {
</span></span></span><span class=line><span class=cl><span class=s2>            [robot_state_feature] (optional): (B, state_dim) batch of robot states.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>            [image_features]: (B, n_cameras, C, H, W) batch of images.
</span></span></span><span class=line><span class=cl><span class=s2>                AND/OR
</span></span></span><span class=line><span class=cl><span class=s2>            [env_state_feature]: (B, env_dim) batch of environment states.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>            [action_feature] (optional, only if training with VAE): (B, chunk_size, action dim) batch of actions.
</span></span></span><span class=line><span class=cl><span class=s2>        }
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            (B, chunk_size, action_dim) batch of action sequences
</span></span></span><span class=line><span class=cl><span class=s2>            Tuple containing the latent PDF&#39;s parameters (mean, log(σ²)) both as (B, L) tensors where L is the
</span></span></span><span class=line><span class=cl><span class=s2>            latent dimension.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 当使用 VAE + 训练模式时，要求 batch 中必须包含监督的动作序列 ACTION</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>use_vae</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>assert</span> <span class=n>ACTION</span> <span class=ow>in</span> <span class=n>batch</span><span class=p>,</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;actions must be provided when using the variational objective in training mode.&#34;</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 估计 batch 大小：优先从图像，若无图像则从环境状态推断</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>batch</span><span class=p>[</span><span class=n>OBS_IMAGES</span><span class=p>][</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>OBS_IMAGES</span> <span class=ow>in</span> <span class=n>batch</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span> <span class=n>batch</span><span class=p>[</span><span class=n>OBS_ENV_STATE</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 1) 准备潜变量（latent）</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>use_vae</span> <span class=ow>and</span> <span class=n>ACTION</span> <span class=ow>in</span> <span class=n>batch</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>training</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 训练 + VAE：通过 VAE encoder 从 [CLS, 关节状态(可选), 动作序列] 推断潜变量分布参数</span>
</span></span><span class=line><span class=cl>            <span class=n>cls_embed</span> <span class=o>=</span> <span class=n>einops</span><span class=o>.</span><span class=n>repeat</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>vae_encoder_cls_embed</span><span class=o>.</span><span class=n>weight</span><span class=p>,</span> <span class=s2>&#34;1 d -&gt; b 1 d&#34;</span><span class=p>,</span> <span class=n>b</span><span class=o>=</span><span class=n>batch_size</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>  <span class=c1># (B, 1, D)</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>robot_state_feature</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>robot_state_embed</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>vae_encoder_robot_state_input_proj</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>batch</span><span class=p>[</span><span class=n>OBS_STATE</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>robot_state_embed</span> <span class=o>=</span> <span class=n>robot_state_embed</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># (B, 1, D)</span>
</span></span><span class=line><span class=cl>            <span class=n>action_embed</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>vae_encoder_action_input_proj</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>batch</span><span class=p>[</span><span class=n>ACTION</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>  <span class=c1># (B, S, D)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>robot_state_feature</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>vae_encoder_input</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>                    <span class=n>cls_embed</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>robot_state_embed</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>action_embed</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=p>]</span>  <span class=c1># (B, S+2, D)</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>vae_encoder_input</span> <span class=o>=</span> <span class=p>[</span><span class=n>cls_embed</span><span class=p>,</span> <span class=n>action_embed</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>vae_encoder_input</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span><span class=n>vae_encoder_input</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 固定位置编码（与原实现保持一致，使用 clone().detach()）</span>
</span></span><span class=line><span class=cl>            <span class=n>pos_embed</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>vae_encoder_pos_enc</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span>  <span class=c1># (1, S+2, D)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># key_padding_mask：前面 CLS 和关节状态不是 padding，后面根据 action_is_pad 指示</span>
</span></span><span class=line><span class=cl>            <span class=c1># False 表示不是 pad；形状 (B, S+1 或 S+2)</span>
</span></span><span class=line><span class=cl>            <span class=n>cls_joint_is_pad</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>full</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>2</span> <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>robot_state_feature</span> <span class=k>else</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>device</span><span class=o>=</span><span class=n>batch</span><span class=p>[</span><span class=n>OBS_STATE</span><span class=p>]</span><span class=o>.</span><span class=n>device</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>key_padding_mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=p>[</span><span class=n>cls_joint_is_pad</span><span class=p>,</span> <span class=n>batch</span><span class=p>[</span><span class=s2>&#34;action_is_pad&#34;</span><span class=p>]],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>  <span class=c1># (bs, seq+1 or 2)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 送入 VAE 编码器，取 CLS 位置的输出（包含全序列信息），再映射得到 (mu, log_sigma_x2)</span>
</span></span><span class=line><span class=cl>            <span class=n>cls_token_out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>vae_encoder</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>vae_encoder_input</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                <span class=n>pos_embed</span><span class=o>=</span><span class=n>pos_embed</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                <span class=n>key_padding_mask</span><span class=o>=</span><span class=n>key_padding_mask</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)[</span><span class=mi>0</span><span class=p>]</span>  <span class=c1># select the class token, with shape (B, D)</span>
</span></span><span class=line><span class=cl>            <span class=n>latent_pdf_params</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>vae_encoder_latent_output_proj</span><span class=p>(</span><span class=n>cls_token_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>mu</span> <span class=o>=</span> <span class=n>latent_pdf_params</span><span class=p>[:,</span> <span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>latent_dim</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=c1># 注意：这里返回的是 2*log(sigma)，与原实现一致</span>
</span></span><span class=line><span class=cl>            <span class=n>log_sigma_x2</span> <span class=o>=</span> <span class=n>latent_pdf_params</span><span class=p>[:,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>latent_dim</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 重参数化采样 latent： z = mu + sigma * eps</span>
</span></span><span class=line><span class=cl>            <span class=n>latent_sample</span> <span class=o>=</span> <span class=n>mu</span> <span class=o>+</span> <span class=n>log_sigma_x2</span><span class=o>.</span><span class=n>div</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>exp</span><span class=p>()</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn_like</span><span class=p>(</span><span class=n>mu</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 推理或未使用 VAE：latent 设为全零（代表“无信息”的先验）</span>
</span></span><span class=line><span class=cl>            <span class=n>mu</span> <span class=o>=</span> <span class=n>log_sigma_x2</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>            <span class=c1># TODO(rcadene, alexander-soare): remove call to `.to` to speedup forward ; precompute and use buffer</span>
</span></span><span class=line><span class=cl>            <span class=n>latent_sample</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=p>[</span><span class=n>batch_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>latent_dim</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>batch</span><span class=p>[</span><span class=n>OBS_STATE</span><span class=p>]</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 2) 准备 Transformer 编码器输入 token 与位置编码</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder_in_tokens</span> <span class=o>=</span> <span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>encoder_latent_input_proj</span><span class=p>(</span><span class=n>latent_sample</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=c1># 1D token 的可学习位置嵌入（先堆为 list，后面与图像 token 一起 stack）</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder_in_pos_embed</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>encoder_1d_feature_pos_embed</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 机器人关节状态 token</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>robot_state_feature</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>encoder_in_tokens</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>encoder_robot_state_input_proj</span><span class=p>(</span><span class=n>batch</span><span class=p>[</span><span class=n>OBS_STATE</span><span class=p>])</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 环境状态 token</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>env_state_feature</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>encoder_in_tokens</span><span class=o>.</span><span class=n>append</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=o>.</span><span class=n>encoder_env_state_input_proj</span><span class=p>(</span><span class=n>batch</span><span class=p>[</span><span class=n>OBS_ENV_STATE</span><span class=p>])</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>image_features</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># 多相机图像：各自经过骨干提特征，再通过 1x1 conv 投影至 dim_model</span>
</span></span><span class=line><span class=cl>            <span class=c1># 注意：对 MPS 设备做过数值稳定性注意（保持与原实现的注释一致）</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>img</span> <span class=ow>in</span> <span class=n>batch</span><span class=p>[</span><span class=n>OBS_IMAGES</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>                <span class=n>cam_features</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>backbone</span><span class=p>(</span><span class=n>img</span><span class=p>)[</span>
</span></span><span class=line><span class=cl>                    <span class=s2>&#34;feature_map&#34;</span>
</span></span><span class=line><span class=cl>                <span class=p>]</span>  <span class=c1># (B, C_backbone, H, W)</span>
</span></span><span class=line><span class=cl>                <span class=n>cam_pos_embed</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder_cam_feat_pos_embed</span><span class=p>(</span><span class=n>cam_features</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>dtype</span><span class=o>=</span><span class=n>cam_features</span><span class=o>.</span><span class=n>dtype</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>cam_features</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder_img_feat_input_proj</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>cam_features</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>  <span class=c1># -&gt; (B, D, H, W)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># 重排为 (Seq, B, D)，其中 Seq = H*W</span>
</span></span><span class=line><span class=cl>                <span class=n>cam_features</span> <span class=o>=</span> <span class=n>einops</span><span class=o>.</span><span class=n>rearrange</span><span class=p>(</span><span class=n>cam_features</span><span class=p>,</span> <span class=s2>&#34;b c h w -&gt; (h w) b c&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>cam_pos_embed</span> <span class=o>=</span> <span class=n>einops</span><span class=o>.</span><span class=n>rearrange</span><span class=p>(</span><span class=n>cam_pos_embed</span><span class=p>,</span> <span class=s2>&#34;b c h w -&gt; (h w) b c&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=c1># 直接 extend（列表形式）以避免先积累后再 concat 的额外开销</span>
</span></span><span class=line><span class=cl>                <span class=n>encoder_in_tokens</span><span class=o>.</span><span class=n>extend</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=n>cam_features</span><span class=p>))</span>
</span></span><span class=line><span class=cl>                <span class=n>encoder_in_pos_embed</span><span class=o>.</span><span class=n>extend</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=n>cam_pos_embed</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 将所有 token 按序列维 stack 成张量：(ES, B, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder_in_tokens</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>encoder_in_tokens</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder_in_pos_embed</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>encoder_in_pos_embed</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 3) 经过 Transformer 编码器/解码器</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder_out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=n>encoder_in_tokens</span><span class=p>,</span> <span class=n>pos_embed</span><span class=o>=</span><span class=n>encoder_in_pos_embed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 解码器输入初始化为全零（S, B, D），再加上可学习的 decoder_pos_embed 作为查询</span>
</span></span><span class=line><span class=cl>        <span class=c1># TODO(rcadene, alexander-soare): remove call to `device` ; precompute and use buffer</span>
</span></span><span class=line><span class=cl>        <span class=n>decoder_in</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>chunk_size</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>dtype</span><span class=o>=</span><span class=n>encoder_in_pos_embed</span><span class=o>.</span><span class=n>dtype</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>device</span><span class=o>=</span><span class=n>encoder_in_pos_embed</span><span class=o>.</span><span class=n>device</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>decoder_out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>decoder_in</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>encoder_out</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>encoder_pos_embed</span><span class=o>=</span><span class=n>encoder_in_pos_embed</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>decoder_pos_embed</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>decoder_pos_embed</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># (S, B, D) -&gt; (B, S, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>decoder_out</span> <span class=o>=</span> <span class=n>decoder_out</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 动作回归头：(B, S, D) -&gt; (B, S, action_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>actions</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>action_head</span><span class=p>(</span><span class=n>decoder_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 返回动作与（可选）VAE 参数（训练使用）</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>actions</span><span class=p>,</span> <span class=p>(</span><span class=n>mu</span><span class=p>,</span> <span class=n>log_sigma_x2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ACTEncoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Convenience module for running multiple encoder layers, maybe followed by normalization.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 一个封装的 Transformer Encoder 堆叠模块，支持 pre-norm 配置，便于复用（包括作为 VAE encoder）</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>ACTConfig</span><span class=p>,</span> <span class=n>is_vae_encoder</span><span class=p>:</span> <span class=nb>bool</span> <span class=o>=</span> <span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>is_vae_encoder</span> <span class=o>=</span> <span class=n>is_vae_encoder</span>
</span></span><span class=line><span class=cl>        <span class=n>num_layers</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>config</span><span class=o>.</span><span class=n>n_vae_encoder_layers</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_vae_encoder</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span> <span class=n>config</span><span class=o>.</span><span class=n>n_encoder_layers</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=n>ACTEncoderLayer</span><span class=p>(</span><span class=n>config</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>)</span> <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>pre_norm</span> <span class=k>else</span> <span class=n>nn</span><span class=o>.</span><span class=n>Identity</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>pos_embed</span><span class=p>:</span> <span class=n>Tensor</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>key_padding_mask</span><span class=p>:</span> <span class=n>Tensor</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 逐层前向；支持外部传入位置编码与 padding 掩码</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>pos_embed</span><span class=o>=</span><span class=n>pos_embed</span><span class=p>,</span> <span class=n>key_padding_mask</span><span class=o>=</span><span class=n>key_padding_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ACTEncoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>ACTConfig</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MultiheadAttention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>n_heads</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>dropout</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 前馈网络 FFN：Linear -&gt; 激活 -&gt; Dropout -&gt; Linear</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>dim_feedforward</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dim_feedforward</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 残差层归一化（支持 pre-norm 或 post-norm）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>activation</span> <span class=o>=</span> <span class=n>get_activation_fn</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>feedforward_activation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pre_norm</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>pre_norm</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>pos_embed</span><span class=p>:</span> <span class=n>Tensor</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span> <span class=n>key_padding_mask</span><span class=p>:</span> <span class=n>Tensor</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 自注意力子层</span>
</span></span><span class=line><span class=cl>        <span class=n>skip</span> <span class=o>=</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>pre_norm</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=n>k</span> <span class=o>=</span> <span class=n>x</span> <span class=k>if</span> <span class=n>pos_embed</span> <span class=ow>is</span> <span class=kc>None</span> <span class=k>else</span> <span class=n>x</span> <span class=o>+</span> <span class=n>pos_embed</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=p>,</span> <span class=n>value</span><span class=o>=</span><span class=n>x</span><span class=p>,</span> <span class=n>key_padding_mask</span><span class=o>=</span><span class=n>key_padding_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>  <span class=c1># note: [0] to select just the output, not the attention weights</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>skip</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># 残差</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 前馈子层</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>pre_norm</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>skip</span> <span class=o>=</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>skip</span> <span class=o>=</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear2</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>activation</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>linear1</span><span class=p>(</span><span class=n>x</span><span class=p>))))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>skip</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>pre_norm</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ACTDecoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>ACTConfig</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Convenience module for running multiple decoder layers followed by normalization.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=n>ACTDecoderLayer</span><span class=p>(</span><span class=n>config</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>n_decoder_layers</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder_out</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>decoder_pos_embed</span><span class=p>:</span> <span class=n>Tensor</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder_pos_embed</span><span class=p>:</span> <span class=n>Tensor</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 逐层 Decoder，包含自注意力与跨注意力（对 encoder_out 进行 cross-attention）</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>x</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>encoder_out</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>decoder_pos_embed</span><span class=o>=</span><span class=n>decoder_pos_embed</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>encoder_pos_embed</span><span class=o>=</span><span class=n>encoder_pos_embed</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ACTDecoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>:</span> <span class=n>ACTConfig</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MultiheadAttention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>n_heads</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>dropout</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>multihead_attn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MultiheadAttention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>n_heads</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>dropout</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># FFN</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>dim_feedforward</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dim_feedforward</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 三个归一化/Dropout 对应三处残差连接</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dim_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>activation</span> <span class=o>=</span> <span class=n>get_activation_fn</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>feedforward_activation</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pre_norm</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>pre_norm</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>maybe_add_pos_embed</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tensor</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span> <span class=n>pos_embed</span><span class=p>:</span> <span class=n>Tensor</span> <span class=o>|</span> <span class=kc>None</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># 若提供了位置编码，则与输入相加（Transformer 常见用法）</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>tensor</span> <span class=k>if</span> <span class=n>pos_embed</span> <span class=ow>is</span> <span class=kc>None</span> <span class=k>else</span> <span class=n>tensor</span> <span class=o>+</span> <span class=n>pos_embed</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder_out</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>decoder_pos_embed</span><span class=p>:</span> <span class=n>Tensor</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder_pos_embed</span><span class=p>:</span> <span class=n>Tensor</span> <span class=o>|</span> <span class=kc>None</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            x: (Decoder Sequence, Batch, Channel) tensor of input tokens.
</span></span></span><span class=line><span class=cl><span class=s2>            encoder_out: (Encoder Sequence, B, C) output features from the last layer of the encoder we are
</span></span></span><span class=line><span class=cl><span class=s2>                cross-attending with.
</span></span></span><span class=line><span class=cl><span class=s2>            encoder_pos_embed: (ES, 1, C) positional embedding for keys (from the encoder).
</span></span></span><span class=line><span class=cl><span class=s2>            decoder_pos_embed: (DS, 1, C) positional embedding for the queries (from the decoder).
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            (DS, B, C) tensor of decoder output features.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 1) 自注意力（Decoder 内部 token 之间交互）</span>
</span></span><span class=line><span class=cl>        <span class=n>skip</span> <span class=o>=</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>pre_norm</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=n>k</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>maybe_add_pos_embed</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>decoder_pos_embed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=p>,</span> <span class=n>value</span><span class=o>=</span><span class=n>x</span><span class=p>)[</span>
</span></span><span class=line><span class=cl>            <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>  <span class=c1># select just the output, not the attention weights</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>skip</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 2) 跨注意力（对 Encoder 输出进行查询）</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>pre_norm</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>skip</span> <span class=o>=</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>skip</span> <span class=o>=</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>multihead_attn</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>query</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>maybe_add_pos_embed</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>decoder_pos_embed</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>key</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>maybe_add_pos_embed</span><span class=p>(</span><span class=n>encoder_out</span><span class=p>,</span> <span class=n>encoder_pos_embed</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>value</span><span class=o>=</span><span class=n>encoder_out</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)[</span><span class=mi>0</span><span class=p>]</span>  <span class=c1># select just the output, not the attention weights</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>skip</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 3) FFN</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>pre_norm</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>skip</span> <span class=o>=</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm3</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>skip</span> <span class=o>=</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear2</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>activation</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>linear1</span><span class=p>(</span><span class=n>x</span><span class=p>))))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>skip</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout3</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>pre_norm</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm3</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_sinusoidal_pos_embedding</span><span class=p>(</span><span class=n>num_positions</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>dimension</span><span class=p>:</span> <span class=nb>int</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;1D sinusoidal positional embeddings as in Attention is All You Need.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        num_positions: Number of token positions required.
</span></span></span><span class=line><span class=cl><span class=s2>    Returns: (num_positions, dimension) position embeddings (the first dimension is the batch dimension).
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 标准的 1D 正弦/余弦位置编码实现，频率按几何级数变化（温度=10000），偶数维使用正弦，奇数维使用余弦。</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_position_angle_vec</span><span class=p>(</span><span class=n>position</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=n>position</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>power</span><span class=p>(</span><span class=mi>10000</span><span class=p>,</span> <span class=mi>2</span> <span class=o>*</span> <span class=p>(</span><span class=n>hid_j</span> <span class=o>//</span> <span class=mi>2</span><span class=p>)</span> <span class=o>/</span> <span class=n>dimension</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>hid_j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>dimension</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>sinusoid_table</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=n>get_position_angle_vec</span><span class=p>(</span><span class=n>pos_i</span><span class=p>)</span> <span class=k>for</span> <span class=n>pos_i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_positions</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>sinusoid_table</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>sinusoid_table</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>::</span><span class=mi>2</span><span class=p>])</span>  <span class=c1># dim 2i</span>
</span></span><span class=line><span class=cl>    <span class=n>sinusoid_table</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>sinusoid_table</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>])</span>  <span class=c1># dim 2i+1</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>sinusoid_table</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ACTSinusoidalPositionEmbedding2d</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;2D sinusoidal positional embeddings similar to what&#39;s presented in Attention Is All You Need.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    The variation is that the position indices are normalized in [0, 2π] (not quite: the lower bound is 1/H
</span></span></span><span class=line><span class=cl><span class=s2>    for the vertical direction, and 1/W for the horizontal direction.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 为 2D 特征图（H,W）生成二维正弦位置编码。与常见实现不同，位置索引被缩放到 [0, 2π] 区间（近似），</span>
</span></span><span class=line><span class=cl>    <span class=c1># 然后同样以几何级数频率生成正/余弦分量，最后在通道维上拼接（y 分量在前，x 分量在后）。</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>dimension</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            dimension: The desired dimension of the embeddings.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dimension</span> <span class=o>=</span> <span class=n>dimension</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_two_pi</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>math</span><span class=o>.</span><span class=n>pi</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_eps</span> <span class=o>=</span> <span class=mf>1e-6</span>
</span></span><span class=line><span class=cl>        <span class=c1># 频率几何级数的“温度”（与 1D 的 10000 一致）</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>_temperature</span> <span class=o>=</span> <span class=mi>10000</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Args:
</span></span></span><span class=line><span class=cl><span class=s2>            x: A (B, C, H, W) batch of 2D feature map to generate the embeddings for.
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            A (1, C, H, W) batch of corresponding sinusoidal positional embeddings.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=c1># 仅需 H、W 形状，因此构造一个形状 (1, H, W) 的“非 mask”</span>
</span></span><span class=line><span class=cl>        <span class=n>not_mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones_like</span><span class=p>(</span><span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=p>:</span><span class=mi>1</span><span class=p>])</span>  <span class=c1># (1, H, W)</span>
</span></span><span class=line><span class=cl>        <span class=c1># y/x 方向的累计和相当于 1..H 与 1..W（原实现从 1 开始，而不是 0）</span>
</span></span><span class=line><span class=cl>        <span class=n>y_range</span> <span class=o>=</span> <span class=n>not_mask</span><span class=o>.</span><span class=n>cumsum</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x_range</span> <span class=o>=</span> <span class=n>not_mask</span><span class=o>.</span><span class=n>cumsum</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 归一化到 [0, 2π]（加入 eps 避免分母为 0）</span>
</span></span><span class=line><span class=cl>        <span class=n>y_range</span> <span class=o>=</span> <span class=n>y_range</span> <span class=o>/</span> <span class=p>(</span><span class=n>y_range</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>:,</span> <span class=p>:]</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>_eps</span><span class=p>)</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>_two_pi</span>
</span></span><span class=line><span class=cl>        <span class=n>x_range</span> <span class=o>=</span> <span class=n>x_range</span> <span class=o>/</span> <span class=p>(</span><span class=n>x_range</span><span class=p>[:,</span> <span class=p>:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>:]</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>_eps</span><span class=p>)</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>_two_pi</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 频率几何序列（偶数/奇数通道分别对应 sin/cos）</span>
</span></span><span class=line><span class=cl>        <span class=n>inverse_frequency</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_temperature</span> <span class=o>**</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=mi>2</span>
</span></span><span class=line><span class=cl>            <span class=o>*</span> <span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>dimension</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>x</span><span class=o>.</span><span class=n>device</span><span class=p>)</span> <span class=o>//</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>dimension</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 扩展最后一维以按通道除以频率：(1, H, W, 1)</span>
</span></span><span class=line><span class=cl>        <span class=n>x_range</span> <span class=o>=</span> <span class=n>x_range</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span> <span class=o>/</span> <span class=n>inverse_frequency</span>  <span class=c1># (1, H, W, 1)</span>
</span></span><span class=line><span class=cl>        <span class=n>y_range</span> <span class=o>=</span> <span class=n>y_range</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span> <span class=o>/</span> <span class=n>inverse_frequency</span>  <span class=c1># (1, H, W, 1)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 交错堆叠 sin/cos，并在通道维上展平：(1, H, W, C//2)</span>
</span></span><span class=line><span class=cl>        <span class=n>pos_embed_x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=p>(</span><span class=n>x_range</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=mi>0</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span><span class=o>.</span><span class=n>sin</span><span class=p>(),</span> <span class=n>x_range</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span><span class=o>.</span><span class=n>cos</span><span class=p>()),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pos_embed_y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=p>(</span><span class=n>y_range</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=mi>0</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span><span class=o>.</span><span class=n>sin</span><span class=p>(),</span> <span class=n>y_range</span><span class=p>[</span><span class=o>...</span><span class=p>,</span> <span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span><span class=o>.</span><span class=n>cos</span><span class=p>()),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pos_embed</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>pos_embed_y</span><span class=p>,</span> <span class=n>pos_embed_x</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=mi>0</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>  <span class=c1># (1, C, H, W)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>pos_embed</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_activation_fn</span><span class=p>(</span><span class=n>activation</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Callable</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Return an activation function given a string.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># 将字符串名称映射到对应的激活函数实现</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>activation</span> <span class=o>==</span> <span class=s2>&#34;relu&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>activation</span> <span class=o>==</span> <span class=s2>&#34;gelu&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>F</span><span class=o>.</span><span class=n>gelu</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>activation</span> <span class=o>==</span> <span class=s2>&#34;glu&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>F</span><span class=o>.</span><span class=n>glu</span>
</span></span><span class=line><span class=cl>    <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;activation should be relu/gelu/glu, not </span><span class=si>{</span><span class=n>activation</span><span class=si>}</span><span class=s2>.&#34;</span><span class=p>)</span></span></span></code></pre></div></div><h3 class="relative group">ACT 模型微调<div id=act-模型微调 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#act-%e6%a8%a1%e5%9e%8b%e5%be%ae%e8%b0%83 aria-label=锚点>#</a></span></h3><h4 class="relative group">关键参数<div id=关键参数 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e5%85%b3%e9%94%ae%e5%8f%82%e6%95%b0 aria-label=锚点>#</a></span></h4><p>详见 <code>configuration_act.py</code>。</p><p>常用项如下（括号内为配置项名称）：</p><ul><li>每次预测的动作数量（<code>chunk_size</code>）：通常 50–100；<ul><li>快速任务：10–30</li><li>中等任务：50–100</li><li>慢速任务：100–200</li><li>一般建议从 50 起步</li></ul></li><li>实际执行的动作步数（<code>n_action_steps</code>）：必须满足 <code>n_action_steps ≤ chunk_size</code>，推荐二者相同（如均为 100）。</li><li>历史观测步数/上下文长度（<code>n_obs_steps</code>）。</li><li>Transformer 维度（<code>dim_model</code>、<code>dim_feedforward</code>）、层数（<code>n_encoder_layers</code>、<code>n_decoder_layers</code>）、注意力头数（<code>n_heads</code>）。</li><li>视觉主干网络（<code>vision_backbone</code>，如 <code>resnet18</code>）。</li></ul><h4 class="relative group">参考命令<div id=参考命令 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e5%8f%82%e8%80%83%e5%91%bd%e4%bb%a4 aria-label=锚点>#</a></span></h4><ol><li>多摄像头配置<div class=highlight-wrapper data-code-lang=shell><div class=codeblock-title><span class=codeblock-lang>SHELL</span><button type=button class=codeblock-wrap-toggle data-code-wrap-toggle aria-label=切换代码块换行模式 aria-pressed=true>不换行</button></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># 针对多摄像头设置的 ACT 训练</span>
</span></span><span class=line><span class=cl>lerobot-train <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.type act <span class=se>\
</span></span></span><span class=line><span class=cl>  --dataset.repo_id <span class=si>${</span><span class=nv>HF_USER</span><span class=si>}</span>/your_dataset <span class=se>\
</span></span></span><span class=line><span class=cl>  --batch_size <span class=m>4</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --steps <span class=m>100000</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --output_dir outputs/train/act_multicam <span class=se>\
</span></span></span><span class=line><span class=cl>  --job_name act_multicam_training <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.device cuda <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.chunk_size <span class=m>100</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.n_action_steps <span class=m>100</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.n_obs_steps <span class=m>2</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.vision_backbone resnet18 <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.dim_model <span class=m>512</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.dim_feedforward <span class=m>3200</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.n_encoder_layers <span class=m>4</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.n_decoder_layers <span class=m>1</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.n_heads <span class=m>8</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.optimizer_lr 1e-5 <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.optimizer_weight_decay 1e-4 <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.push_to_hub <span class=nb>false</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --save_checkpoint <span class=nb>true</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --wandb.enable true</span></span></code></pre></div></div></li><li>内存优化配置<div class=highlight-wrapper data-code-lang=shell><div class=codeblock-title><span class=codeblock-lang>SHELL</span><button type=button class=codeblock-wrap-toggle data-code-wrap-toggle aria-label=切换代码块换行模式 aria-pressed=true>不换行</button></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># 针对显存较小的 GPU</span>
</span></span><span class=line><span class=cl>lerobot-train <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.type act <span class=se>\
</span></span></span><span class=line><span class=cl>  --dataset.repo_id io-ai-data/lerobot_data <span class=se>\
</span></span></span><span class=line><span class=cl>  --batch_size <span class=m>2</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --steps <span class=m>75000</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --output_dir outputs/train/act_memory_opt <span class=se>\
</span></span></span><span class=line><span class=cl>  --job_name act_memory_optimized <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.device cuda <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.chunk_size <span class=m>100</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.n_action_steps <span class=m>100</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.n_obs_steps <span class=m>1</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.vision_backbone resnet18 <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.dim_model <span class=m>256</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.optimizer_lr 1e-5 <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.use_amp <span class=nb>true</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --num_workers <span class=m>2</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --policy.push_to_hub <span class=nb>false</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --save_checkpoint <span class=nb>true</span> <span class=se>\
</span></span></span><span class=line><span class=cl>  --wandb.enable true</span></span></code></pre></div></div></li></ol><h2 class="relative group">结语<div id=结语 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e7%bb%93%e8%af%ad aria-label=锚点>#</a></span></h2><p>本文对 ACT 模型的原理、架构、训练与推理流程、消融实验、代码实现与调参与实践建议进行了梳理，期望对你的应用落地有所帮助。</p><h2 class="relative group"><em>参考资料</em><div id=参考资料 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100 select-none"><a class="text-primary-300 dark:text-neutral-700 !no-underline" href=#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99 aria-label=锚点>#</a></span></h2><div class="expand-wrapper prose dark:prose-invert max-w-prose zen-mode-content"><input id=expand-10 class=expand-toggle type=checkbox>
<label for=expand-10 class=expand-title><span class=expand-icon><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-down"><polyline points="6 9 12 15 18 9"/></svg>
</span>点击展开查看参考资料</label><div class=expand-content><div class=expand-inner><ul><li><a href=https://arxiv.org/abs/2304.13705 target=_blank>ACT 论文地址</a></li><li><a href=https://ar5iv.labs.arxiv.org/html/2304.13705 target=_blank>ACT 论文在线阅读</a></li><li><a href=https://huggingface.co/docs/lerobot/act target=_blank>LeRobot ACT 文档</a></li><li><a href=https://github.com/huggingface/lerobot/tree/main/src/lerobot/policies/act target=_blank>LeRobot ACT 代码实现</a></li><li><a href=https://io-ai.tech/platform/guides/Pipeline/LeRobot/ACT/ target=_blank>ACT 模型微调</a></li><li><a href=https://github.com/tonyzhaozh/act target=_blank>ACT 代码实现</a></li><li><a href=https://zhuanlan.zhihu.com/p/1921873980442280840 target=_blank>【VA 系列】ALOHA(ACT)</a></li><li><a href=https://www.bilibili.com/video/BV1xGF3eeEjB/ target=_blank>【精析】ACT 模型 | ALOHA | Action Chunking Transformer</a></li></ul></div></div></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full me-4" width=96 height=96 alt=xiadengma src=/img/author_hu_76f37802f31e48ac.jpg data-zoom-src=/img/author_hu_9efbc1c1dad84e42.jpg><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">作者</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">xiadengma</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/xiadengma target=_blank aria-label=Github title=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://xiadengma.com/ target=_blank aria-label=Link title=Link rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 640 512"><path fill="currentColor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg></span></span></a></div></div></div></div><div class=mb-10></div><details class="mt-2 mb-5 overflow-hidden rounded-lg ms-0 ps-5"><summary class="py-1 text-lg font-semibold cursor-pointer bg-primary-200 text-neutral-800 -ms-5 ps-5 dark:bg-primary-800 dark:text-neutral-100">深入浅出具身智能 -
系列文章</summary><div class="py-1 border-dotted border-neutral-300 border-s-1 -ms-5 ps-5 dark:border-neutral-600">§ :
本文</div><div class="py-1 border-dotted border-neutral-300 border-s-1 -ms-5 ps-5 dark:border-neutral-600"><a href=/posts/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-vae--cvae/>§ :
深入浅出 VAE & CVAE</a></div><div class="py-1 border-dotted border-neutral-300 border-s-1 -ms-5 ps-5 dark:border-neutral-600"><a href=/posts/%E4%B8%80%E4%BA%9B%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/>§ :
一些数学基础概念</a></div></details></div><script type=text/javascript src=/js/page.min.54b6f4371722649edbe871e431d8670d670878c22be8f36e229fe53cc9b786fe25a834def5e6de621f7a3e37b72bc8cd73839aa5ed907ed6cbd45cd3e1b0fa20.js integrity="sha512-VLb0NxciZJ7b6HHkMdhnDWcIeMIr6PNuIp/lPMm3hv4lqDTe9ebeYh96Pje3K8jNc4Oape2QftbL1FzT4bD6IA==" data-oid="views_posts/深入浅出 ACT/index.md" data-oid-likes="likes_posts/深入浅出 ACT/index.md"></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span class="flex flex-col"><a class="flex text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" href=/posts/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA-vae--cvae/><span class=leading-6><span class="inline-block rtl:rotate-180">&larr;</span>&ensp;深入浅出 VAE & CVAE
</span></a><span class="ms-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-10-19T02:47:15+08:00>2025年10月19日</time>
</span></span><span class="flex flex-col items-end"><a class="flex text-right text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" href=/posts/gnome49-.icns%E6%96%87%E4%BB%B6%E9%A2%84%E8%A7%88%E4%BF%AE%E5%A4%8D/><span class=leading-6>Gnome49 .Icns文件预览修复&ensp;<span class="inline-block rtl:rotate-180">&rarr;</span>
</span></a><span class="me-6 mt-1 text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-10-20T17:26:27+08:00>2025年10月20日</time></span></span></div></div></footer></article><div id=scroll-to-top class="fixed bottom-6 end-6 z-50 transform translate-y-4 opacity-0 duration-200"><a href=#the-top class="bf-scroll-to-top__btn pointer-events-auto flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label=返回顶部 title=返回顶部><span class=sr-only>返回顶部</span>
<svg class="bf-scroll-to-top__icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" aria-hidden="true"><path d="M12 19V5"/><path d="m5 12 7-7 7 7"/></svg></a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">© 2025 <a href=https://xiadengma.com target=_blank>XIADENGMA</a> | All rights reserved | <a href=https://beian.miit.gov.cn target=_blank>浙ICP备20005376号-2</a></p><p class="text-xs text-neutral-500 dark:text-neutral-400">🛸 by Hugo & Blowfish</p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script><div style=display:none class="bf-no-justify bf-algorithm bf-algorithm__caption bf-algorithm__body bf-figure-group bf-figure-group__grid bf-fig-16 bf-fig-single"></div><script>(()=>{const t="bf_code_wrap_mode",e="bf-code-nowrap",n="button.codeblock-wrap-toggle[data-code-wrap-toggle]";function i(){const e=localStorage.getItem(t);return e==="nowrap"?"nowrap":"wrap"}function s(s,o){const i=document.documentElement;s==="nowrap"?i.classList.add(e):i.classList.remove(e),o&&localStorage.setItem(t,s);const a=i.classList.contains(e);document.querySelectorAll(n).forEach(e=>{e.setAttribute("aria-pressed",a?"false":"true"),e.textContent=a?"折行":"滚动"})}function a(){const t=document.documentElement.classList.contains(e);s(t?"wrap":"nowrap",!0)}function o(){s(i(),!1),document.addEventListener("click",e=>{const t=e.target.closest?.(n);if(!t)return;e.preventDefault(),a(),t.blur()})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",o):o()})()</script><script>(()=>{const n="bf-mermaid-viewer",s="bf-open",g="bf-modal-open",h=.1,r=6,o=1.15,e={scale:1,baseW:0,baseH:0,pointerId:null,startX:0,startY:0,startScrollLeft:0,startScrollTop:0};function l(e,t,n){return Math.min(n,Math.max(t,e))}function O(){let e=document.getElementById(n);if(e)return e;e=document.createElement("div"),e.id=n,e.className="bf-mermaid-viewer",e.setAttribute("aria-hidden","true"),e.innerHTML=`
        <div class="bf-mermaid-viewer__backdrop" data-bf-mermaid-close></div>
        <div class="bf-mermaid-viewer__panel" role="dialog" aria-modal="true" aria-label="Mermaid 查看器">
          <div class="bf-mermaid-viewer__toolbar">
            <button type="button" class="bf-mermaid-viewer__tool" data-bf-mermaid-zoom-out aria-label="缩小" title="缩小">
              <svg class="bf-mermaid-viewer__icon" viewBox="0 0 24 24" aria-hidden="true">
                <path d="M5 12h14" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" />
              </svg>
            </button>
            <button type="button" class="bf-mermaid-viewer__tool" data-bf-mermaid-reset aria-label="适配" title="适配">
              <svg class="bf-mermaid-viewer__icon" viewBox="0 0 24 24" aria-hidden="true">
                <path d="M4 9V5h4M20 9V5h-4M4 15v4h4M20 15v4h-4" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" />
              </svg>
              <span class="bf-mermaid-viewer__label" data-bf-mermaid-zoom-label>100%</span>
            </button>
            <button type="button" class="bf-mermaid-viewer__tool" data-bf-mermaid-zoom-in aria-label="放大" title="放大">
              <svg class="bf-mermaid-viewer__icon" viewBox="0 0 24 24" aria-hidden="true">
                <path d="M12 5v14M5 12h14" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" />
              </svg>
            </button>
          </div>
          <button type="button" class="bf-mermaid-viewer__close" data-bf-mermaid-close aria-label="关闭" title="关闭">
            <svg class="bf-mermaid-viewer__icon" viewBox="0 0 24 24" aria-hidden="true">
              <path d="M6 6l12 12M18 6L6 18" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" />
            </svg>
          </button>
          <div class="bf-mermaid-viewer__canvas" data-bf-mermaid-canvas>
            <div class="bf-mermaid-viewer__stage" data-bf-mermaid-stage></div>
          </div>
        </div>
      `,document.body.appendChild(e),e.addEventListener("click",e=>{e.target.closest?.("[data-bf-mermaid-close]")&&(e.preventDefault(),u())}),document.addEventListener("keydown",e=>{(e.key==="Escape"||e.key==="Esc")&&u()});const t=e.querySelector("[data-bf-mermaid-canvas]"),s=e.querySelector("[data-bf-mermaid-zoom-in]"),i=e.querySelector("[data-bf-mermaid-zoom-out]"),r=e.querySelector("[data-bf-mermaid-reset]");return s?.addEventListener("click",e=>{e.preventDefault(),a(o)}),i?.addEventListener("click",e=>{e.preventDefault(),a(1/o)}),r?.addEventListener("click",e=>{e.preventDefault(),p()}),t?.addEventListener("wheel",y,{passive:!1}),t?.addEventListener("pointerdown",b),t?.addEventListener("pointermove",j),t?.addEventListener("pointerup",d),t?.addEventListener("pointercancel",d),e}function i(){const e=document.getElementById(n);return!!e&&e.classList.contains(s)}function t(){const e=O();return{root:e,canvas:e.querySelector("[data-bf-mermaid-canvas]"),stage:e.querySelector("[data-bf-mermaid-stage]"),resetBtn:e.querySelector("[data-bf-mermaid-reset]"),zoomLabel:e.querySelector("[data-bf-mermaid-zoom-label]")}}function w(e){const t=e?.viewBox?.baseVal;if(t&&t.width>0&&t.height>0)return{w:t.width,h:t.height};const n=e.getAttribute("width")||"",s=e.getAttribute("height")||"",o=n&&!n.includes("%")?parseFloat(n):0,i=s&&!s.includes("%")?parseFloat(s):0;if(o>0&&i>0)return{w:o,h:i};const a=e.getBoundingClientRect();return{w:a.width||800,h:a.height||600}}function v(){const{stage:e}=t();return e?.querySelector("svg")||null}function m(){const{resetBtn:s,zoomLabel:o}=t(),n=v();if(!n)return;const a=Math.max(1,e.baseW*e.scale),r=Math.max(1,e.baseH*e.scale);n.style.width=`${a}px`,n.style.height=`${r}px`,n.style.maxWidth="none",n.style.maxHeight="none",n.style.display="block";const i=`${Math.round(e.scale*100)}%`;o?o.textContent=i:s&&(s.textContent=i)}function f(){const{canvas:e}=t();if(!e)return;requestAnimationFrame(()=>{e.scrollLeft=Math.max(0,(e.scrollWidth-e.clientWidth)/2),e.scrollTop=Math.max(0,(e.scrollHeight-e.clientHeight)/2)})}function p(){const{canvas:n}=t();if(!n||!e.baseW||!e.baseH)return;const s=getComputedStyle(n),a=(parseFloat(s.paddingLeft)||0)+(parseFloat(s.paddingRight)||0),c=(parseFloat(s.paddingTop)||0)+(parseFloat(s.paddingBottom)||0),o=n.clientWidth-a,i=n.clientHeight-c;if(!o||!i)return;const d=Math.min(o/e.baseW,i/e.baseH)*.98;e.scale=l(d,h,r),m(),f()}function a(t){const n=l(e.scale*t,h,r);if(n===e.scale)return;e.scale=n,m(),f()}function y(e){if(!i())return;e.preventDefault(),a(e.deltaY<0?o:1/o)}function b(n){if(!i())return;if(n.pointerType==="mouse"&&n.button!==0)return;const{canvas:s}=t();if(!s)return;e.pointerId=n.pointerId,e.startX=n.clientX,e.startY=n.clientY,e.startScrollLeft=s.scrollLeft,e.startScrollTop=s.scrollTop,s.classList.add("bf-grabbing"),s.setPointerCapture?.(n.pointerId)}function j(n){if(!i())return;if(e.pointerId==null||e.pointerId!==n.pointerId)return;const{canvas:s}=t();if(!s)return;const o=n.clientX-e.startX,a=n.clientY-e.startY;s.scrollLeft=e.startScrollLeft-o,s.scrollTop=e.startScrollTop-a}function d(n){const{canvas:s}=t();if(e.pointerId==null||e.pointerId!==n.pointerId)return;e.pointerId=null,s?.classList.remove("bf-grabbing");try{s?.releasePointerCapture?.(n.pointerId)}catch{}}function _(n){const{root:o,stage:i}=t();if(!o||!i||!n)return;i.innerHTML="";const a=n.cloneNode(!0);a.removeAttribute("style"),i.appendChild(a);const r=w(a);e.baseW=r.w,e.baseH=r.h,e.scale=1,o.classList.add(s),o.setAttribute("aria-hidden","false"),document.documentElement.classList.add(g),requestAnimationFrame(()=>{p()})}function u(){const e=document.getElementById(n);if(!e)return;e.classList.remove(s),e.setAttribute("aria-hidden","true"),document.documentElement.classList.remove(g);const t=e.querySelector("[data-bf-mermaid-stage]");t&&(t.innerHTML="")}function c(){if(!document.querySelector(".article-content .mermaid"))return;document.addEventListener("click",e=>{const t=document.getElementById(n);if(t&&t.classList.contains(s)&&t.contains(e.target))return;const o=e.target?.closest?.(".article-content .mermaid");if(!o)return;const i=o.querySelector("svg");if(!i)return;const a=window.getSelection?.();if(a&&String(a).length>0)return;_(i)})}document.readyState==="loading"?document.addEventListener("DOMContentLoaded",c):c()})()</script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-500" data-url=https://blog.xiadengma.com/><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=搜索 tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="关闭 (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>